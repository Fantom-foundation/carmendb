Index: go/database/mpt/state.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright (c) 2024 Fantom Foundation\n//\n// Use of this software is governed by the Business Source License included\n// in the LICENSE file and at fantom.foundation/bsl11.\n//\n// Change Date: 2028-4-16\n//\n// On the date above, in accordance with the Business Source License, use of\n// this software will be governed by the GNU Lesser General Public License v3.\n\npackage mpt\n\nimport (\n\t\"bufio\"\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"hash\"\n\t\"io\"\n\t\"maps\"\n\t\"os\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"github.com/Fantom-foundation/Carmen/go/database/mpt/shared\"\n\t\"github.com/Fantom-foundation/Carmen/go/state\"\n\n\t\"github.com/Fantom-foundation/Carmen/go/backend\"\n\t\"github.com/Fantom-foundation/Carmen/go/common\"\n\t\"golang.org/x/crypto/sha3\"\n)\n\n//go:generate mockgen -source state.go -destination state_mocks.go -package mpt\n\n// Database is a global single access point to Merkle-Patricia-Trie (MPT) data.\n// It contains public methods to retrieve information about accounts\n// and storage slots, furthermore, it allows for  MPT nodes hashing.\n// MPT database maintains a DAG of MPT trees that can be each accessed\n// via a single root node.\n// The database is an appendable storage, where the current state is modified,\n// accessible via the current root node, and eventually sealed and appended\n// to the historical database.\n// Root nodes allow for traversing the respective MPT tree hierarchy\n// to access the history.\n// A Freeze method is provided to seal the current MPT so that further updates\n// will take place on a new version of the MPT.\n// The Current MPT tree may be modified, and the modifications are destructive,\n// until the tree is sealed by Freeze.\ntype Database interface {\n\tcommon.FlushAndCloser\n\tcommon.MemoryFootprintProvider\n\n\t// GetAccountInfo retrieves account information for input root and account address.\n\tGetAccountInfo(rootRef *NodeReference, addr common.Address) (AccountInfo, bool, error)\n\n\t// SetAccountInfo sets the input account into the storage under the input root and the address.\n\tSetAccountInfo(rootRef *NodeReference, addr common.Address, info AccountInfo) (NodeReference, error)\n\n\t// GetValue retrieves storage slot for input root, account address, and storage key.\n\tGetValue(rootRef *NodeReference, addr common.Address, key common.Key) (common.Value, error)\n\n\t// SetValue sets storage slot for input root, account address, and storage key.\n\tSetValue(rootRef *NodeReference, addr common.Address, key common.Key, value common.Value) (NodeReference, error)\n\n\t// ClearStorage removes all storage slots for the input address and the root.\n\tClearStorage(rootRef *NodeReference, addr common.Address) (NodeReference, error)\n\n\t// Freeze seals current trie, preventing further updates to it.\n\tFreeze(ref *NodeReference) error\n\n\t// VisitTrie allows for travertines the whole trie under the input root\n\tVisitTrie(rootRef *NodeReference, visitor NodeVisitor) error\n\n\t// Dump provides a debug print of the whole trie under the input root\n\tDump(rootRef *NodeReference)\n\n\t// Check verifies internal invariants of the Trie instance. If the trie is\n\t// self-consistent, nil is returned and the Trie is ready to be accessed. If\n\t// errors are detected, the Trie is to be considered in an invalid state and\n\t// the behavior of all other operations is undefined.\n\tCheck(rootRef *NodeReference) error\n\n\t// CheckAll verifies internal invariants of a set of Trie instances rooted by\n\t// the given nodes. It is a generalization of the Check() function.\n\tCheckAll(rootRefs []*NodeReference) error\n\n\t// CheckErrors returns an error that might have been\n\t// encountered on this forest in the past.\n\t// If the result is not empty, this\n\t// Forest is to be considered corrupted and should be discarded.\n\tCheckErrors() error\n\n\tupdateHashesFor(ref *NodeReference) (common.Hash, *NodeHashes, error)\n\tsetHashesFor(root *NodeReference, hashes *NodeHashes) error\n}\n\n// LiveState represents a single  Merkle-Patricia-Trie (MPT) view to the Database\n// as it was accessed for a single root.\n// It allows for reading and updating state\n// of accounts, storage slots, and codes.\n// Access to the data is provided via a set of getters,\n// while the update is provides via a single Apply function.\ntype LiveState interface {\n\tcommon.UpdateTarget\n\tcommon.MemoryFootprintProvider\n\tstate.LiveDB\n\n\t// GetHash provides hash root of this MPT.\n\t// The hash is recomputed if it is not available.\n\tGetHash() (hash common.Hash, err error)\n\n\t// GetCodeForHash retrieves bytecode stored\n\t// under the input hash.\n\tGetCodeForHash(hash common.Hash) []byte\n\n\t// GetCodes retrieves all codes and their hashes.\n\tGetCodes() (map[common.Hash][]byte, error)\n\n\t// UpdateHashes recomputes hash root of this trie.\n\tUpdateHashes() (common.Hash, *NodeHashes, error)\n\n\t// Root provides root of this trie.\n\tRoot() NodeReference\n\n\tcloseWithError(externalError error) error\n\tsetHashes(hashes *NodeHashes) error\n}\n\n// MptState implementation of a state utilizes an MPT based data structure. While\n// functionally equivalent to the Ethereum State MPT, hashes are computed using\n// a configurable algorithm.\n//\n// The main role of the MptState is to provide an adapter between a LiveTrie and\n// Carmen's State interface. Also, it retains an index of contract codes.\ntype MptState struct {\n\tdirectory string\n\tlock      common.LockFile\n\ttrie      *LiveTrie\n\tcode      map[common.Hash][]byte\n\tcodeDirty bool\n\tcodeMutex sync.Mutex\n\tcodefile  string\n\thasher    hash.Hash\n}\n\n// The capacity of an MPT's node cache must be at least as large as the maximum\n// number of nodes modified in a block. Evaluations show that most blocks\n// modify less than 2000 nodes. However, one block, presumably the one handling\n// the opera fork at ~4.5M, modifies 434.589 nodes. Thus, the cache size of a\n// MPT processing Fantom's history should be at least ~500.000 nodes.\nconst DefaultMptStateCapacity = 10_000_000\nconst MinMptStateCapacity = 2_000\n\nvar emptyCodeHash = common.GetHash(sha3.NewLegacyKeccak256(), []byte{})\n\nfunc newMptState(directory string, lock common.LockFile, trie *LiveTrie) (*MptState, error) {\n\tcodefile := directory + \"/codes.dat\"\n\tcodes, err := readCodes(codefile)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &MptState{\n\t\tdirectory: directory,\n\t\tlock:      lock,\n\t\ttrie:      trie,\n\t\tcode:      codes,\n\t\tcodefile:  codefile,\n\t}, nil\n}\n\nfunc openStateDirectory(directory string) (common.LockFile, error) {\n\tlock, err := LockDirectory(directory)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := tryMarkDirty(directory); err != nil {\n\t\treturn nil, errors.Join(err, lock.Release())\n\t}\n\n\treturn lock, nil\n}\n\nfunc tryMarkDirty(directory string) error {\n\tdirty, err := isDirty(directory)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif dirty {\n\t\treturn fmt.Errorf(\"unable to open %s, content is dirty, likely corrupted\", directory)\n\t}\n\treturn markDirty(directory)\n}\n\n// OpenGoMemoryState loads state information from the given directory and\n// creates a Trie entirely retained in memory.\nfunc OpenGoMemoryState(directory string, config MptConfig, cacheCapacity int) (*MptState, error) {\n\tlock, err := openStateDirectory(directory)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttrie, err := OpenInMemoryLiveTrie(directory, config, cacheCapacity)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newMptState(directory, lock, trie)\n}\n\nfunc OpenGoFileState(directory string, config MptConfig, cacheCapacity int) (*MptState, error) {\n\tlock, err := openStateDirectory(directory)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttrie, err := OpenFileLiveTrie(directory, config, cacheCapacity)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newMptState(directory, lock, trie)\n}\n\nfunc (s *MptState) CreateAccount(address common.Address) (err error) {\n\t_, exists, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif exists {\n\t\t// For existing accounts, only clear the storage, preserve the rest.\n\t\treturn s.trie.ClearStorage(address)\n\t}\n\t// Create account with hash of empty code.\n\treturn s.trie.SetAccountInfo(address, AccountInfo{\n\t\tCodeHash: emptyCodeHash,\n\t})\n}\n\nfunc (s *MptState) Exists(address common.Address) (bool, error) {\n\t_, exists, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn false, err\n\t}\n\treturn exists, nil\n}\n\nfunc (s *MptState) DeleteAccount(address common.Address) error {\n\treturn s.trie.SetAccountInfo(address, AccountInfo{})\n}\n\nfunc (s *MptState) GetBalance(address common.Address) (balance common.Balance, err error) {\n\tinfo, exists, err := s.trie.GetAccountInfo(address)\n\tif !exists || err != nil {\n\t\treturn common.Balance{}, err\n\t}\n\treturn info.Balance, nil\n}\n\nfunc (s *MptState) SetBalance(address common.Address, balance common.Balance) (err error) {\n\tinfo, exists, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif info.Balance == balance {\n\t\treturn nil\n\t}\n\tinfo.Balance = balance\n\tif !exists {\n\t\tinfo.CodeHash = emptyCodeHash\n\t}\n\treturn s.trie.SetAccountInfo(address, info)\n}\n\nfunc (s *MptState) GetNonce(address common.Address) (nonce common.Nonce, err error) {\n\tinfo, _, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn common.Nonce{}, err\n\t}\n\treturn info.Nonce, nil\n}\n\nfunc (s *MptState) SetNonce(address common.Address, nonce common.Nonce) (err error) {\n\tinfo, exists, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif info.Nonce == nonce {\n\t\treturn nil\n\t}\n\tinfo.Nonce = nonce\n\tif !exists {\n\t\tinfo.CodeHash = emptyCodeHash\n\t}\n\treturn s.trie.SetAccountInfo(address, info)\n}\n\nfunc (s *MptState) GetStorage(address common.Address, key common.Key) (value common.Value, err error) {\n\treturn s.trie.GetValue(address, key)\n}\n\nfunc (s *MptState) SetStorage(address common.Address, key common.Key, value common.Value) error {\n\treturn s.trie.SetValue(address, key, value)\n}\n\nfunc (s *MptState) GetCode(address common.Address) (value []byte, err error) {\n\tinfo, exists, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !exists {\n\t\treturn nil, nil\n\t}\n\ts.codeMutex.Lock()\n\tres := s.code[info.CodeHash]\n\ts.codeMutex.Unlock()\n\treturn res, nil\n}\n\nfunc (s *MptState) GetCodeForHash(hash common.Hash) []byte {\n\ts.codeMutex.Lock()\n\tres := s.code[hash]\n\ts.codeMutex.Unlock()\n\treturn res\n}\n\nfunc (s *MptState) GetCodeSize(address common.Address) (size int, err error) {\n\tcode, err := s.GetCode(address)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn len(code), err\n}\n\nfunc (s *MptState) SetCode(address common.Address, code []byte) (err error) {\n\tvar codeHash common.Hash\n\tif s.hasher == nil {\n\t\ts.hasher = sha3.NewLegacyKeccak256()\n\t}\n\tcodeHash = common.GetHash(s.hasher, code)\n\n\tinfo, exists, err := s.trie.GetAccountInfo(address)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !exists && len(code) == 0 {\n\t\treturn nil\n\t}\n\tif info.CodeHash == codeHash {\n\t\treturn nil\n\t}\n\tinfo.CodeHash = codeHash\n\ts.codeMutex.Lock()\n\ts.code[codeHash] = code\n\ts.codeDirty = true\n\ts.codeMutex.Unlock()\n\treturn s.trie.SetAccountInfo(address, info)\n}\n\nfunc (s *MptState) GetCodeHash(address common.Address) (hash common.Hash, err error) {\n\tinfo, exists, err := s.trie.GetAccountInfo(address)\n\tif !exists || err != nil {\n\t\treturn emptyCodeHash, err\n\t}\n\treturn info.CodeHash, nil\n}\n\nfunc (s *MptState) GetRootId() NodeId {\n\treturn s.trie.root.Id()\n}\n\nfunc (s *MptState) GetHash() (hash common.Hash, err error) {\n\thash, hints, err := s.trie.UpdateHashes()\n\tif hints != nil {\n\t\thints.Release()\n\t}\n\treturn hash, err\n}\n\nfunc (s *MptState) Apply(block uint64, update common.Update) (archiveUpdateHints common.Releaser, err error) {\n\tif err := update.ApplyTo(s); err != nil {\n\t\treturn nil, err\n\t}\n\t_, hints, err := s.trie.UpdateHashes()\n\treturn hints, err\n}\n\nfunc (s *MptState) Visit(visitor NodeVisitor) error {\n\treturn s.trie.VisitTrie(visitor)\n}\n\nfunc (s *MptState) GetCodes() (map[common.Hash][]byte, error) {\n\ts.codeMutex.Lock()\n\tres := maps.Clone(s.code)\n\ts.codeMutex.Unlock()\n\treturn res, nil\n}\n\n// Flush codes and state trie\nfunc (s *MptState) Flush() error {\n\t// flush codes\n\tvar err error\n\ts.codeMutex.Lock()\n\tif s.codeDirty {\n\t\terr = writeCodes(s.code, s.codefile)\n\t\tif err == nil {\n\t\t\ts.codeDirty = false\n\t\t}\n\t}\n\ts.codeMutex.Unlock()\n\treturn errors.Join(\n\t\ts.trie.forest.CheckErrors(),\n\t\terr,\n\t\ts.trie.Flush(),\n\t)\n}\n\nfunc (s *MptState) Close() error {\n\treturn s.closeWithError(nil)\n}\n\nfunc (s *MptState) closeWithError(externalError error) error {\n\t// Only if the state can be successfully closed, the directory is to\n\t// be marked as clean. Otherwise, the dirty flag needs to be retained.\n\terr := errors.Join(\n\t\texternalError,\n\t\ts.Flush(),\n\t\ts.trie.Close(),\n\t)\n\tif err == nil {\n\t\terr = markClean(s.directory)\n\t}\n\treturn errors.Join(\n\t\terr,\n\t\ts.lock.Release(),\n\t)\n}\n\nfunc (s *MptState) GetSnapshotableComponents() []backend.Snapshotable {\n\t//panic(\"not implemented\")\n\treturn nil\n}\n\nfunc (s *MptState) RunPostRestoreTasks() error {\n\t//panic(\"not implemented\")\n\treturn nil\n}\n\n// GetMemoryFootprint provides sizes of individual components of the state in the memory\nfunc (s *MptState) GetMemoryFootprint() *common.MemoryFootprint {\n\tmf := common.NewMemoryFootprint(unsafe.Sizeof(*s))\n\tmf.AddChild(\"trie\", s.trie.GetMemoryFootprint())\n\tvar sizeCodes uint\n\ts.codeMutex.Lock()\n\tfor k, v := range s.code {\n\t\tsizeCodes += uint(len(k) + len(v))\n\t}\n\ts.codeMutex.Unlock()\n\tmf.AddChild(\"codes\", common.NewMemoryFootprint(uintptr(sizeCodes)))\n\treturn mf\n}\n\nfunc (s *MptState) UpdateHashes() (common.Hash, *NodeHashes, error) {\n\treturn s.trie.UpdateHashes()\n}\n\nfunc (s *MptState) Root() NodeReference {\n\treturn s.trie.root\n}\n\nfunc (s *MptState) setHashes(hashes *NodeHashes) error {\n\treturn s.trie.setHashes(hashes)\n}\n\n// readCodes parses the content of the given file if it exists or returns\n// a an empty code collection if there is no such file.\nfunc readCodes(filename string) (map[common.Hash][]byte, error) {\n\t// If there is no file, initialize and return an empty code collection.\n\tif _, err := os.Stat(filename); err != nil {\n\t\treturn map[common.Hash][]byte{}, nil\n\t}\n\n\tfile, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer file.Close()\n\treader := bufio.NewReader(file)\n\treturn parseCodes(reader)\n}\n\nfunc parseCodes(reader io.Reader) (map[common.Hash][]byte, error) {\n\t// If the file exists, parse it and return its content.\n\tres := map[common.Hash][]byte{}\n\t// The format is simple: [<key>, <length>, <code>]*\n\tvar hash common.Hash\n\tvar length [4]byte\n\tfor {\n\t\tif _, err := io.ReadFull(reader, hash[:]); err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\treturn res, nil\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tif _, err := io.ReadFull(reader, length[:]); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tsize := binary.BigEndian.Uint32(length[:])\n\t\tcode := make([]byte, size)\n\t\tif _, err := io.ReadFull(reader, code[:]); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tres[hash] = code\n\t}\n}\n\n// writeCodes write the given map of codes to the given file.\nfunc writeCodes(codes map[common.Hash][]byte, filename string) (err error) {\n\tfile, err := os.Create(filename)\n\tif err != nil {\n\t\treturn err\n\t}\n\twriter := bufio.NewWriter(file)\n\treturn errors.Join(\n\t\twriteCodesTo(codes, writer),\n\t\twriter.Flush(),\n\t\tfile.Close())\n}\n\nfunc writeCodesTo(codes map[common.Hash][]byte, writer io.Writer) (err error) {\n\t// The format is simple: [<key>, <length>, <code>]*\n\tfor key, code := range codes {\n\t\tif _, err := writer.Write(key[:]); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar length [4]byte\n\t\tbinary.BigEndian.PutUint32(length[:], uint32(len(code)))\n\t\tif _, err := writer.Write(length[:]); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif _, err := writer.Write(code); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\n// EstimatePerNodeMemoryUsage returns an estimated upper bound for the\n// amount of memory used per MPT node. This values is provided to facilitate\n// a conversion between memory limits expressed in bytes and MPT cache\n// sizes defined by the number of stored nodes.\nfunc EstimatePerNodeMemoryUsage() int {\n\n\t// The largest node is the BranchNode with ~944 bytes, which is\n\t// likely allocated into 1 KB memory slots. Thus, a memory usage\n\t// of 1 KB is used for the notes\n\tmaxNodeSize := 1 << 10\n\n\t// Additionally, every node in the node cache needs a owner slot\n\t// and a NodeID/ownerPosition entry pair in the index of the cache.\n\tnodeCacheSlotSize := unsafe.Sizeof(nodeOwner{}) +\n\t\tunsafe.Sizeof(NodeId(0)) +\n\t\tunsafe.Sizeof(ownerPosition(0)) +\n\t\tunsafe.Sizeof(shared.Shared[Node]{})\n\n\treturn maxNodeSize + int(nodeCacheSlotSize)\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/go/database/mpt/state.go b/go/database/mpt/state.go
--- a/go/database/mpt/state.go	(revision cb0032b7724512a252766f973bddcacdc9752c54)
+++ b/go/database/mpt/state.go	(date 1717749839251)
@@ -65,6 +65,9 @@
 	// ClearStorage removes all storage slots for the input address and the root.
 	ClearStorage(rootRef *NodeReference, addr common.Address) (NodeReference, error)
 
+	// HasEmptyStorage returns true if account has empty storage.
+	HasEmptyStorage(rootRef *NodeReference, addr common.Address) (bool, error)
+
 	// Freeze seals current trie, preventing further updates to it.
 	Freeze(ref *NodeReference) error
 
Index: go/database/mpt/forest.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright (c) 2024 Fantom Foundation\n//\n// Use of this software is governed by the Business Source License included\n// in the LICENSE file and at fantom.foundation/bsl11.\n//\n// Change Date: 2028-4-16\n//\n// On the date above, in accordance with the Business Source License, use of\n// this software will be governed by the GNU Lesser General Public License v3.\n\npackage mpt\n\nimport (\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io\"\n\t\"log\"\n\t\"os\"\n\t\"sort\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"unsafe\"\n\n\t\"github.com/Fantom-foundation/Carmen/go/backend/stock\"\n\t\"github.com/Fantom-foundation/Carmen/go/backend/stock/file\"\n\t\"github.com/Fantom-foundation/Carmen/go/backend/stock/memory\"\n\t\"github.com/Fantom-foundation/Carmen/go/backend/stock/synced\"\n\t\"github.com/Fantom-foundation/Carmen/go/common\"\n\t\"github.com/Fantom-foundation/Carmen/go/database/mpt/shared\"\n)\n\ntype StorageMode bool\n\nconst (\n\t// Immutable is the mode of an archive or a read-only state on the disk.\n\t// All nodes written to disk will be finalized and never updated again.\n\tImmutable StorageMode = true\n\t// Mutable is the mode of a LiveDB in which the state on the disk can be\n\t// modified through destructive updates.\n\tMutable StorageMode = false\n\t// forestClosedErr is an error returned when a forest is already closed.\n\tforestClosedErr = common.ConstError(\"forest already closed\")\n)\n\n// printWarningDefaultNodeFreezing allows for printing a warning that a node is going to be frozen\n// as a consequence of its flushing to the disk.\nconst printWarningDefaultNodeFreezing = false\n\nfunc (m StorageMode) String() string {\n\tif m == Immutable {\n\t\treturn \"Immutable\"\n\t} else {\n\t\treturn \"Mutable\"\n\t}\n}\n\n// Root is used to identify and verify root nodes of trees in forests.\ntype Root struct {\n\tNodeRef NodeReference\n\tHash    common.Hash\n}\n\n// ForestConfig summarizes forest instance configuration options that affect\n// the functional and non-functional properties of a forest but do not change\n// the on-disk format.\ntype ForestConfig struct {\n\tMode                   StorageMode // whether to perform destructive or constructive updates\n\tCacheCapacity          int         // the maximum number of nodes retained in memory\n\twriteBufferChannelSize int         // the maximum number of elements retained in the write buffer channel\n}\n\n// Forest is a utility node managing nodes for one or more Tries.\n// It provides the common foundation for the Live and Archive Tries.\n//\n// Forests are thread safe. Thus, read and write operations may be\n// conducted concurrently.\ntype Forest struct {\n\tconfig MptConfig\n\n\t// The stock containers managing individual node types.\n\tbranches   stock.Stock[uint64, BranchNode]\n\textensions stock.Stock[uint64, ExtensionNode]\n\taccounts   stock.Stock[uint64, AccountNode]\n\tvalues     stock.Stock[uint64, ValueNode]\n\n\t// Indicates whether all values in the stock should be considered\n\t// frozen, and thus immutable as required for the archive case or\n\t// mutable, as for the live-db-only case.\n\tstorageMode StorageMode\n\n\t// A unified cache for all node types.\n\tnodeCache NodeCache\n\n\t// The hasher managing node hashes for this forest.\n\thasher hasher\n\n\t// Cached hashers for keys and addresses (thread safe).\n\tkeyHasher     CachedHasher[common.Key]\n\taddressHasher CachedHasher[common.Address]\n\n\t// A buffer for asynchronously writing nodes to files.\n\twriteBuffer WriteBuffer\n\n\t// A mutex synchronizing the transfer of elements between the cache, the\n\t// write buffer, and stocks (=disks).\n\tnodeTransferMutex sync.Mutex\n\n\t// Utilities to manage a background worker releasing nodes.\n\treleaseQueue chan<- NodeId   // send EmptyId to trigger sync signal\n\treleaseSync  <-chan struct{} // signaled whenever the release worker reaches a sync point\n\treleaseError <-chan error    // errors detected by the release worker\n\treleaseDone  <-chan struct{} // closed when the release worker is done\n\n\t// A list of issues encountered while performing operations on the forest.\n\t// If this list is non-empty, no guarantees are provided on the correctness\n\t// of the maintained forest. Thus, it should be considered corrupted.\n\terrors []error\n\n\t// A flag indicating whether the forest is closed.\n\tclosed atomic.Bool\n}\n\nfunc OpenInMemoryForest(directory string, mptConfig MptConfig, forestConfig ForestConfig) (*Forest, error) {\n\tif _, err := checkForestMetadata(directory, mptConfig, forestConfig.Mode); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsuccess := false\n\tvar err error\n\tclosers := make(closers, 0, 4)\n\tdefer func() {\n\t\t// if opening the forest was not successful, close all opened stocks.\n\t\tif !success {\n\t\t\terr = errors.Join(err, closers.CloseAll())\n\t\t}\n\t}()\n\n\taccountEncoder, branchEncoder, extensionEncoder, valueEncoder := getEncoder(mptConfig)\n\tbranches, err := memory.OpenStock[uint64, BranchNode](branchEncoder, directory+\"/branches\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, branches)\n\n\textensions, err := memory.OpenStock[uint64, ExtensionNode](extensionEncoder, directory+\"/extensions\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, extensions)\n\n\taccounts, err := memory.OpenStock[uint64, AccountNode](accountEncoder, directory+\"/accounts\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, accounts)\n\n\tvalues, err := memory.OpenStock[uint64, ValueNode](valueEncoder, directory+\"/values\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, values)\n\n\tsuccess = true\n\treturn makeForest(mptConfig, directory, branches, extensions, accounts, values, forestConfig)\n}\n\nfunc OpenFileForest(directory string, mptConfig MptConfig, forestConfig ForestConfig) (*Forest, error) {\n\tif _, err := checkForestMetadata(directory, mptConfig, forestConfig.Mode); err != nil {\n\t\treturn nil, err\n\t}\n\n\tsuccess := false\n\tvar err error\n\tclosers := make(closers, 0, 4)\n\tdefer func() {\n\t\t// if opening the forest was not successful, close all opened stocks.\n\t\tif !success {\n\t\t\terr = errors.Join(err, closers.CloseAll())\n\t\t}\n\t}()\n\n\taccountEncoder, branchEncoder, extensionEncoder, valueEncoder := getEncoder(mptConfig)\n\tbranches, err := file.OpenStock[uint64, BranchNode](branchEncoder, directory+\"/branches\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, branches)\n\n\textensions, err := file.OpenStock[uint64, ExtensionNode](extensionEncoder, directory+\"/extensions\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, extensions)\n\n\taccounts, err := file.OpenStock[uint64, AccountNode](accountEncoder, directory+\"/accounts\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, accounts)\n\n\tvalues, err := file.OpenStock[uint64, ValueNode](valueEncoder, directory+\"/values\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tclosers = append(closers, values)\n\n\tsuccess = true\n\treturn makeForest(mptConfig, directory, branches, extensions, accounts, values, forestConfig)\n}\n\n// closers is a shortcut for the list of io.Closer.\ntype closers []io.Closer\n\n// CloseAll closes all the closers and returns an error if any errors occurred during the closing process.\nfunc (c closers) CloseAll() error {\n\tvar errs []error\n\tfor _, closer := range c {\n\t\terrs = append(errs, closer.Close())\n\t}\n\treturn errors.Join(errs...)\n}\n\nfunc checkForestMetadata(directory string, config MptConfig, mode StorageMode) (ForestMetadata, error) {\n\tpath := directory + \"/forest.json\"\n\tmeta, present, err := ReadForestMetadata(path)\n\tif err != nil {\n\t\treturn meta, err\n\t}\n\n\t// Check present metadata to match expected configuration.\n\tif present {\n\t\tif want, got := config.Name, meta.Configuration; want != got {\n\t\t\treturn meta, fmt.Errorf(\"unexpected MPT configuration in directory, wanted %v, got %v\", want, got)\n\t\t}\n\t\tif want, got := StorageMode(mode == Mutable), StorageMode(meta.Mutable); want != got {\n\t\t\treturn meta, fmt.Errorf(\"unexpected MPT storage mode in directory, wanted %v, got %v\", want, got)\n\t\t}\n\t\treturn meta, nil\n\t}\n\n\t// Write metadata to disk to create new forest.\n\tmeta = ForestMetadata{\n\t\tConfiguration: config.Name,\n\t\tMutable:       mode == Mutable,\n\t}\n\n\t// Update on-disk meta-data.\n\tmetadata, err := json.Marshal(meta)\n\treturn meta, errors.Join(err, os.WriteFile(path, metadata, 0600))\n}\n\nfunc makeForest(\n\tmptConfig MptConfig,\n\tdirectory string,\n\tbranches stock.Stock[uint64, BranchNode],\n\textensions stock.Stock[uint64, ExtensionNode],\n\taccounts stock.Stock[uint64, AccountNode],\n\tvalues stock.Stock[uint64, ValueNode],\n\tforestConfig ForestConfig,\n) (*Forest, error) {\n\treleaseQueue := make(chan NodeId, 1<<16) // NodeIds are small and a large buffer increases resilience.\n\treleaseSync := make(chan struct{})\n\treleaseError := make(chan error, 1)\n\treleaseDone := make(chan struct{})\n\n\tres := &Forest{\n\t\tconfig:        mptConfig,\n\t\tbranches:      synced.Sync(branches),\n\t\textensions:    synced.Sync(extensions),\n\t\taccounts:      synced.Sync(accounts),\n\t\tvalues:        synced.Sync(values),\n\t\tstorageMode:   forestConfig.Mode,\n\t\tnodeCache:     NewNodeCache(forestConfig.CacheCapacity),\n\t\thasher:        mptConfig.Hashing.createHasher(),\n\t\tkeyHasher:     NewKeyHasher(),\n\t\taddressHasher: NewAddressHasher(),\n\t\treleaseQueue:  releaseQueue,\n\t\treleaseSync:   releaseSync,\n\t\treleaseError:  releaseError,\n\t\treleaseDone:   releaseDone,\n\t}\n\n\t// Run a background worker releasing entire tries of nodes on demand.\n\tgo func() {\n\t\tdefer close(releaseDone)\n\t\tdefer close(releaseError)\n\t\tdefer close(releaseSync)\n\t\tfor id := range releaseQueue {\n\t\t\tif id.IsEmpty() {\n\t\t\t\treleaseSync <- struct{}{}\n\t\t\t} else {\n\t\t\t\tref := NewNodeReference(id)\n\t\t\t\thandle, err := res.getWriteAccess(&ref)\n\t\t\t\tif err != nil {\n\t\t\t\t\treleaseError <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\terr = handle.Get().Release(res, &ref, handle)\n\t\t\t\thandle.Release()\n\t\t\t\tif err != nil {\n\t\t\t\t\treleaseError <- err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}()\n\n\tchannelSize := forestConfig.writeBufferChannelSize\n\tif channelSize <= 0 {\n\t\tchannelSize = 1024 // the default value\n\t}\n\n\tres.writeBuffer = makeWriteBuffer(writeBufferSink{res}, channelSize)\n\treturn res, nil\n}\n\nfunc (s *Forest) GetAccountInfo(rootRef *NodeReference, addr common.Address) (AccountInfo, bool, error) {\n\thandle, err := s.getReadAccess(rootRef)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain read access to node %v: %w\", rootRef.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t\treturn AccountInfo{}, false, err\n\t}\n\tdefer handle.Release()\n\tpath := AddressToNibblePath(addr, s)\n\tinfo, exists, err := handle.Get().GetAccount(s, addr, path[:])\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to fetch account information for account %v: %w\", addr, err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn info, exists, err\n}\n\nfunc (s *Forest) SetAccountInfo(rootRef *NodeReference, addr common.Address, info AccountInfo) (NodeReference, error) {\n\troot, err := s.getWriteAccess(rootRef)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain write access to node %v: %w\", rootRef.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t\treturn NodeReference{}, err\n\t}\n\tdefer root.Release()\n\tpath := AddressToNibblePath(addr, s)\n\tnewRoot, _, err := root.Get().SetAccount(s, rootRef, root, addr, path[:], info)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to update account information for account %v: %w\", addr, err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn newRoot, err\n}\n\nfunc (s *Forest) GetValue(rootRef *NodeReference, addr common.Address, key common.Key) (common.Value, error) {\n\troot, err := s.getReadAccess(rootRef)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain read access to node %v: %w\", rootRef.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t\treturn common.Value{}, err\n\t}\n\tdefer root.Release()\n\tpath := AddressToNibblePath(addr, s)\n\tvalue, _, err := root.Get().GetSlot(s, addr, path[:], key)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to fetch value for %v/%v: %w\", addr, key, err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn value, err\n}\n\nfunc (s *Forest) SetValue(rootRef *NodeReference, addr common.Address, key common.Key, value common.Value) (NodeReference, error) {\n\troot, err := s.getWriteAccess(rootRef)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain write access to node %v: %w\", rootRef.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t\treturn NodeReference{}, err\n\t}\n\tdefer root.Release()\n\tpath := AddressToNibblePath(addr, s)\n\tnewRoot, _, err := root.Get().SetSlot(s, rootRef, root, addr, path[:], key, value)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to update value for %v/%v: %w\", addr, key, err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn newRoot, err\n}\n\nfunc (s *Forest) ClearStorage(rootRef *NodeReference, addr common.Address) (NodeReference, error) {\n\troot, err := s.getWriteAccess(rootRef)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain write access to node %v: %w\", rootRef.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t\treturn NodeReference{}, err\n\t}\n\tdefer root.Release()\n\tpath := AddressToNibblePath(addr, s)\n\tnewRoot, _, err := root.Get().ClearStorage(s, rootRef, root, addr, path[:])\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to clear storage for %v: %w\", addr, err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn newRoot, err\n}\n\nfunc (s *Forest) VisitTrie(rootRef *NodeReference, visitor NodeVisitor) error {\n\troot, err := s.getViewAccess(rootRef)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain view access to node %v: %w\", rootRef.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t\treturn err\n\t}\n\tdefer root.Release()\n\t_, err = root.Get().Visit(s, rootRef, 0, visitor)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"error during trie visit: %w\", err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn err\n}\n\nfunc (s *Forest) updateHashesFor(ref *NodeReference) (common.Hash, *NodeHashes, error) {\n\thash, hints, err := s.hasher.updateHashes(ref, s)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"error during hash update: %w\", err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn hash, hints, err\n}\n\nfunc (s *Forest) setHashesFor(root *NodeReference, hashes *NodeHashes) error {\n\tfor _, cur := range hashes.GetHashes() {\n\t\twrite, err := s.getMutableNodeByPath(root, cur.Path)\n\t\tif err != nil {\n\t\t\terr = fmt.Errorf(\"error during location of node at %v: %w\", cur.Path, err)\n\t\t\ts.errors = append(s.errors, err)\n\t\t\treturn err\n\t\t}\n\t\twrite.Get().SetHash(cur.Hash)\n\t\twrite.Release()\n\t}\n\treturn nil\n}\n\nfunc (s *Forest) getHashFor(ref *NodeReference) (common.Hash, error) {\n\thash, err := s.hasher.getHash(ref, s)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"error while retrieving hash for node %v: %w\", ref.Id(), err)\n\t\ts.errors = append(s.errors, err)\n\t}\n\treturn hash, err\n}\n\nfunc (s *Forest) hashKey(key common.Key) common.Hash {\n\thash, _ := s.keyHasher.Hash(key)\n\treturn hash\n}\n\nfunc (s *Forest) hashAddress(address common.Address) common.Hash {\n\thash, _ := s.addressHasher.Hash(address)\n\treturn hash\n}\n\nfunc (f *Forest) Freeze(ref *NodeReference) error {\n\tif f.storageMode != Immutable {\n\t\treturn fmt.Errorf(\"node-freezing only supported in archive mode\")\n\t}\n\troot, err := f.getWriteAccess(ref)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"failed to obtain write access to node %v: %w\", ref.Id(), err)\n\t\tf.errors = append(f.errors, err)\n\t\treturn err\n\t}\n\tdefer root.Release()\n\terr = root.Get().Freeze(f, root)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"error while freezing trie rooted by %v: %w\", ref.Id(), err)\n\t\tf.errors = append(f.errors, err)\n\t}\n\treturn err\n}\n\n// CheckErrors returns an error that might have been\n// encountered on this forest in the past.\n// If the result is not empty, this\n// Forest is to be considered corrupted and should be discarded.\nfunc (s *Forest) CheckErrors() error {\n\treturn errors.Join(s.errors...)\n}\n\nfunc (s *Forest) Flush() error {\n\t// Wait for releaser to finish its current tasks.\n\ts.releaseQueue <- EmptyId() // signals a sync request\n\t<-s.releaseSync\n\n\t// Consume potential operation and release errors.\n\terrs := []error{\n\t\ts.CheckErrors(),\n\t\ts.collectReleaseWorkerErrors(),\n\t}\n\n\t// Get snapshot of set of dirty Node IDs.\n\tids := make([]NodeId, 0, 1<<16)\n\ts.nodeCache.ForEach(func(id NodeId, node *shared.Shared[Node]) {\n\t\thandle := node.GetViewHandle()\n\t\tdirty := handle.Get().IsDirty()\n\t\thandle.Release()\n\t\tif dirty {\n\t\t\tids = append(ids, id)\n\t\t}\n\t})\n\n\terrs = append(errs, s.flushDirtyIds(ids))\n\n\treturn errors.Join(\n\t\terrors.Join(errs...),\n\t\ts.writeBuffer.Flush(),\n\t\ts.accounts.Flush(),\n\t\ts.branches.Flush(),\n\t\ts.extensions.Flush(),\n\t\ts.values.Flush(),\n\t)\n}\n\nfunc (s *Forest) flushDirtyIds(ids []NodeId) error {\n\tvar errs []error\n\t// Flush dirty keys in order (to avoid excessive seeking).\n\tsort.Slice(ids, func(i, j int) bool { return ids[i] < ids[j] })\n\tfor _, id := range ids {\n\t\tref := NewNodeReference(id)\n\t\tnode, present := s.nodeCache.Get(&ref)\n\t\tif present {\n\t\t\thandle := node.GetWriteHandle()\n\t\t\tnode := handle.Get()\n\t\t\terr := s.flushNode(id, node)\n\t\t\tif err == nil {\n\t\t\t\tnode.MarkClean()\n\t\t\t} else {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t\thandle.Release()\n\t\t} else {\n\t\t\terrs = append(errs, fmt.Errorf(\"missing dirty node %v in node cache\", id))\n\t\t}\n\t}\n\n\treturn errors.Join(errs...)\n}\n\nfunc (s *Forest) Close() error {\n\t// Ensure that the forest is only closed once.\n\tif !s.closed.CompareAndSwap(false, true) {\n\t\treturn forestClosedErr\n\t}\n\n\tvar errs []error\n\terrs = append(errs, s.Flush())\n\n\t// shut down release worker\n\tclose(s.releaseQueue)\n\t<-s.releaseDone\n\n\t// Consume potential release errors.\n\terrs = append(errs, s.collectReleaseWorkerErrors())\n\n\treturn errors.Join(\n\t\terrors.Join(errs...),\n\t\ts.writeBuffer.Close(),\n\t\ts.accounts.Close(),\n\t\ts.branches.Close(),\n\t\ts.extensions.Close(),\n\t\ts.values.Close(),\n\t)\n}\n\nfunc (s *Forest) collectReleaseWorkerErrors() error {\n\tvar errs []error\nloop:\n\tfor {\n\t\tselect {\n\t\tcase err, open := <-s.releaseError:\n\t\t\tif !open {\n\t\t\t\tbreak loop\n\t\t\t}\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\tdefault:\n\t\t\tbreak loop\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\n// GetMemoryFootprint provides sizes of individual components of the state in the memory\nfunc (s *Forest) GetMemoryFootprint() *common.MemoryFootprint {\n\tmf := common.NewMemoryFootprint(unsafe.Sizeof(*s))\n\tmf.AddChild(\"accounts\", s.accounts.GetMemoryFootprint())\n\tmf.AddChild(\"branches\", s.branches.GetMemoryFootprint())\n\tmf.AddChild(\"extensions\", s.extensions.GetMemoryFootprint())\n\tmf.AddChild(\"values\", s.values.GetMemoryFootprint())\n\tmf.AddChild(\"cache\", s.nodeCache.GetMemoryFootprint())\n\tmf.AddChild(\"hashedKeysCache\", s.keyHasher.GetMemoryFootprint())\n\tmf.AddChild(\"hashedAddressesCache\", s.addressHasher.GetMemoryFootprint())\n\treturn mf\n}\n\n// Dump prints the content of the Trie to the console. Mainly intended for debugging.\nfunc (s *Forest) Dump(rootRef *NodeReference) {\n\troot, err := s.getViewAccess(rootRef)\n\tif err != nil {\n\t\tfmt.Printf(\"Failed to fetch root: %v\", err)\n\t\treturn\n\t}\n\tdefer root.Release()\n\troot.Get().Dump(os.Stdout, s, rootRef, \"\")\n}\n\n// Check verifies internal invariants of the Trie instance. If the trie is\n// self-consistent, nil is returned and the Trie is ready to be accessed. If\n// errors are detected, the Trie is to be considered in an invalid state and\n// the behavior of all other operations is undefined.\nfunc (s *Forest) Check(rootRef *NodeReference) error {\n\treturn s.CheckAll([]*NodeReference{rootRef})\n}\n\n// CheckAll verifies internal invariants of a set of Trie instances rooted by\n// the given nodes. It is a generalization of the Check() function.\nfunc (s *Forest) CheckAll(rootRefs []*NodeReference) error {\n\treturn CheckForest(s, rootRefs)\n}\n\n// -- NodeManager interface --\n\nfunc (s *Forest) getConfig() MptConfig {\n\treturn s.config\n}\n\nfunc (s *Forest) getSharedNode(ref *NodeReference) (*shared.Shared[Node], error) {\n\tres, found := s.nodeCache.Get(ref)\n\tif found {\n\t\treturn res, nil\n\t}\n\n\t// Check whether the node is in the write buffer.\n\t// Note: although Cancel is thread safe, it is important to make sure\n\t// that this part is only run by a single thread to avoid one thread\n\t// recovering a node from the buffer and another fetching it from the\n\t// storage. This synchronization is currently ensured by acquiring the\n\t// nodeTransferMutex and holding it until the end of the function.\n\t// Using a global lock that does not differentiate between node IDs may\n\t// cause performance issues since it is delaying unrelated lookup\n\t// operations. However, the impact should be small since cache misses\n\t// should be infrequent enough. Unless it is detected in CPU profiles\n\t// and traces, this lock should be fine.\n\ts.nodeTransferMutex.Lock()\n\tdefer s.nodeTransferMutex.Unlock()\n\n\tid := ref.Id()\n\tres, found = s.writeBuffer.Cancel(id)\n\tif found {\n\t\tmasterCopy, _ := s.addToCacheHoldingTransferMutex(ref, res)\n\t\tif masterCopy != res {\n\t\t\tpanic(\"failed to reinstate element from write buffer\")\n\t\t}\n\t\treturn res, nil\n\t}\n\n\t// Load the node from persistent storage.\n\tvar node Node\n\tvar err error\n\tif id.IsValue() {\n\t\tvalue, e := s.values.Get(id.Index())\n\t\tnode, err = &value, e\n\t} else if id.IsAccount() {\n\t\tvalue, e := s.accounts.Get(id.Index())\n\t\tnode, err = &value, e\n\t} else if id.IsBranch() {\n\t\tvalue, e := s.branches.Get(id.Index())\n\t\tnode, err = &value, e\n\t} else if id.IsExtension() {\n\t\tvalue, e := s.extensions.Get(id.Index())\n\t\tnode, err = &value, e\n\t} else if id.IsEmpty() {\n\t\tnode = EmptyNode{}\n\t}\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Everything loaded from the stock is in sync and thus clean.\n\tnode.MarkClean()\n\n\t// Everything that is loaded from an archive is to be considered\n\t// frozen, and thus immutable.\n\tif s.storageMode == Immutable {\n\t\tnode.MarkFrozen()\n\t}\n\n\t// if there has been a concurrent fetch, use the other value\n\tinstance, _ := s.addToCacheHoldingTransferMutex(ref, shared.MakeShared[Node](node))\n\treturn instance, nil\n}\n\nfunc getAccess[H any](\n\tf *Forest,\n\tref *NodeReference,\n\tgetAccess func(*shared.Shared[Node]) H,\n\trelease func(H),\n\tdef H,\n) (H, error) {\n\tinstance, err := f.getSharedNode(ref)\n\tif err != nil {\n\t\treturn def, err\n\t}\n\tfor {\n\t\t// Obtain needed access and make sure the instance access was obtained\n\t\t// for is still valid (by re-fetching the instance and check that it\n\t\t// has not changed). This is not super efficient, and may be improved\n\t\t// in the future by merging this functionality into called operations.\n\t\tres := getAccess(instance)\n\t\tif actual, err := f.getSharedNode(ref); err == nil && actual == instance {\n\t\t\treturn res, nil\n\t\t} else if err != nil {\n\t\t\trelease(res)\n\t\t\treturn def, err\n\t\t} else {\n\t\t\trelease(res)\n\t\t\tinstance = actual\n\t\t}\n\t}\n}\n\nfunc (s *Forest) getReadAccess(ref *NodeReference) (shared.ReadHandle[Node], error) {\n\treturn getAccess(s, ref,\n\t\tfunc(s *shared.Shared[Node]) shared.ReadHandle[Node] {\n\t\t\treturn s.GetReadHandle()\n\t\t},\n\t\tfunc(p shared.ReadHandle[Node]) {\n\t\t\tp.Release()\n\t\t},\n\t\tshared.ReadHandle[Node]{},\n\t)\n}\n\nfunc (s *Forest) getViewAccess(ref *NodeReference) (shared.ViewHandle[Node], error) {\n\treturn getAccess(s, ref,\n\t\tfunc(s *shared.Shared[Node]) shared.ViewHandle[Node] {\n\t\t\treturn s.GetViewHandle()\n\t\t},\n\t\tfunc(p shared.ViewHandle[Node]) {\n\t\t\tp.Release()\n\t\t},\n\t\tshared.ViewHandle[Node]{},\n\t)\n}\n\nfunc (s *Forest) getHashAccess(ref *NodeReference) (shared.HashHandle[Node], error) {\n\treturn getAccess(s, ref,\n\t\tfunc(s *shared.Shared[Node]) shared.HashHandle[Node] {\n\t\t\treturn s.GetHashHandle()\n\t\t},\n\t\tfunc(p shared.HashHandle[Node]) {\n\t\t\tp.Release()\n\t\t},\n\t\tshared.HashHandle[Node]{},\n\t)\n}\n\nfunc (f *Forest) getWriteAccess(ref *NodeReference) (shared.WriteHandle[Node], error) {\n\treturn getAccess(f, ref,\n\t\tfunc(s *shared.Shared[Node]) shared.WriteHandle[Node] {\n\t\t\t// When gaining write access to nodes, they need to be touched to make sure\n\t\t\t// modified nodes are at the head of the cache's LRU queue to be evicted last.\n\t\t\tf.nodeCache.Touch(ref)\n\t\t\treturn s.GetWriteHandle()\n\t\t},\n\t\tfunc(p shared.WriteHandle[Node]) {\n\t\t\tp.Release()\n\t\t},\n\t\tshared.WriteHandle[Node]{},\n\t)\n}\n\nfunc (s *Forest) getMutableNodeByPath(root *NodeReference, path NodePath) (shared.WriteHandle[Node], error) {\n\t// Navigate down the trie using read access.\n\tnext := root\n\tlast := shared.ReadHandle[Node]{}\n\tlastValid := false\n\tfor i := 0; i < path.Length(); i++ {\n\t\tcur, err := s.getReadAccess(next)\n\t\tif lastValid {\n\t\t\tlast.Release()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn shared.WriteHandle[Node]{}, err\n\t\t}\n\t\tlast = cur\n\t\tlastValid = true\n\t\tswitch n := cur.Get().(type) {\n\t\tcase *BranchNode:\n\t\t\tnext = &n.children[path.Get(byte(i))]\n\t\tcase *AccountNode:\n\t\t\tnext = &n.storage\n\t\tcase *ExtensionNode:\n\t\t\tnext = &n.next\n\t\tdefault:\n\t\t\tif lastValid {\n\t\t\t\tlast.Release()\n\t\t\t}\n\t\t\treturn shared.WriteHandle[Node]{}, fmt.Errorf(\"no node for path: %v\", path)\n\t\t}\n\t}\n\n\t// The last step requires write access.\n\tres, err := s.getWriteAccess(next)\n\tif lastValid {\n\t\tlast.Release()\n\t}\n\treturn res, err\n}\n\nfunc (s *Forest) addToCache(ref *NodeReference, node *shared.Shared[Node]) (value *shared.Shared[Node], present bool) {\n\ts.nodeTransferMutex.Lock()\n\tdefer s.nodeTransferMutex.Unlock()\n\treturn s.addToCacheHoldingTransferMutex(ref, node)\n}\n\nfunc (s *Forest) addToCacheHoldingTransferMutex(ref *NodeReference, node *shared.Shared[Node]) (value *shared.Shared[Node], present bool) {\n\t// Replacing the element in the already thread safe node cache needs to be\n\t// guarded by the `getTransferMutex` since an evicted node have to\n\t// be moved to the write buffer in an atomic step.\n\tcurrent, present, evictedId, evictedNode, evicted := s.nodeCache.GetOrSet(ref, node)\n\tif present {\n\t\t// If a present element is re-used, it needs to be touched to be at the\n\t\t// head of the cache's LRU queue -- just like a newly inserted node\n\t\t// would be. Methods like createBranch depend on this to be covered here.\n\t\ts.nodeCache.Touch(ref)\n\t}\n\tif !evicted {\n\t\treturn current, present\n\t}\n\n\t// Clean nodes can be ignored, dirty nodes need to be written.\n\tif handle, ok := evictedNode.TryGetViewHandle(); ok {\n\t\tdirty := handle.Get().IsDirty()\n\t\thandle.Release()\n\t\tif !dirty {\n\t\t\treturn current, present\n\t\t}\n\t}\n\n\t// Enqueue evicted node for asynchronous write to file.\n\ts.writeBuffer.Add(evictedId, evictedNode)\n\treturn current, present\n}\n\nfunc (s *Forest) flushNode(id NodeId, node Node) error {\n\t// Note: flushing nodes in Archive mode will implicitly freeze them,\n\t// since after the reload they will be considered frozen. This may\n\t// cause temporary states between updates to be accidentally frozen,\n\t// leaving unreferenced nodes in the archive, but it is not causing\n\t// correctness issues. However, if the node-cache size is sufficiently\n\t// large, such cases should be rare. Nevertheless, a warning is\n\t// printed here to get informed if this changes in the future.\n\tif printWarningDefaultNodeFreezing && s.storageMode == Immutable && !node.IsFrozen() {\n\t\tlog.Printf(\"WARNING: non-frozen node flushed to disk causing implicit freeze\")\n\t}\n\n\tif id.IsValue() {\n\t\treturn s.values.Set(id.Index(), *node.(*ValueNode))\n\t} else if id.IsAccount() {\n\t\treturn s.accounts.Set(id.Index(), *node.(*AccountNode))\n\t} else if id.IsBranch() {\n\t\treturn s.branches.Set(id.Index(), *node.(*BranchNode))\n\t} else if id.IsExtension() {\n\t\treturn s.extensions.Set(id.Index(), *node.(*ExtensionNode))\n\t}\n\treturn nil\n}\n\nfunc (s *Forest) createAccount() (NodeReference, shared.WriteHandle[Node], error) {\n\ti, err := s.accounts.New()\n\tif err != nil {\n\t\treturn NodeReference{}, shared.WriteHandle[Node]{}, err\n\t}\n\tref := NewNodeReference(AccountId(i))\n\tnode := new(AccountNode)\n\tinstance, present := s.addToCache(&ref, shared.MakeShared[Node](node))\n\tif present {\n\t\twrite := instance.GetWriteHandle()\n\t\t*write.Get().(*AccountNode) = *node\n\t\twrite.Release()\n\t}\n\treturn ref, instance.GetWriteHandle(), err\n}\n\nfunc (s *Forest) createBranch() (NodeReference, shared.WriteHandle[Node], error) {\n\ti, err := s.branches.New()\n\tif err != nil {\n\t\treturn NodeReference{}, shared.WriteHandle[Node]{}, err\n\t}\n\tref := NewNodeReference(BranchId(i))\n\tnode := new(BranchNode)\n\tinstance, present := s.addToCache(&ref, shared.MakeShared[Node](node))\n\tif present {\n\t\twrite := instance.GetWriteHandle()\n\t\t*write.Get().(*BranchNode) = *node\n\t\twrite.Release()\n\t}\n\treturn ref, instance.GetWriteHandle(), err\n}\n\nfunc (s *Forest) createExtension() (NodeReference, shared.WriteHandle[Node], error) {\n\ti, err := s.extensions.New()\n\tif err != nil {\n\t\treturn NodeReference{}, shared.WriteHandle[Node]{}, err\n\t}\n\tref := NewNodeReference(ExtensionId(i))\n\tnode := new(ExtensionNode)\n\tinstance, present := s.addToCache(&ref, shared.MakeShared[Node](node))\n\tif present {\n\t\twrite := instance.GetWriteHandle()\n\t\t*write.Get().(*ExtensionNode) = *node\n\t\twrite.Release()\n\t}\n\treturn ref, instance.GetWriteHandle(), err\n}\n\nfunc (s *Forest) createValue() (NodeReference, shared.WriteHandle[Node], error) {\n\ti, err := s.values.New()\n\tif err != nil {\n\t\treturn NodeReference{}, shared.WriteHandle[Node]{}, err\n\t}\n\tref := NewNodeReference(ValueId(i))\n\tnode := new(ValueNode)\n\tinstance, present := s.addToCache(&ref, shared.MakeShared[Node](node))\n\tif present {\n\t\twrite := instance.GetWriteHandle()\n\t\t*write.Get().(*ValueNode) = *node\n\t\twrite.Release()\n\t}\n\treturn ref, instance.GetWriteHandle(), err\n}\n\nfunc (s *Forest) release(ref *NodeReference) error {\n\t// Released nodes will not be needed,\n\t// so they are moved in the cache to the least priority.\n\t// This way they do not occupy space for other nodes\n\t// written/read in parallel.\n\t// Furthermore, it prevents cache exhaustion when\n\t// deleting many nodes in parallel.\n\t// It fixes: https://github.com/Fantom-foundation/Carmen/issues/691\n\t// If this line is removed, this test fails:\n\t//  go test ./database/mpt/...  -run TestForest_AsyncDelete_CacheIsNotExhausted\n\ts.nodeCache.Release(ref)\n\n\tid := ref.Id()\n\tif id.IsAccount() {\n\t\treturn s.accounts.Delete(id.Index())\n\t}\n\tif id.IsBranch() {\n\t\treturn s.branches.Delete(id.Index())\n\t}\n\tif id.IsExtension() {\n\t\treturn s.extensions.Delete(id.Index())\n\t}\n\tif id.IsValue() {\n\t\treturn s.values.Delete(id.Index())\n\t}\n\treturn fmt.Errorf(\"unable to release node %v\", id)\n}\n\nfunc (s *Forest) releaseTrieAsynchronous(ref NodeReference) {\n\tid := ref.Id()\n\tif !id.IsEmpty() { // empty Id is used for signalling sync requests\n\t\ts.releaseQueue <- id\n\t}\n}\n\nfunc getEncoder(config MptConfig) (\n\tstock.ValueEncoder[AccountNode],\n\tstock.ValueEncoder[BranchNode],\n\tstock.ValueEncoder[ExtensionNode],\n\tstock.ValueEncoder[ValueNode],\n) {\n\tswitch config.HashStorageLocation {\n\tcase HashStoredWithParent:\n\t\tif config.TrackSuffixLengthsInLeafNodes {\n\t\t\treturn AccountNodeWithPathLengthEncoderWithChildHash{},\n\t\t\t\tBranchNodeEncoderWithChildHashes{},\n\t\t\t\tExtensionNodeEncoderWithChildHash{},\n\t\t\t\tValueNodeWithPathLengthEncoderWithoutNodeHash{}\n\t\t}\n\t\treturn AccountNodeEncoderWithChildHash{},\n\t\t\tBranchNodeEncoderWithChildHashes{},\n\t\t\tExtensionNodeEncoderWithChildHash{},\n\t\t\tValueNodeEncoderWithoutNodeHash{}\n\tcase HashStoredWithNode:\n\t\tif config.TrackSuffixLengthsInLeafNodes {\n\t\t\treturn AccountNodeWithPathLengthEncoderWithNodeHash{},\n\t\t\t\tBranchNodeEncoderWithNodeHash{},\n\t\t\t\tExtensionNodeEncoderWithNodeHash{},\n\t\t\t\tValueNodeWithPathLengthEncoderWithNodeHash{}\n\t\t}\n\t\treturn AccountNodeEncoderWithNodeHash{},\n\t\t\tBranchNodeEncoderWithNodeHash{},\n\t\t\tExtensionNodeEncoderWithNodeHash{},\n\t\t\tValueNodeEncoderWithNodeHash{}\n\tdefault:\n\t\tpanic(fmt.Sprintf(\"unknown mode: %v\", config.HashStorageLocation))\n\t}\n}\n\ntype writeBufferSink struct {\n\tforest *Forest\n}\n\nfunc (s writeBufferSink) Write(id NodeId, handle shared.ViewHandle[Node]) error {\n\treturn s.forest.flushNode(id, handle.Get())\n}\n\n// -- Forest metadata --\n\n// ForestMetadata is the helper type to read and write metadata from/to the disk.\ntype ForestMetadata struct {\n\tConfiguration string\n\tMutable       bool\n}\n\n// ReadForestMetadata parses the content of the given file if it exists or returns\n// a default-initialized metadata struct if there is no such file.\nfunc ReadForestMetadata(filename string) (ForestMetadata, bool, error) {\n\n\t// If there is no file, initialize and return default metadata.\n\tif _, err := os.Stat(filename); err != nil {\n\t\treturn ForestMetadata{}, false, nil\n\t}\n\n\t// If the file exists, parse it and return its content.\n\tdata, err := os.ReadFile(filename)\n\tif err != nil {\n\t\treturn ForestMetadata{}, false, err\n\t}\n\n\tvar meta ForestMetadata\n\tif err := json.Unmarshal(data, &meta); err != nil {\n\t\treturn meta, false, err\n\t}\n\treturn meta, true, nil\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/go/database/mpt/forest.go b/go/database/mpt/forest.go
--- a/go/database/mpt/forest.go	(revision cb0032b7724512a252766f973bddcacdc9752c54)
+++ b/go/database/mpt/forest.go	(date 1717749935926)
@@ -332,6 +332,19 @@
 	return info, exists, err
 }
 
+func (s *Forest) HasEmptyStorage(rootRef *NodeReference, addr common.Address) (bool, error) {
+	handle, err := s.getReadAccess(rootRef)
+	if err != nil {
+		err = fmt.Errorf("failed to obtain read access to node %v: %w", rootRef.Id(), err)
+		s.errors = append(s.errors, err)
+		return false, err
+	}
+	defer handle.Release()
+	path := AddressToNibblePath(addr, s)
+	info, exists, err := handle.Get().GetAccount()
+
+}
+
 func (s *Forest) SetAccountInfo(rootRef *NodeReference, addr common.Address, info AccountInfo) (NodeReference, error) {
 	root, err := s.getWriteAccess(rootRef)
 	if err != nil {
Index: go/state/state_db.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright (c) 2024 Fantom Foundation\n//\n// Use of this software is governed by the Business Source License included\n// in the LICENSE file and at fantom.foundation/bsl11.\n//\n// Change Date: 2028-4-16\n//\n// On the date above, in accordance with the Business Source License, use of\n// this software will be governed by the GNU Lesser General Public License v3.\n\n// Deprecated: external users should switch to the carmen package as the new top-level API\npackage state\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"maps\"\n\t\"math/big\"\n\t\"sync\"\n\t\"unsafe\"\n\n\t\"github.com/Fantom-foundation/Carmen/go/common\"\n)\n\n//go:generate mockgen -source state_db.go -destination state_db_mock.go -package state\n\n// VmStateDB defines the basic operations that can be conducted on a StateDB as\n// required by an EVM implementation.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\ntype VmStateDB interface {\n\t// Account management.\n\tCreateAccount(common.Address)\n\tExist(common.Address) bool\n\tEmpty(common.Address) bool\n\n\tSuicide(common.Address) bool\n\tHasSuicided(common.Address) bool\n\n\t// Balance\n\tGetBalance(common.Address) *big.Int\n\tAddBalance(common.Address, *big.Int)\n\tSubBalance(common.Address, *big.Int)\n\n\t// Nonce\n\tGetNonce(common.Address) uint64\n\tSetNonce(common.Address, uint64)\n\n\t// Read and update storage.\n\tGetCommittedState(common.Address, common.Key) common.Value\n\tGetState(common.Address, common.Key) common.Value\n\tSetState(common.Address, common.Key, common.Value)\n\tGetTransientState(common.Address, common.Key) common.Value\n\tSetTransientState(common.Address, common.Key, common.Value)\n\n\t// Code management.\n\tGetCode(common.Address) []byte\n\tSetCode(common.Address, []byte)\n\tGetCodeHash(common.Address) common.Hash\n\tGetCodeSize(common.Address) int\n\n\t// Refund tracking.\n\tAddRefund(uint64)\n\tSubRefund(uint64)\n\tGetRefund() uint64\n\n\t// Log management:\n\t// AddLog adds a log into the current transaction.\n\tAddLog(*common.Log)\n\t// GetLogs provides logs produced in the current transaction.\n\tGetLogs() []*common.Log\n\n\t// Access list tracking.\n\tClearAccessList()\n\tAddAddressToAccessList(common.Address)\n\tAddSlotToAccessList(common.Address, common.Key)\n\tIsAddressInAccessList(common.Address) bool\n\tIsSlotInAccessList(common.Address, common.Key) (addressPresent bool, slotPresent bool)\n\n\t// Transaction scope management.\n\tSnapshot() int\n\tRevertToSnapshot(int)\n\n\tBeginTransaction()\n\tEndTransaction()\n\n\t// GetTransactionChanges provides a set of accounts and their slots, which have been\n\t// potentially changed in the current transaction.\n\t// Must be called before EndTransaction call.\n\tGetTransactionChanges() map[common.Address][]common.Key\n\n\t// Deprecated: not necessary, to be removed\n\tAbortTransaction()\n\n\t// GetHash obtains a cryptographically unique hash of the committed state.\n\tGetHash() common.Hash\n\n\t// Check checks the state of the DB and reports an error if issues have been\n\t// encountered. Check should be called periodically to validate all interactions\n\t// with a StateDB instance. If an error is reported, all operations since the\n\t// last successful check need to be considered invalid.\n\tCheck() error\n}\n\n// StateDB serves as the public interface definition of a Carmen StateDB.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\ntype StateDB interface {\n\tVmStateDB\n\n\tBeginBlock()\n\tEndBlock(number uint64)\n\n\tBeginEpoch()\n\tEndEpoch(number uint64)\n\n\t// Flushes committed state to disk.\n\t// Deprecated: these methods shuold not be called, one statedb inst should not close/flush global databsae\n\tFlush() error\n\tClose() error\n\n\t// StartBulkLoad initiates a bulk load operation by-passing internal caching and\n\t// snapshot, transaction, block, or epoch handling to support faster initialization\n\t// of StateDB instances. All updates of a bulk-load call are committed to the DB\n\t// as a single block with the given block number. Bulk-loads may only be started\n\t// outside the scope of any block.\n\tStartBulkLoad(block uint64) BulkLoad\n\n\t// GetArchiveStateDB provides a historical state view for given block.\n\t// An error is returned if the archive is not enabled or if it is empty.\n\tGetArchiveStateDB(block uint64) (NonCommittableStateDB, error)\n\n\t// GetArchiveBlockHeight provides the last block height available in the archive.\n\t// An empty archive is signaled by an extra return value. An error is returned if the\n\t// archive is not enabled or some other issue has occurred.\n\tGetArchiveBlockHeight() (height uint64, empty bool, err error)\n\n\t// GetMemoryFootprint computes an approximation of the memory used by this state.\n\tGetMemoryFootprint() *common.MemoryFootprint\n\n\tResetBlockContext()\n}\n\n// NonCommittableStateDB is the public interface offered for views on states that can not\n// be permanently modified. The prime example for those are views on historic blocks backed\n// by an archive. While volatile transaction internal changes are supported, there is no\n// way offered for committing those.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\ntype NonCommittableStateDB interface {\n\tVmStateDB\n\n\t// Copy creates a copy of the StateDB, including all uncommitted changes.\n\t// Should be used only in-between transactions, as the tx context is not copied.\n\t// Any change to the copy does not affect the original StateDB, except the state caches.\n\t// Available for non-committable states only, as a commit to the backing state\n\t// makes all other StateDBs with the same backing state invalid.\n\tCopy() NonCommittableStateDB\n\n\t// Release should be called whenever this instance is no longer needed to allow\n\t// held resources to be reused for future requests. After the release, no more\n\t// operations may be conducted on this StateDB instance.\n\tRelease()\n}\n\n// BulkLoad serves as the public interface for loading preset data into the state DB.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\ntype BulkLoad interface {\n\tCreateAccount(common.Address)\n\tSetBalance(common.Address, *big.Int)\n\tSetNonce(common.Address, uint64)\n\tSetState(common.Address, common.Key, common.Value)\n\tSetCode(common.Address, []byte)\n\tClose() error\n}\n\n// stateDB is the internal implementation of the StateDB interface.\ntype stateDB struct {\n\t// The underlying state data is read/written to.\n\tstate State\n\n\t// A transaction local cache for account states to avoid double-fetches and support rollbacks.\n\taccounts map[common.Address]*accountState\n\n\t// A transaction local cache of balances to avoid double-fetches and support rollbacks.\n\tbalances map[common.Address]*balanceValue\n\n\t// A transaction local cache of nonces to avoid double-fetches and support rollbacks.\n\tnonces map[common.Address]*nonceValue\n\n\t// A transaction local cache of storage values to avoid double-fetches and support rollbacks.\n\tdata *common.FastMap[slotId, *slotValue]\n\n\t// Transient storage is a temporary storage that gets deleted after each transaction.\n\ttransientStorage *common.FastMap[slotId, common.Value]\n\n\t// A transaction local cache of contract codes and their properties.\n\tcodes map[common.Address]*codeValue\n\n\t// A list of accounts to be deleted at the end of the transaction.\n\taccountsToDelete []common.Address\n\n\t// Tracks the clearing state of individual accounts.\n\tclearedAccounts map[common.Address]accountClearingState\n\n\t// A list of operations undoing modifications applied on the inner state if a snapshot revert needs to be performed.\n\tundo []func()\n\n\t// The refund accumulated in the current transaction.\n\trefund uint64\n\n\t// The list of log messages recorded for the current transaction.\n\tlogs []*common.Log\n\n\t// The amount of logs in the current block.\n\tlogsInBlock uint\n\n\t// A set of accessed addresses in the current transaction.\n\taccessedAddresses map[common.Address]bool\n\n\t// A set of accessed slots in the current transaction.\n\taccessedSlots *common.FastMap[slotId, bool]\n\n\t// A set of slots with current value (possibly) different from the committed value - for needs of committing.\n\twrittenSlots map[*slotValue]bool\n\n\t// A non-transactional local cache of stored storage values.\n\tstoredDataCache *common.LruCache[slotId, storedDataCacheValue]\n\n\t// A non-transactional reincarnation counter for accounts. This is used to efficiently invalidate data in\n\t// the storedDataCache upon account deletion. The maintained values are internal information only.\n\treincarnation map[common.Address]uint64\n\n\t// A list of addresses, which have possibly become empty in the transaction\n\temptyCandidates []common.Address\n\n\t// True, if this state DB is allowed to apply changes to the underlying state, false otherwise.\n\tcanApplyChanges bool\n\n\t// A list of errors encountered during DB interactions.\n\terrors []error\n}\n\ntype accountLifeCycleState int\n\n// The life-cycle states of an account as seen by the StateDB\n//  - unknown     ... the state has not been fetched from the DB; only valid for the original field in the account state\n//  - NonExisting ... the account is known to not exist\n//  - Exists      ... the account is known to exist\n//  - Suicided    ... the account existed during the current transaction, but suicided\n//\n// The following transitions are allowed:\n//\n//    Unknown -- Load --> NonExisting\n//    Unknown -- Load --> Exists\n//\n//    NonExisting -- CreateAccount --> Exists\n//\n//    Exists -- CreateAccount --> Exists\n//    Exists -- Suicide --> Suicided\n//    Exists -- EndTransaction --> NonExisting    // if account was empty\n//\n//    Suicided -- CreateAccount --> Exists\n//    Suicided -- EndTransaction --> NonExisting\n//\n// Accounts with the state Suicided can only exist during a transaction. At the end of a\n// transaction, Suicided accounts transition automatically into NonExisting accounts.\n\nconst (\n\taccountNonExisting    accountLifeCycleState = 1\n\taccountExists         accountLifeCycleState = 2\n\taccountSelfDestructed accountLifeCycleState = 3\n)\n\nfunc (s accountLifeCycleState) String() string {\n\tswitch s {\n\tcase accountNonExisting:\n\t\treturn \"NonExisting\"\n\tcase accountExists:\n\t\treturn \"Exists\"\n\tcase accountSelfDestructed:\n\t\treturn \"Suicided\"\n\t}\n\treturn \"?\"\n}\n\n// accountState maintains the state of an account during a transaction.\ntype accountState struct {\n\t// The committed account state, set to kUnknown if never fetched.\n\toriginal accountLifeCycleState\n\t// The current account state visible to the state DB users.\n\tcurrent accountLifeCycleState\n}\n\ntype accountClearingState int\n\nconst (\n\t// noClearing is the state of an account not be to cleared (make sure this has the default value 0)\n\tnoClearing accountClearingState = 0\n\t// pendingClearing is the state of an account that is scheduled for clearing at the end of the current transaction but should still appear like it exists.\n\tpendingClearing accountClearingState = 1\n\t// cleared is the state of an account that should appear as it has been cleared.\n\tcleared accountClearingState = 2\n\t// same as cleared, but some SetState has been invoked on the account after its cleaning. So the cached state may be tainted.\n\tclearedAndTainted accountClearingState = 3\n)\n\nfunc (s accountClearingState) String() string {\n\tswitch s {\n\tcase noClearing:\n\t\treturn \"noClearing\"\n\tcase pendingClearing:\n\t\treturn \"pendingClearing\"\n\tcase cleared:\n\t\treturn \"cleared\"\n\tcase clearedAndTainted:\n\t\treturn \"clearedAndTainted\"\n\t}\n\treturn \"?\"\n}\n\n// balanceVale maintains a balance during a transaction.\ntype balanceValue struct {\n\t// The committed balance of an account, missing if never fetched.\n\toriginal *big.Int\n\t// The current value of the account balance visible to the state DB users.\n\tcurrent big.Int\n}\n\n// nonceValue maintains a nonce during a transaction.\ntype nonceValue struct {\n\t// The committed nonce of an account, missing if never fetched.\n\toriginal *uint64\n\t// The current nonce of an account visible to the state DB users.\n\tcurrent uint64\n}\n\n// slotId identifies a storage location.\ntype slotId struct {\n\taddr common.Address\n\tkey  common.Key\n}\n\ntype slotHasher struct{}\n\nfunc (h slotHasher) Hash(id slotId) uint16 {\n\treturn uint16(id.addr[19])<<8 | uint16(id.key[31])\n}\n\nfunc (s *slotId) Compare(other *slotId) int {\n\tc := s.addr.Compare(&other.addr)\n\tif c < 0 {\n\t\treturn -1\n\t}\n\tif c > 0 {\n\t\treturn 1\n\t}\n\treturn s.key.Compare(&other.key)\n}\n\n// slotValue maintains the value of a slot.\ntype slotValue struct {\n\t// The value in the DB, missing if never fetched.\n\tstored common.Value\n\t// The value committed by the last completed transaction.\n\tcommitted common.Value\n\t// The current value as visible to the state DB users.\n\tcurrent common.Value\n\t// Whether the stored value is known.\n\tstoredKnown bool\n\t// Whether the committed value is known.\n\tcommittedKnown bool\n}\n\n// codeValue maintains the code associated to a given address.\ntype codeValue struct {\n\tcode      []byte\n\tsize      int\n\thash      *common.Hash\n\tdirty     bool // < set if code has been updated in transaction\n\tcodeValid bool // < set if code is loaded from the state (or written as dirty)\n\tsizeValid bool // < set if size is loaded from the state (or written as dirty)\n}\n\nconst defaultStoredDataCacheSize = 1000000 // ~ 100 MiB of memory for this cache.\nconst nonCommittableStoredDataCacheSize = 100\n\n// storedDataCacheValue maintains the cached version of a value in the store. To\n// support the efficient clearing of values cached for accounts being deleted, an\n// additional account reincarnation counter is added.\ntype storedDataCacheValue struct {\n\tvalue         common.Value // < the cached version of the value in the store\n\treincarnation uint64       // < the reincarnation the cached value belongs to\n}\n\n// CreateStateDBUsing creates a StateDB instance wrapping the given state supporting\n// all operations including end-of-block operations mutating the underlying state.\n// Note: any StateDB instanced becomes invalid if the underlying state is\n// modified by any other StateDB instance or through any other direct modification.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\nfunc CreateStateDBUsing(state State) StateDB {\n\treturn CreateCustomStateDBUsing(state, defaultStoredDataCacheSize)\n}\n\n// CreateCustomStateDBUsing is the same as CreateStateDBUsing but allows the caller to specify\n// the capacity of the stored Data cache used in the resulting instance. The default\n// cache size used by CreateCustomStateDBUsing may be too large if StateDB instances\n// only have a short live time. In such cases, the initialization and destruction of\n// the maintained data cache may dominate execution time.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\nfunc CreateCustomStateDBUsing(state State, storedDataCacheSize int) StateDB {\n\tif storedDataCacheSize <= 0 {\n\t\tstoredDataCacheSize = defaultStoredDataCacheSize\n\t}\n\treturn createStateDBWith(state, storedDataCacheSize, true)\n}\n\n// CreateNonCommittableStateDBUsing creates a read-only StateDB instance wrapping\n// the given state supporting all operations specified by the VmStateDB interface.\n// Note: any StateDB instanced becomes invalid if the underlying state is\n// modified by any other StateDB instance or through any other direct modification.\n//\n// Deprecated: external users should switch to the carmen package as the new top-level API\nfunc CreateNonCommittableStateDBUsing(state State) NonCommittableStateDB {\n\t// Since StateDB instances are big objects costly to create we reuse those using\n\t// a pool of objects. However, instances need to be properly reset.\n\tdb := nonCommittableStateDbPool.Get().(*stateDB)\n\tdb.resetState(state)\n\treturn &nonCommittableStateDB{db}\n}\n\nfunc createStateDBWith(state State, storedDataCacheCapacity int, canApplyChanges bool) *stateDB {\n\treturn &stateDB{\n\t\tstate:             state,\n\t\taccounts:          map[common.Address]*accountState{},\n\t\tbalances:          map[common.Address]*balanceValue{},\n\t\tnonces:            map[common.Address]*nonceValue{},\n\t\tdata:              common.NewFastMap[slotId, *slotValue](slotHasher{}),\n\t\ttransientStorage:  common.NewFastMap[slotId, common.Value](slotHasher{}),\n\t\tstoredDataCache:   common.NewLruCache[slotId, storedDataCacheValue](storedDataCacheCapacity),\n\t\treincarnation:     map[common.Address]uint64{},\n\t\tcodes:             map[common.Address]*codeValue{},\n\t\trefund:            0,\n\t\taccessedAddresses: map[common.Address]bool{},\n\t\taccessedSlots:     common.NewFastMap[slotId, bool](slotHasher{}),\n\t\twrittenSlots:      map[*slotValue]bool{},\n\t\taccountsToDelete:  make([]common.Address, 0, 100),\n\t\tundo:              make([]func(), 0, 100),\n\t\tclearedAccounts:   make(map[common.Address]accountClearingState),\n\t\temptyCandidates:   make([]common.Address, 0, 100),\n\t\tcanApplyChanges:   canApplyChanges,\n\t}\n}\n\nfunc (s *stateDB) setAccountState(addr common.Address, state accountLifeCycleState) {\n\ts.Exist(addr) // < make sure s.accounts[addr] is initialized\n\tval, exists := s.accounts[addr]\n\t// exists will be false when calling s.Exists() did not succeed\n\tif !exists || val.current == state {\n\t\treturn\n\t}\n\toldState := val.current\n\tval.current = state\n\ts.undo = append(s.undo, func() {\n\t\tval.current = oldState\n\t})\n}\n\nfunc (s *stateDB) Exist(addr common.Address) bool {\n\tif val, exists := s.accounts[addr]; exists {\n\t\treturn val.current == accountExists || val.current == accountSelfDestructed // self-destructed accounts still exist till the end of the transaction.\n\t}\n\texists, err := s.state.Exists(addr)\n\tif err != nil {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to get account state for %v: %w\", addr, err))\n\t\treturn false\n\t}\n\tstate := accountNonExisting\n\tif exists {\n\t\tstate = accountExists\n\t}\n\ts.accounts[addr] = &accountState{\n\t\toriginal: state,\n\t\tcurrent:  state,\n\t}\n\treturn exists\n}\n\nfunc (s *stateDB) CreateAccount(addr common.Address) {\n\ts.setNonceInternal(addr, 0)\n\ts.setCodeInternal(addr, []byte{})\n\n\texists := s.Exist(addr)\n\ts.setAccountState(addr, accountExists)\n\n\t// Created because touched - will be deleted at the end of the transaction if it stays empty\n\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\n\t// Initialize the balance with 0, unless the account existed before.\n\t// Thus, accounts previously marked as unknown (default) or deleted\n\t// will get their balance reset. In particular, deleted accounts that\n\t// are restored will have an empty balance. However, for accounts that\n\t// already existed before this create call the balance is preserved.\n\tif !exists {\n\t\ts.resetBalance(addr)\n\t}\n\n\t// Reset storage of the account, to purge any potential former values.\n\ts.data.ForEach(func(slot slotId, value *slotValue) {\n\t\tif slot.addr == addr {\n\t\t\t// Support rollback of account creation.\n\t\t\tbackup := *value\n\t\t\ts.undo = append(s.undo, func() {\n\t\t\t\t*value = backup\n\t\t\t})\n\n\t\t\t// Clear cached values.\n\t\t\tvalue.stored = common.Value{}\n\t\t\tvalue.storedKnown = true\n\t\t\tvalue.committed = common.Value{}\n\t\t\tvalue.committedKnown = true\n\t\t\tvalue.current = common.Value{}\n\t\t}\n\t})\n\n\t// Mark account to be treated like if was already committed.\n\toldState := s.clearedAccounts[addr]\n\ts.clearedAccounts[addr] = cleared\n\ts.undo = append(s.undo, func() {\n\t\ts.clearedAccounts[addr] = oldState\n\t})\n}\n\nfunc (s *stateDB) createAccountIfNotExists(addr common.Address) bool {\n\tif s.Exist(addr) {\n\t\treturn false\n\t}\n\ts.setAccountState(addr, accountExists)\n\n\t// Initialize the balance with 0, unless the account existed before.\n\t// Thus, accounts previously marked as unknown (default) or deleted\n\t// will get their balance reset. In particular, deleted accounts that\n\t// are restored will have an empty balance. However, for accounts that\n\t// already existed before this create call the balance is preserved.\n\ts.resetBalance(addr)\n\n\treturn true\n}\n\n// Suicide marks the given account as suicided.\n// This clears the account balance.\n// The account still exist until the state is committed.\nfunc (s *stateDB) Suicide(addr common.Address) bool {\n\tif !s.Exist(addr) {\n\t\treturn false\n\t}\n\n\ts.setAccountState(addr, accountSelfDestructed)\n\n\ts.resetBalance(addr)\n\tdeleteListLength := len(s.accountsToDelete)\n\ts.accountsToDelete = append(s.accountsToDelete, addr)\n\ts.undo = append(s.undo, func() {\n\t\ts.accountsToDelete = s.accountsToDelete[0:deleteListLength]\n\t})\n\n\t// Mark account for clearing to plan its removing on commit and\n\t// to avoid fetching new data into the cache during the ongoing block.\n\toldState := s.clearedAccounts[addr]\n\tif oldState == noClearing {\n\t\ts.clearedAccounts[addr] = pendingClearing\n\t\ts.undo = append(s.undo, func() {\n\t\t\ts.clearedAccounts[addr] = oldState\n\t\t})\n\t}\n\n\treturn true\n}\n\nfunc (s *stateDB) HasSuicided(addr common.Address) bool {\n\tstate := s.accounts[addr]\n\treturn state != nil && state.current == accountSelfDestructed\n}\n\nfunc (s *stateDB) Empty(addr common.Address) bool {\n\t// Defined as balance == nonce == code == 0\n\treturn s.GetBalance(addr).Sign() == 0 && s.GetNonce(addr) == 0 && s.GetCodeSize(addr) == 0\n}\n\nfunc clone(val *big.Int) *big.Int {\n\tres := new(big.Int)\n\tres.Set(val)\n\treturn res\n}\n\nfunc (s *stateDB) GetBalance(addr common.Address) *big.Int {\n\t// Check cache first.\n\tif val, exists := s.balances[addr]; exists {\n\t\treturn clone(&val.current) // Do not hand out a pointer to the internal copy!\n\t}\n\t// Since the value is not present, we need to fetch it from the store.\n\tbalance, err := s.state.GetBalance(addr)\n\tif err != nil {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to load balance for address %v: %w\", addr, err))\n\t\treturn new(big.Int) // We need to return something that allows the VM to continue.\n\t}\n\tres := balance.ToBigInt()\n\ts.balances[addr] = &balanceValue{\n\t\toriginal: res,\n\t\tcurrent:  *res,\n\t}\n\treturn clone(res) // Do not hand out a pointer to the internal copy!\n}\n\nfunc (s *stateDB) AddBalance(addr common.Address, diff *big.Int) {\n\ts.createAccountIfNotExists(addr)\n\n\tif diff == nil || diff.Sign() == 0 {\n\t\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\t\treturn\n\t}\n\tif diff.Sign() < 0 {\n\t\ts.SubBalance(addr, diff.Abs(diff))\n\t\treturn\n\t}\n\n\toldValue := s.GetBalance(addr)\n\tnewValue := new(big.Int).Add(oldValue, diff)\n\n\ts.balances[addr].current = *newValue\n\ts.undo = append(s.undo, func() {\n\t\ts.balances[addr].current = *oldValue\n\t})\n}\n\nfunc (s *stateDB) SubBalance(addr common.Address, diff *big.Int) {\n\ts.createAccountIfNotExists(addr)\n\n\tif diff == nil || diff.Sign() == 0 {\n\t\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\t\treturn\n\t}\n\tif diff.Sign() < 0 {\n\t\ts.AddBalance(addr, diff.Abs(diff))\n\t\treturn\n\t}\n\n\toldValue := s.GetBalance(addr)\n\tnewValue := new(big.Int).Sub(oldValue, diff)\n\n\tif newValue.Sign() == 0 {\n\t\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\t}\n\n\ts.balances[addr].current = *newValue\n\ts.undo = append(s.undo, func() {\n\t\ts.balances[addr].current = *oldValue\n\t})\n}\n\nfunc (s *stateDB) resetBalance(addr common.Address) {\n\tif val, exists := s.balances[addr]; exists {\n\t\tif val.current.Sign() != 0 {\n\t\t\toldValue := val.current\n\t\t\tval.current = *big.NewInt(0)\n\t\t\ts.undo = append(s.undo, func() {\n\t\t\t\tval.current = oldValue\n\t\t\t})\n\t\t}\n\t} else {\n\t\ts.balances[addr] = &balanceValue{\n\t\t\toriginal: nil,\n\t\t\tcurrent:  *big.NewInt(0),\n\t\t}\n\t\ts.undo = append(s.undo, func() {\n\t\t\tdelete(s.balances, addr)\n\t\t})\n\t}\n}\n\nfunc (s *stateDB) GetNonce(addr common.Address) uint64 {\n\t// Check cache first.\n\tif val, exists := s.nonces[addr]; exists {\n\t\treturn val.current\n\t}\n\n\t// Since the value is not present, we need to fetch it from the store.\n\tnonce, err := s.state.GetNonce(addr)\n\tif err != nil {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to load nonce for address %v: %w\", addr, err))\n\t\treturn 0\n\t}\n\tres := nonce.ToUint64()\n\ts.nonces[addr] = &nonceValue{\n\t\toriginal: &res,\n\t\tcurrent:  res,\n\t}\n\treturn res\n}\n\nfunc (s *stateDB) SetNonce(addr common.Address, nonce uint64) {\n\ts.setNonceInternal(addr, nonce)\n\tif s.createAccountIfNotExists(addr) && nonce == 0 {\n\t\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\t}\n}\n\nfunc (s *stateDB) setNonceInternal(addr common.Address, nonce uint64) {\n\tif val, exists := s.nonces[addr]; exists {\n\t\tif val.current != nonce {\n\t\t\toldValue := val.current\n\t\t\tval.current = nonce\n\t\t\ts.undo = append(s.undo, func() {\n\t\t\t\tval.current = oldValue\n\t\t\t})\n\t\t}\n\t} else {\n\t\ts.nonces[addr] = &nonceValue{\n\t\t\toriginal: nil,\n\t\t\tcurrent:  nonce,\n\t\t}\n\t\ts.undo = append(s.undo, func() {\n\t\t\tdelete(s.nonces, addr)\n\t\t})\n\t}\n}\n\nfunc (s *stateDB) GetCommittedState(addr common.Address, key common.Key) common.Value {\n\t// Check cache first.\n\tsid := slotId{addr, key}\n\tval, exists := s.data.Get(sid)\n\tif exists && val.committedKnown {\n\t\treturn val.committed\n\t}\n\t// If the value is not present, fetch it from the store.\n\treturn s.loadStoredState(sid, val)\n}\n\nfunc (s *stateDB) loadStoredState(sid slotId, val *slotValue) common.Value {\n\tif clearingState, found := s.clearedAccounts[sid.addr]; found && (clearingState == cleared || clearingState == clearedAndTainted) {\n\t\t// If the account has been cleared in a committed transaction within the current block,\n\t\t// the effects are not yet updated in the data base. So it must not be read from the DB\n\t\t// before the next block.\n\t\treturn common.Value{}\n\t}\n\treincarnation := s.reincarnation[sid.addr]\n\tvar stored storedDataCacheValue\n\tstored, found := s.storedDataCache.Get(sid)\n\tif !found {\n\t\tvar err error\n\t\tstored.value, err = s.state.GetStorage(sid.addr, sid.key)\n\t\tif err != nil {\n\t\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to load storage location %v/%v: %w\", sid.addr, sid.key, err))\n\t\t\treturn common.Value{}\n\t\t}\n\t\tstored.reincarnation = reincarnation\n\t\ts.storedDataCache.Set(sid, stored)\n\t}\n\t// If the cached value is out-dated, the current value is zero. If the same slot would\n\t// have been updated since the clearing, it would have also been updated in the cache.\n\tif stored.reincarnation < reincarnation {\n\t\tstored.value = common.Value{}\n\t}\n\n\t// Remember the stored value for future accesses.\n\tif val != nil {\n\t\tval.committed, val.committedKnown = stored.value, true\n\t\tval.stored, val.storedKnown = stored.value, true\n\t} else {\n\t\ts.data.Put(sid, &slotValue{\n\t\t\tstored:         stored.value,\n\t\t\tcommitted:      stored.value,\n\t\t\tcurrent:        stored.value,\n\t\t\tstoredKnown:    true,\n\t\t\tcommittedKnown: true,\n\t\t})\n\t}\n\treturn stored.value\n}\n\nfunc (s *stateDB) GetState(addr common.Address, key common.Key) common.Value {\n\t// Check whether the slot is already cached/modified.\n\tsid := slotId{addr, key}\n\tif val, exists := s.data.Get(sid); exists {\n\t\treturn val.current\n\t}\n\t// Fetch missing slot values (will also populate the cache).\n\treturn s.loadStoredState(sid, nil)\n}\n\nfunc (s *stateDB) SetState(addr common.Address, key common.Key, value common.Value) {\n\tif s.createAccountIfNotExists(addr) {\n\t\t// The account was implicitly created and may have to be removed at the end of the block.\n\t\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\t}\n\tsid := slotId{addr, key}\n\tif entry, exists := s.data.Get(sid); exists {\n\t\tif entry.current != value {\n\t\t\toldValue := entry.current\n\t\t\tentry.current = value\n\t\t\ts.writtenSlots[entry] = true\n\t\t\ts.undo = append(s.undo, func() {\n\t\t\t\tentry.current = oldValue\n\t\t\t})\n\t\t}\n\t} else {\n\t\tentry = &slotValue{current: value}\n\t\ts.data.Put(sid, entry)\n\t\ts.writtenSlots[entry] = true\n\t\ts.undo = append(s.undo, func() {\n\t\t\tentry, _ := s.data.Get(sid)\n\t\t\tif entry.committedKnown {\n\t\t\t\tentry.current = entry.committed\n\t\t\t} else {\n\t\t\t\ts.data.Remove(sid)\n\t\t\t}\n\t\t\tdelete(s.writtenSlots, entry)\n\t\t})\n\t}\n\toldState := s.clearedAccounts[addr]\n\tif oldState == cleared {\n\t\ts.clearedAccounts[addr] = clearedAndTainted\n\t\ts.undo = append(s.undo, func() { s.clearedAccounts[addr] = oldState })\n\t}\n}\n\nfunc (s *stateDB) GetTransientState(addr common.Address, key common.Key) common.Value {\n\tsid := slotId{addr, key}\n\tval, _ := s.transientStorage.Get(sid)\n\treturn val\n}\n\nfunc (s *stateDB) SetTransientState(addr common.Address, key common.Key, value common.Value) {\n\tsid := slotId{addr, key}\n\tcurrentValue, _ := s.transientStorage.Get(sid)\n\tif currentValue == value {\n\t\treturn\n\t}\n\n\t// Save previous value for rollbacks\n\toldValue := currentValue\n\ts.undo = append(s.undo, func() {\n\t\ts.transientStorage.Put(sid, oldValue)\n\t})\n\n\ts.transientStorage.Put(sid, value)\n}\n\nfunc (s *stateDB) GetCode(addr common.Address) []byte {\n\tval, exists := s.codes[addr]\n\tif !exists {\n\t\tval = &codeValue{}\n\t\ts.codes[addr] = val\n\t}\n\tif !val.codeValid {\n\t\tcode, err := s.state.GetCode(addr)\n\t\tif err != nil {\n\t\t\ts.errors = append(s.errors, fmt.Errorf(\"unable to obtain code for %v: %w\", addr, err))\n\t\t\treturn nil\n\t\t}\n\t\tval.code, val.codeValid = code, true\n\t\tval.size, val.sizeValid = len(code), true\n\t}\n\treturn val.code\n}\n\nfunc (s *stateDB) SetCode(addr common.Address, code []byte) {\n\ts.createAccountIfNotExists(addr)\n\ts.setCodeInternal(addr, code)\n\tif len(code) == 0 {\n\t\ts.emptyCandidates = append(s.emptyCandidates, addr)\n\t}\n}\n\nfunc (s *stateDB) setCodeInternal(addr common.Address, code []byte) {\n\tval, exists := s.codes[addr]\n\tif !exists {\n\t\tval = &codeValue{dirty: true}\n\t\tval.code, val.codeValid = code, true\n\t\tval.size, val.sizeValid = len(code), true\n\t\ts.codes[addr] = val\n\t\ts.undo = append(s.undo, func() {\n\t\t\tdelete(s.codes, addr)\n\t\t})\n\t} else {\n\t\told := *val\n\t\tval.code, val.codeValid = code, true\n\t\tval.size, val.sizeValid = len(code), true\n\t\tval.hash = nil\n\t\tval.dirty = true\n\t\ts.undo = append(s.undo, func() {\n\t\t\t*(s.codes[addr]) = old\n\t\t})\n\t}\n}\n\nfunc (s *stateDB) GetCodeHash(addr common.Address) common.Hash {\n\t// The hash of the code of a non-existing account is always zero.\n\tif !s.Exist(addr) {\n\t\treturn common.Hash{}\n\t}\n\tval, exists := s.codes[addr]\n\tif !exists {\n\t\tval = &codeValue{}\n\t\ts.codes[addr] = val\n\t}\n\tif val.dirty && val.hash == nil {\n\t\t// If the code is dirty (=uncommitted) the hash needs to be computed on the fly.\n\t\thash := common.GetKeccak256Hash(val.code)\n\t\tval.hash = &hash\n\t}\n\tif val.hash == nil {\n\t\t// hash not loaded, code not dirty - needs to load the hash from the state\n\t\thash, err := s.state.GetCodeHash(addr)\n\t\tif err != nil {\n\t\t\ts.errors = append(s.errors, fmt.Errorf(\"unable to obtain code hash for %v: %w\", addr, err))\n\t\t\treturn common.Hash{}\n\t\t}\n\t\tval.hash = &hash\n\t}\n\treturn *val.hash\n}\n\nfunc (s *stateDB) GetCodeSize(addr common.Address) int {\n\tval, exists := s.codes[addr]\n\tif !exists {\n\t\tval = &codeValue{}\n\t\ts.codes[addr] = val\n\t}\n\tif !val.sizeValid {\n\t\tsize, err := s.state.GetCodeSize(addr)\n\t\tif err != nil {\n\t\t\ts.errors = append(s.errors, fmt.Errorf(\"unable to obtain code size for %v: %w\", addr, err))\n\t\t\treturn 0\n\t\t}\n\t\tval.size, val.sizeValid = size, true\n\t}\n\treturn val.size\n}\n\nfunc (s *stateDB) AddRefund(amount uint64) {\n\told := s.refund\n\ts.refund += amount\n\ts.undo = append(s.undo, func() {\n\t\ts.refund = old\n\t})\n}\nfunc (s *stateDB) SubRefund(amount uint64) {\n\tif amount > s.refund {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to lower refund, attempted to removed %d from current refund %d\", amount, s.refund))\n\t\treturn\n\t}\n\told := s.refund\n\ts.refund -= amount\n\ts.undo = append(s.undo, func() {\n\t\ts.refund = old\n\t})\n}\n\nfunc (s *stateDB) GetRefund() uint64 {\n\treturn s.refund\n}\n\nfunc (s *stateDB) AddLog(log *common.Log) {\n\tsize := len(s.logs)\n\tlog.Index = s.logsInBlock\n\ts.logs = append(s.logs, log)\n\ts.logsInBlock++\n\ts.undo = append(s.undo, func() {\n\t\ts.logs = s.logs[0:size]\n\t\ts.logsInBlock--\n\t})\n}\n\nfunc (s *stateDB) GetLogs() []*common.Log {\n\treturn s.logs\n}\n\nfunc (s *stateDB) ClearAccessList() {\n\tif len(s.accessedAddresses) > 0 {\n\t\ts.accessedAddresses = make(map[common.Address]bool)\n\t}\n\tif s.accessedSlots.Size() > 0 {\n\t\ts.accessedSlots.Clear()\n\t}\n}\n\nfunc (s *stateDB) AddAddressToAccessList(addr common.Address) {\n\t_, found := s.accessedAddresses[addr]\n\tif !found {\n\t\ts.accessedAddresses[addr] = true\n\t\ts.undo = append(s.undo, func() {\n\t\t\tdelete(s.accessedAddresses, addr)\n\t\t})\n\t}\n}\n\nfunc (s *stateDB) AddSlotToAccessList(addr common.Address, key common.Key) {\n\ts.AddAddressToAccessList(addr)\n\tsid := slotId{addr, key}\n\t_, found := s.accessedSlots.Get(sid)\n\tif !found {\n\t\ts.accessedSlots.Put(sid, true)\n\t\ts.undo = append(s.undo, func() {\n\t\t\ts.accessedSlots.Remove(sid)\n\t\t})\n\t}\n}\n\nfunc (s *stateDB) IsAddressInAccessList(addr common.Address) bool {\n\t_, found := s.accessedAddresses[addr]\n\treturn found\n}\n\nfunc (s *stateDB) IsSlotInAccessList(addr common.Address, key common.Key) (addressPresent bool, slotPresent bool) {\n\t_, found := s.accessedSlots.Get(slotId{addr, key})\n\tif found {\n\t\treturn true, true\n\t}\n\treturn s.IsAddressInAccessList(addr), false\n}\n\nfunc (s *stateDB) Snapshot() int {\n\treturn len(s.undo)\n}\n\nfunc (s *stateDB) RevertToSnapshot(id int) {\n\tif id < 0 || len(s.undo) < id {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to revert to invalid snapshot id %d, allowed range 0 - %d\", id, len(s.undo)))\n\t\treturn\n\t}\n\tfor len(s.undo) > id {\n\t\ts.undo[len(s.undo)-1]()\n\t\ts.undo = s.undo[:len(s.undo)-1]\n\t}\n}\n\nfunc (s *stateDB) BeginTransaction() {\n\t// Ignored\n}\n\nfunc (s *stateDB) EndTransaction() {\n\t// Updated committed state of storage.\n\tfor value := range s.writtenSlots {\n\t\tvalue.committed, value.committedKnown = value.current, true\n\t}\n\n\t// EIP-161: At the end of the transaction, any account touched by the execution of that transaction\n\t// which is now empty SHALL instead become non-existent (i.e. deleted).\n\tfor _, addr := range s.emptyCandidates {\n\t\tif s.Empty(addr) {\n\t\t\ts.accountsToDelete = append(s.accountsToDelete, addr)\n\t\t\t// Mark the account storage state to be cleaned below.\n\t\t\ts.clearedAccounts[addr] = pendingClearing\n\t\t}\n\t}\n\n\t// Delete accounts scheduled for deletion - by suicide or because they are empty.\n\tif len(s.accountsToDelete) > 0 {\n\t\tfor _, addr := range s.accountsToDelete {\n\t\t\t// Transition accounts marked by suicide to be deleted.\n\t\t\tif s.HasSuicided(addr) {\n\t\t\t\ts.setAccountState(addr, accountNonExisting)\n\t\t\t\ts.setCodeInternal(addr, []byte{})\n\t\t\t\ts.clearedAccounts[addr] = pendingClearing\n\t\t\t}\n\n\t\t\t// If the account was already cleared because it was recreated, we skip this part.\n\t\t\tif state, found := s.clearedAccounts[addr]; found && (state == cleared || state == clearedAndTainted) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\t// Note: storage state is handled through the clearedAccount map\n\t\t\t// the clearing of the data and storedDataCache at various phases\n\t\t\t// of the block processing.\n\t\t\ts.setAccountState(addr, accountNonExisting)\n\t\t\ts.resetBalance(addr) // reset balance if balance is set after suicide\n\t\t\ts.setNonceInternal(addr, 0)\n\t\t\ts.setCodeInternal(addr, []byte{})\n\n\t\t\t// Clear cached value states for the targeted account.\n\t\t\ts.data.ForEach(func(slot slotId, value *slotValue) {\n\t\t\t\tif slot.addr == addr {\n\t\t\t\t\t// Clear cached values.\n\t\t\t\t\tvalue.stored = common.Value{}\n\t\t\t\t\tvalue.storedKnown = true\n\t\t\t\t\tvalue.committed = common.Value{}\n\t\t\t\t\tvalue.committedKnown = true\n\t\t\t\t\tvalue.current = common.Value{}\n\t\t\t\t}\n\t\t\t})\n\n\t\t\t// Signal to future fetches in this block that this account should be considered cleared.\n\t\t\ts.clearedAccounts[addr] = cleared\n\t\t}\n\n\t\ts.accountsToDelete = s.accountsToDelete[0:0]\n\t}\n\n\ts.writtenSlots = map[*slotValue]bool{}\n\t// Reset state, in particular seal effects by forgetting undo list.\n\ts.resetTransactionContext()\n}\n\nfunc (s *stateDB) GetTransactionChanges() map[common.Address][]common.Key {\n\tchanges := make(map[common.Address][]common.Key)\n\tfor addr := range s.accounts {\n\t\tchanges[addr] = nil\n\t}\n\tfor addr := range s.balances {\n\t\tchanges[addr] = nil\n\t}\n\tfor addr := range s.nonces {\n\t\tchanges[addr] = nil\n\t}\n\tfor addr := range s.codes {\n\t\tchanges[addr] = nil\n\t}\n\ts.data.ForEach(func(slot slotId, value *slotValue) {\n\t\tif !value.committedKnown || value.committed != value.current {\n\t\t\tchanges[slot.addr] = append(changes[slot.addr], slot.key)\n\t\t}\n\t})\n\treturn changes\n}\n\n// Deprecated: not necessary, to be removed\nfunc (s *stateDB) AbortTransaction() {\n\t// Revert all effects and reset transaction context.\n\ts.RevertToSnapshot(0)\n\ts.resetTransactionContext()\n}\n\nfunc (s *stateDB) BeginBlock() {\n\t// ignored\n}\n\nfunc (s *stateDB) EndBlock(block uint64) {\n\tif !s.canApplyChanges {\n\t\terr := fmt.Errorf(\"unable to process EndBlock event in StateDB without permission to apply changes\")\n\t\ts.errors = append(s.errors, err)\n\t\treturn\n\t}\n\n\t// Skip applying changes if there have been any issues.\n\tif err := s.Check(); err != nil {\n\t\treturn\n\t}\n\n\tupdate := common.Update{}\n\n\t// Clear all accounts that have been deleted at some point during this block.\n\t// This will cause all storage slots of that accounts to be reset before new\n\t// values may be written in the subsequent updates.\n\tnonExistingAccounts := map[common.Address]bool{}\n\tfor addr, clearingState := range s.clearedAccounts {\n\t\tif clearingState == cleared || clearingState == clearedAndTainted {\n\t\t\tif s.accounts[addr].original == accountExists {\n\t\t\t\t// Pretend this account was originally deleted, such that in the loop below\n\t\t\t\t// it would be detected as re-created in case its new state is Existing.\n\t\t\t\ts.accounts[addr].original = accountNonExisting\n\t\t\t\t// If the account was not later re-created, we mark it for deletion.\n\t\t\t\tif s.accounts[addr].current != accountExists {\n\t\t\t\t\tupdate.AppendDeleteAccount(addr)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tnonExistingAccounts[addr] = true\n\t\t\t}\n\t\t\t// Increment the reincarnation counter of cleared addresses to invalidate\n\t\t\t// cached entries in the stored data cache.\n\t\t\ts.reincarnation[addr] = s.reincarnation[addr] + 1\n\t\t}\n\t}\n\n\t// (Re-)create new or resurrected accounts.\n\tfor addr, value := range s.accounts {\n\t\tif value.original != value.current {\n\t\t\tif value.current == accountExists {\n\t\t\t\tupdate.AppendCreateAccount(addr)\n\t\t\t\tdelete(nonExistingAccounts, addr)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Update balances.\n\tfor addr, value := range s.balances {\n\t\tif _, found := nonExistingAccounts[addr]; found {\n\t\t\tcontinue\n\t\t}\n\t\tif value.original == nil || value.original.Cmp(&value.current) != 0 {\n\t\t\tnewBalance, err := common.ToBalance(&value.current)\n\t\t\tif err != nil {\n\t\t\t\ts.errors = append(s.errors, fmt.Errorf(\"unable to convert big.Int balance %v to common.Balance: %w\", &value.current, err))\n\t\t\t} else {\n\t\t\t\tupdate.AppendBalanceUpdate(addr, newBalance)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Update nonces.\n\tfor addr, value := range s.nonces {\n\t\tif _, found := nonExistingAccounts[addr]; found {\n\t\t\tcontinue\n\t\t}\n\t\tif value.original == nil || *value.original != value.current {\n\t\t\tupdate.AppendNonceUpdate(addr, common.ToNonce(s.nonces[addr].current))\n\t\t}\n\t}\n\n\t// Update storage values in state DB\n\ts.data.ForEach(func(slot slotId, value *slotValue) {\n\t\tif !value.storedKnown || value.stored != value.current {\n\t\t\tupdate.AppendSlotUpdate(slot.addr, slot.key, value.current)\n\t\t\ts.storedDataCache.Set(slot, storedDataCacheValue{value.current, s.reincarnation[slot.addr]})\n\t\t}\n\t})\n\n\t// Update modified codes.\n\tfor addr, value := range s.codes {\n\t\tif _, found := nonExistingAccounts[addr]; found {\n\t\t\tcontinue\n\t\t}\n\t\tif value.dirty {\n\t\t\tupdate.AppendCodeUpdate(addr, s.codes[addr].code)\n\t\t}\n\t}\n\n\t// Skip applying changes if there have been any issues.\n\tif err := s.Check(); err != nil {\n\t\treturn\n\t}\n\n\t// Send the update to the state.\n\tif err := s.state.Apply(block, update); err != nil {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to apply update for block %d: %w\", block, err))\n\t\treturn\n\t}\n\n\t// Reset internal state for next block\n\ts.ResetBlockContext()\n}\n\nfunc (s *stateDB) BeginEpoch() {\n\t// ignored\n}\n\nfunc (s *stateDB) EndEpoch(uint64) {\n\t// ignored\n}\n\nfunc (s *stateDB) GetHash() common.Hash {\n\thash, err := s.state.GetHash()\n\tif err != nil {\n\t\ts.errors = append(s.errors, fmt.Errorf(\"failed to compute hash: %w\", err))\n\t\treturn common.Hash{}\n\t}\n\treturn hash\n}\n\nfunc (s *stateDB) Check() error {\n\treturn errors.Join(\n\t\terrors.Join(s.errors...),\n\t\ts.state.Check())\n}\n\nfunc (s *stateDB) Flush() error {\n\treturn errors.Join(\n\t\ts.Check(),\n\t\ts.state.Flush(),\n\t)\n}\n\nfunc (s *stateDB) Close() error {\n\treturn errors.Join(\n\t\ts.Flush(),\n\t\ts.state.Close(),\n\t)\n}\n\nfunc (s *stateDB) StartBulkLoad(block uint64) BulkLoad {\n\ts.storedDataCache.Clear()\n\treturn &bulkLoad{s, common.Update{}, block, nil}\n}\n\nfunc (s *stateDB) GetMemoryFootprint() *common.MemoryFootprint {\n\tconst addressSize = 20\n\tconst keySize = 32\n\tconst hashSize = 32\n\tconst slotIdSize = addressSize + keySize\n\n\tmf := common.NewMemoryFootprint(unsafe.Sizeof(*s))\n\tmf.AddChild(\"state\", s.state.GetMemoryFootprint())\n\n\t// For account-states, balances, and nonces an over-approximation should be sufficient.\n\tmf.AddChild(\"accounts\", common.NewMemoryFootprint(uintptr(len(s.accounts))*(addressSize+unsafe.Sizeof(accountState{})+unsafe.Sizeof(common.AccountState(0)))))\n\tmf.AddChild(\"balances\", common.NewMemoryFootprint(uintptr(len(s.balances))*(addressSize+unsafe.Sizeof(balanceValue{}))))\n\tmf.AddChild(\"nonces\", common.NewMemoryFootprint(uintptr(len(s.nonces))*(addressSize+unsafe.Sizeof(nonceValue{})+8)))\n\tmf.AddChild(\"slots\", common.NewMemoryFootprint(uintptr(s.data.Size())*(slotIdSize+unsafe.Sizeof(slotValue{}))))\n\n\tvar sum uintptr = 0\n\tfor _, value := range s.codes {\n\t\tsum += addressSize\n\t\tif value.hash != nil {\n\t\t\tsum += hashSize\n\t\t}\n\t\tsum += uintptr(len(value.code))\n\t}\n\tmf.AddChild(\"codes\", common.NewMemoryFootprint(sum))\n\n\tvar boolean bool\n\tconst boolSize = unsafe.Sizeof(boolean)\n\tmf.AddChild(\"accessedAddresses\", common.NewMemoryFootprint(uintptr(len(s.accessedAddresses))*(addressSize+boolSize)))\n\tmf.AddChild(\"accessedSlots\", common.NewMemoryFootprint(uintptr(s.accessedSlots.Size())*(slotIdSize+boolSize)))\n\tmf.AddChild(\"writtenSlots\", common.NewMemoryFootprint(uintptr(len(s.writtenSlots))*(boolSize+unsafe.Sizeof(&slotValue{}))))\n\tmf.AddChild(\"storedDataCache\", s.storedDataCache.GetMemoryFootprint(0))\n\tmf.AddChild(\"reincarnation\", common.NewMemoryFootprint(uintptr(len(s.reincarnation))*(addressSize+unsafe.Sizeof(uint64(0)))))\n\tmf.AddChild(\"emptyCandidates\", common.NewMemoryFootprint(uintptr(len(s.emptyCandidates))*(addressSize)))\n\n\treturn mf\n}\n\nfunc (s *stateDB) GetArchiveStateDB(block uint64) (NonCommittableStateDB, error) {\n\tarchiveState, err := s.state.GetArchiveState(block)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn CreateNonCommittableStateDBUsing(archiveState), nil\n}\n\nfunc (s *stateDB) GetArchiveBlockHeight() (uint64, bool, error) {\n\treturn s.state.GetArchiveBlockHeight()\n}\n\nfunc (s *stateDB) resetTransactionContext() {\n\ts.refund = 0\n\ts.ClearAccessList()\n\ts.transientStorage.Clear()\n\ts.undo = s.undo[0:0]\n\ts.emptyCandidates = s.emptyCandidates[0:0]\n\ts.logs = s.logs[0:0]\n}\n\nfunc (s *stateDB) ResetBlockContext() {\n\ts.accounts = make(map[common.Address]*accountState, len(s.accounts))\n\ts.balances = make(map[common.Address]*balanceValue, len(s.balances))\n\ts.nonces = make(map[common.Address]*nonceValue, len(s.nonces))\n\ts.data.Clear()\n\ts.clearedAccounts = make(map[common.Address]accountClearingState)\n\ts.codes = make(map[common.Address]*codeValue)\n\ts.logsInBlock = 0\n\ts.resetTransactionContext()\n}\n\nfunc (s *stateDB) resetState(state State) {\n\ts.ResetBlockContext()\n\ts.storedDataCache.Clear()\n\ts.reincarnation = map[common.Address]uint64{}\n\ts.errors = s.errors[0:0]\n\ts.state = state\n}\n\ntype bulkLoad struct {\n\tdb     *stateDB\n\tupdate common.Update\n\tblock  uint64\n\terrs   []error\n}\n\nfunc (l *bulkLoad) CreateAccount(addr common.Address) {\n\tl.update.AppendCreateAccount(addr)\n}\n\nfunc (l *bulkLoad) SetBalance(addr common.Address, value *big.Int) {\n\tnewBalance, err := common.ToBalance(value)\n\tif err != nil {\n\t\tl.errs = append(l.errs, fmt.Errorf(\"unable to convert big.Int balance to common.Balance: %w\", err))\n\t\treturn\n\t}\n\tl.update.AppendBalanceUpdate(addr, newBalance)\n}\n\nfunc (l *bulkLoad) SetNonce(addr common.Address, value uint64) {\n\tl.update.AppendNonceUpdate(addr, common.ToNonce(value))\n}\n\nfunc (l *bulkLoad) SetState(addr common.Address, key common.Key, value common.Value) {\n\tl.update.AppendSlotUpdate(addr, key, value)\n}\n\nfunc (l *bulkLoad) SetCode(addr common.Address, code []byte) {\n\tl.update.AppendCodeUpdate(addr, code)\n}\n\nfunc (l *bulkLoad) apply() {\n\t// Apply the update to the DB as one new block.\n\tif err := l.update.Normalize(); err != nil {\n\t\tl.errs = append(l.errs, err)\n\t\treturn\n\t}\n\terr := l.db.state.Apply(l.block, l.update)\n\tl.update = common.Update{}\n\tif err != nil {\n\t\tl.errs = append(l.errs, err)\n\t}\n}\n\nfunc (l *bulkLoad) Close() error {\n\tl.apply()\n\t// Return if errors occurred\n\tif l.errs != nil {\n\t\treturn errors.Join(l.errs...)\n\t}\n\n\t// Flush out all inserted data.\n\tif err := l.db.state.Flush(); err != nil {\n\t\treturn err\n\t}\n\t// Compute hash to bring cached hashes up-to-date.\n\t_, err := l.db.state.GetHash()\n\t// Reset state to allow starting bulk-load with existing database.\n\tl.db.resetState(l.db.state)\n\treturn err\n}\n\nvar nonCommittableStateDbPool = sync.Pool{\n\tNew: func() any {\n\t\t// We use a smaller stored-data cache size to support faster initialization\n\t\t// and resetting of instances. NonCommittable instances are expected to live\n\t\t// only for the duration of a few transactions.\n\t\treturn createStateDBWith(nil, nonCommittableStoredDataCacheSize, false)\n\t},\n}\n\ntype nonCommittableStateDB struct {\n\t*stateDB\n}\n\nfunc (db *nonCommittableStateDB) Copy() NonCommittableStateDB {\n\tcp := nonCommittableStateDbPool.Get().(*stateDB)\n\tcp.resetState(db.state)\n\n\tmaps.Copy(cp.accounts, db.accounts)\n\tmaps.Copy(cp.balances, db.balances)\n\tmaps.Copy(cp.nonces, db.nonces)\n\tdb.data.CopyTo(cp.data)\n\tmaps.Copy(cp.codes, db.codes)\n\tmaps.Copy(cp.clearedAccounts, db.clearedAccounts)\n\tmaps.Copy(cp.reincarnation, db.reincarnation)\n\tcp.logsInBlock = db.logsInBlock\n\t// we suppose ended tx - we may skip members,\n\t// which are reset at the end of every tx\n\n\treturn &nonCommittableStateDB{cp}\n}\n\nfunc (db *nonCommittableStateDB) Release() {\n\tif db.stateDB != nil {\n\t\tnonCommittableStateDbPool.Put(db.stateDB)\n\t\tdb.stateDB = nil\n\t}\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/go/state/state_db.go b/go/state/state_db.go
--- a/go/state/state_db.go	(revision cb0032b7724512a252766f973bddcacdc9752c54)
+++ b/go/state/state_db.go	(date 1717686786986)
@@ -53,6 +53,7 @@
 	GetTransientState(common.Address, common.Key) common.Value
 	SetTransientState(common.Address, common.Key, common.Value)
 
+	GetStorageRoot(common.Address) common.Value
 	// Code management.
 	GetCode(common.Address) []byte
 	SetCode(common.Address, []byte)
@@ -242,6 +243,10 @@
 	errors []error
 }
 
+func (s *stateDB) GetStorageRoot(address common.Address) common.Value {
+	s.state.get
+}
+
 type accountLifeCycleState int
 
 // The life-cycle states of an account as seen by the StateDB
Index: go/database/mpt/nodes.go
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>// Copyright (c) 2024 Fantom Foundation\n//\n// Use of this software is governed by the Business Source License included\n// in the LICENSE file and at fantom.foundation/bsl11.\n//\n// Change Date: 2028-4-16\n//\n// On the date above, in accordance with the Business Source License, use of\n// this software will be governed by the GNU Lesser General Public License v3.\n\npackage mpt\n\n//go:generate mockgen -source nodes.go -destination nodes_mocks.go -package mpt\n\nimport (\n\t\"encoding/binary\"\n\t\"errors\"\n\t\"fmt\"\n\t\"github.com/Fantom-foundation/Carmen/go/common\"\n\t\"github.com/Fantom-foundation/Carmen/go/database/mpt/shared\"\n\t\"io\"\n\t\"slices\"\n)\n\n// This file defines the interface and implementation of all node types in a\n// Merkle Patricia Tries (MPT). There are five different types of nodes:\n//\n//  - empty nodes     ... the root node of empty sub-tries\n//  - branch nodes    ... inner trie nodes splitting navigation paths\n//  - extension nodes ... shortcuts for long-sequences of 1-child branches\n//  - account nodes   ... mid-level nodes reached after consuming an address\n//                        path storing account information and being the root\n//                        of the account's storage trie. It can be considered\n//                        the leaf nodes of the state trie and the root of the\n//                        per-account storage tries.\n//  - value nodes     ... leaf-level nodes reached after consuming a key path\n//                        rooted by an account's storage root node.\n//\n// All nodes implement a common interface as defined below. Besides allowing\n// the encoding of account and storage information in the node structure, nodes\n// can also be frozen or released. Frozen nodes can no longer be modified and\n// subsequent modifications cause modifications to be applied on a clone of the\n// targeted node. Releasing nodes frees up allocated resources for itself and\n// all nodes in the sub-tree rooted by the released node.\n//\n// To address nodes during navigation, NodeIds are used.\n//\n// Nodes are designed to be used in Forests, which is a multi-rooted extension\n// of trees. Thus, individual nodes may be part of multiple trees induced by\n// different root nodes in the forest. Tree-shaped MPTs are a special case of\n// a forest with a single root. To avoid unwanted side-effects, all nodes\n// shared as part of multiple trees should be frozen before being shared.\n\n// Node defines an interface for all nodes in the MPT.\ntype Node interface {\n\t// GetAccount retrieves the account information associated to a given\n\t// account. All non-covered accounts have the implicit empty-info\n\t// associated.\n\t// The function requires the following parameters:\n\t//  - source  ... providing abstract access to resolving other nodes\n\t//  - address ... the address of the account to be located\n\t//  - path    ... the remaining path to be navigated to reach the account\n\t// The following results are produced:\n\t//  - info    ... the value associated to the key or zero\n\t//  - exists  ... true if the value is present, false otherwise\n\t//  - err     ... if the resolution of some node failed\n\t// This function is only supported for nodes in the MPT located between\n\t// the root node and an AccountNode.\n\tGetAccount(source NodeSource, address common.Address, path []Nibble) (info AccountInfo, exists bool, err error)\n\n\t// SetAccount updates the AccountInformation associated to a given\n\t// address in this trie. If the new AccountInfo is empty, the\n\t// account and all its storage is deleted.\n\t// The function requires the following parameters:\n\t//  - manager ... to look-up, create, and release nodes\n\t//  - thisId  ... the NodeID of the node this function has been called on\n\t//  - address ... the Address of the account to be updated\n\t//  - path    ... the remaining path to be navigated to reach the account\n\t//  - info    ... the new information to be assigned to the account\n\t// The following results are produced:\n\t//  - newRoot ... the new root of the sub-trie after the update (it may no\n\t//                longer be thisId and callers need to react accordingly)\n\t//  - changed ... true if the content of the sub-trie has changed and, for\n\t//                instance, the node's hash needs to be updated\n\t//  - err     ... if resolving, creating, or releasing nodes failed at some\n\t//                point during the update.\n\t// This function is only supported for nodes in the MPT located between\n\t// the root node and an AccountNode.\n\tSetAccount(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, info AccountInfo) (newRoot NodeReference, changed bool, err error)\n\n\t// GetValue retrieves a value associated to a key in the storage trie\n\t// associated to an account in an MPT. All non-covered locations have the\n\t// implicit zero value.\n\t// The function requires the following parameters:\n\t//  - source  ... providing abstract access to resolving other nodes\n\t//  - key     ... the key of the value to be located\n\t//  - path    ... the remaining path to be navigated to reach the value\n\t// The following results are produced:\n\t//  - value   ... the value associated to the key or zero\n\t//  - exists  ... true if the value is present, false otherwise\n\t//  - err     ... if the resolution of some node failed\n\t// This function is only supported for nodes in the MPT located in a\n\t// storage trie rooted by an AccountNode.\n\tGetValue(source NodeSource, key common.Key, path []Nibble) (value common.Value, exists bool, err error)\n\n\t// SetValue updates the value associated to a given key in the storage\n\t// trie associated to an account in an MPT. If the new value is zero the\n\t// path reaching the value is removed from the MPT.\n\t// The function requires the following parameters:\n\t//  - manager ... to look-up, create, and release nodes\n\t//  - thisId  ... the NodeID of the node this function has been called on\n\t//  - key     ... the key of the value to be updated\n\t//  - path    ... the remaining path to be navigated to reach the value\n\t//  - value    ... the new value to be assigned with the key\n\t// The following results are produced:\n\t//  - newRoot ... the new root of the sub-trie after the update (it may no\n\t//                longer be thisId and callers need to react accordingly)\n\t//  - changed ... true if the content of the sub-trie has changed and, for\n\t//                instance, the node's hash needs to be updated\n\t//  - err     ... if resolving, creating, or releasing nodes failed at some\n\t//                point during the update.\n\t// This function is only supported for nodes in the MPT located in a\n\t// storage trie rooted by an AccountNode.\n\tSetValue(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], key common.Key, path []Nibble, value common.Value) (newRoot NodeReference, changed bool, err error)\n\n\t// GetSlot retrieves a value of a slot addressed by a given key being part\n\t// of a given account. It is a combination of GetAccount() followed by\n\t// GetValue().\n\t// The function requires the following parameters:\n\t//  - source  ... providing abstract access to resolving other nodes\n\t//  - address ... the Address of the account to be updated\n\t//  - key     ... the key of the value to be located\n\t//  - path    ... the remaining path to be navigated to reach the account\n\t//                or, if already passed, the value\n\t// The following results are produced:\n\t//  - value   ... the value associated to the key or zero\n\t//  - exists  ... true if the value is present, false otherwise\n\t//  - err     ... if the resolution of some node failed\n\t// This function is only supported for nodes in the MPT located between\n\t// the root node and an AccountNode.\n\tGetSlot(source NodeSource, address common.Address, path []Nibble, key common.Key) (value common.Value, exists bool, err error)\n\n\t// SetSlot updates a value of a slot addressed by a given key being part\n\t// of a given account. It is a combination of GetAccount() followed by\n\t// SetValue().\n\t// The function requires the following parameters:\n\t//  - manager ... to look-up, create, and release nodes\n\t//  - thisId  ... the NodeID of the node this function has been called on\n\t//  - address ... the Address of the account to be updated\n\t//  - key     ... the key of the value to be updated\n\t//  - path    ... the remaining path to be navigated to reach the account\n\t//                or, if already passed, the value\n\t//  - value   ... the new value to be assigned with the key\n\t// The following results are produced:\n\t//  - newRoot ... the new root of the sub-trie after the update (it may no\n\t//                longer be thisId and callers need to react accordingly)\n\t//  - changed ... true if the content of the sub-trie has changed and, for\n\t//                instance, the node's hash needs to be updated\n\t//  - err     ... if resolving, creating, or releasing nodes failed at some\n\t//                point during the update.\n\t// This function is only supported for nodes in the MPT located between\n\t// the root node and an AccountNode.\n\tSetSlot(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, key common.Key, value common.Value) (newRoot NodeReference, changed bool, err error)\n\n\t// ClearStorage deletes the entire storage associated to an account. For\n\t// parameter information and return values see SetValue().\n\tClearStorage(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble) (newRoot NodeReference, changed bool, err error)\n\n\t// Release releases this node and all non-frozen nodes in the sub-tree\n\t// rooted by this node. Only non-frozen nodes can be released.\n\tRelease(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node]) error\n\n\t// IsDirty returns whether this node's state is different in memory than it\n\t// is on disk. All nodes are created dirty and may only be cleaned by marking\n\t// them as such.\n\tIsDirty() bool\n\n\t// MarkClean marks this node as clean. This function should be called when an\n\t// in memory version of a node got synced with its on-disk copy.\n\tMarkClean()\n\n\t// GetHash obtains the potentially dirty hash currently retained for this node.\n\tGetHash() (hash common.Hash, dirty bool)\n\n\t// SetHash updates this nodes hash.\n\tSetHash(common.Hash)\n\n\t// IsFrozen indicates whether the given node is frozen or not.\n\tIsFrozen() bool\n\n\t// Freeze freezes this node and the entire sub-tree induced by it. After\n\t// freezing the node it can no longer be modified or released.\n\tFreeze(manager NodeManager, this shared.WriteHandle[Node]) error\n\n\t// MarkFrozen marks the current node as frozen, without freezing the\n\t// sub-tree. This might be used when loading frozen nodes from secondary\n\t// storage.\n\tMarkFrozen()\n\n\t// Check verifies internal invariants of this node. It is mainly intended\n\t// to validate invariants in unit tests and for issue diagnostics.\n\tCheck(source NodeSource, thisRef *NodeReference, path []Nibble) error\n\n\t// Dump dumps this node and its sub-trees to the console. It is mainly\n\t// intended for debugging and may be very costly for larger instances.\n\tDump(dest io.Writer, source NodeSource, thisRef *NodeReference, indent string) error\n\n\t// Visit visits this and all nodes in the respective sub-tree. The\n\t// visitor is called by each encountered node, with the proper NodeInfo\n\t// set. Visiting aborts if the visitor returns or prune sub-tree as\n\t// requested by the visitor. The function returns whether the visiting\n\t// process has been aborted and/or an error occurred.\n\tVisit(source NodeSource, thisRef *NodeReference, depth int, visitor NodeVisitor) (abort bool, err error)\n}\n\n// NodeSource is an interface for any object capable of resolving NodeIds into\n// Nodes. It is intended to be implemented by a Node-governing component\n// handling the life-cycle of nodes and loading/storing nodes to persistent\n// storage. It also serves as a central source for trie configuration flags.\ntype NodeSource interface {\n\tgetConfig() MptConfig\n\tgetReadAccess(*NodeReference) (shared.ReadHandle[Node], error)\n\tgetViewAccess(*NodeReference) (shared.ViewHandle[Node], error)\n\tgetHashFor(*NodeReference) (common.Hash, error)\n\thashKey(common.Key) common.Hash\n\thashAddress(address common.Address) common.Hash\n}\n\n// NodeManager is a mutable extension of a NodeSource enabling the creation,\n// update, invalidation, and releasing of nodes.\ntype NodeManager interface {\n\tNodeSource\n\n\tgetHashAccess(*NodeReference) (shared.HashHandle[Node], error)\n\tgetWriteAccess(*NodeReference) (shared.WriteHandle[Node], error)\n\n\tcreateAccount() (NodeReference, shared.WriteHandle[Node], error)\n\tcreateBranch() (NodeReference, shared.WriteHandle[Node], error)\n\tcreateExtension() (NodeReference, shared.WriteHandle[Node], error)\n\tcreateValue() (NodeReference, shared.WriteHandle[Node], error)\n\n\trelease(*NodeReference) error\n\treleaseTrieAsynchronous(NodeReference)\n}\n\n// ----------------------------------------------------------------------------\n//                               Utilities\n// ----------------------------------------------------------------------------\n\n// VisitPathToStorage visits all nodes from the input storage root following the input storage key.\n// Each encountered node is passed to the visitor.\n// If no more nodes are available on the path, the execution ends.\n// If the key does not exist, the function returns false.\n// The function returns an error if the path cannot be iterated due to error propagated from the node source.\n// Nodes provided via the visitor are made available with the view privilege.\nfunc VisitPathToStorage(source NodeSource, storageRoot *NodeReference, key common.Key, visitor NodeVisitor) (bool, error) {\n\tpath := KeyToNibblePath(key, source)\n\treturn visitPathTo(source, storageRoot, path, nil, &key, visitor)\n}\n\n// VisitPathToAccount visits all nodes from the input root following the input account address.\n// Each encountered node is passed to the visitor.\n// If no more nodes are available on the path, the execution ends.\n// If the account address does not exist, the function returns false.\n// The function returns an error if the path cannot be iterated due to error propagated from the node source.\n// Nodes provided via the visitor are made available with the view privilege.\nfunc VisitPathToAccount(source NodeSource, root *NodeReference, address common.Address, visitor NodeVisitor) (bool, error) {\n\tpath := AddressToNibblePath(address, source)\n\treturn visitPathTo(source, root, path, &address, nil, visitor)\n}\n\n// visitPathTo visits all nodes from the input root following the input path.\n// Each encountered node is passed to the visitor.\n// If no more nodes are available on the path, the execution ends.\n// If the path does not exist, the function returns false.\n// The function returns an error if the path cannot be iterated due to error propagated from the node source.\n// When the function reaches either an account node or a value node it is compared to the input address or key.\n// If either the address or key matches the node, this function terminates.\n// It means this function can be used to find either an account node or a value node,\n// but it cannot find both at the same time.\nfunc visitPathTo(source NodeSource, root *NodeReference, path []Nibble, address *common.Address, key *common.Key, visitor NodeVisitor) (bool, error) {\n\tnodeId := root\n\n\tvar last shared.ViewHandle[Node]\n\tvar found, done bool\n\tvar lastNodeId *NodeReference\n\tfor !done {\n\t\thandle, err := source.getViewAccess(nodeId)\n\t\tif last.Valid() {\n\t\t\tlast.Release()\n\t\t}\n\t\tif err != nil {\n\t\t\treturn false, err\n\t\t}\n\t\tlast = handle\n\t\tlastNodeId = nodeId\n\t\tnode := handle.Get()\n\n\t\tswitch n := node.(type) {\n\t\tcase *ExtensionNode:\n\t\t\tif n.path.IsPrefixOf(path) {\n\t\t\t\tnodeId = &n.next\n\t\t\t\tpath = path[n.path.Length():]\n\t\t\t} else {\n\t\t\t\tdone = true\n\t\t\t}\n\t\tcase *BranchNode:\n\t\t\tif len(path) == 0 {\n\t\t\t\tdone = true\n\t\t\t} else {\n\t\t\t\tnodeId = &n.children[path[0]]\n\t\t\t\tpath = path[1:]\n\t\t\t}\n\t\tcase *AccountNode:\n\t\t\tif address != nil && n.address == *address {\n\t\t\t\tfound = true\n\t\t\t}\n\t\t\tdone = true\n\t\tcase *ValueNode:\n\t\t\tif key != nil && n.key == *key {\n\t\t\t\tfound = true\n\t\t\t}\n\t\t\tdone = true\n\t\tdefault:\n\t\t\tdone = true\n\t\t}\n\n\t\t// visit when we are in the middle of the path or when we found the result\n\t\tif !done || found {\n\t\t\tif res := visitor.Visit(last.Get(), NodeInfo{Id: lastNodeId.Id()}); res != VisitResponseContinue {\n\t\t\t\tdone = true\n\t\t\t}\n\t\t}\n\t}\n\n\tlast.Release()\n\treturn found, nil\n}\n\n// CheckForest evaluates invariants throughout all nodes reachable from the\n// given list of roots. Executed checks include node-specific checks like the\n// minimum number of child nodes of a BranchNode, the correct placement of\n// nodes within the forest, and the absence of zero values. The function also\n// checks the proper sharing of nodes in multiple tries rooted by different\n// nodes. A reuse is only valid if the node's position within the respective\n// tries is compatible -- thus, the node is reachable through the same\n// navigation path.\nfunc CheckForest(source NodeSource, roots []*NodeReference) error {\n\t// The check algorithm is based on an iterative depth-first traversal\n\t// where information on encountered nodes is cached to avoid multiple\n\t// evaluations.\n\tworkList := []NodeId{}\n\tcontexts := map[NodeId]nodeCheckContext{}\n\tfor _, ref := range roots {\n\t\tworkList = append(workList, ref.Id())\n\t\tcontexts[ref.Id()] = nodeCheckContext{\n\t\t\troot:           ref.Id(),\n\t\t\thasSeenAccount: false,\n\t\t\tpath:           nil,\n\t\t}\n\t}\n\n\t// scheduleNode verifies that the given node is reached consistently\n\t// with earlier encounters or schedules the node for future checks\n\t// if this is the first time a path to this node was discovered.\n\tscheduleNode := func(ref *NodeReference, root NodeId, accountSeen bool, path []Nibble) error {\n\t\tcontext := nodeCheckContext{\n\t\t\troot:           root,\n\t\t\thasSeenAccount: accountSeen,\n\t\t\tpath:           path,\n\t\t}\n\t\tprevious, found := contexts[ref.Id()]\n\t\tif found {\n\t\t\tif !context.isCompatible(&previous) {\n\t\t\t\treturn fmt.Errorf(\n\t\t\t\t\t\"invalid reuse of node %v: reachable from %v through %v and from %v through %v\",\n\t\t\t\t\tref.Id(), previous.root, previous.path, context.root, context.path,\n\t\t\t\t)\n\t\t\t}\n\t\t\treturn nil\n\t\t} else {\n\t\t\tcontexts[ref.Id()] = context\n\t\t}\n\t\tworkList = append(workList, ref.Id())\n\t\treturn nil\n\t}\n\n\tcount := 0\n\tfor len(workList) > 0 {\n\t\tcurId := workList[len(workList)-1]\n\t\tworkList = workList[:len(workList)-1]\n\n\t\t// TODO [cleanup]: replace this by an observer\n\t\tcount++\n\t\tif count%100000 == 0 {\n\t\t\tfmt.Printf(\"Checking %v (%d), |ws| = %d, |contexts| = %d\\n\", curId, count, len(workList), len(contexts))\n\t\t}\n\n\t\tcontext := contexts[curId]\n\t\tcurNodeRef := NewNodeReference(curId)\n\t\thandle, err := source.getViewAccess(&curNodeRef)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnode := handle.Get()\n\t\terr = node.Check(source, &curNodeRef, context.path)\n\t\tif err != nil {\n\t\t\thandle.Release()\n\t\t\treturn err\n\t\t}\n\n\t\t// schedule child nodes to be checked\n\t\tswitch cur := node.(type) {\n\t\tcase EmptyNode:\n\t\t\t// terminal node without children\n\t\tcase *AccountNode:\n\t\t\tstorage := cur.storage\n\t\t\tif !storage.id.IsEmpty() {\n\t\t\t\terr = scheduleNode(&storage, context.root, true, nil)\n\t\t\t}\n\t\tcase *BranchNode:\n\t\t\tfor i := 0; i < 16; i++ {\n\t\t\t\tchild := cur.children[i]\n\t\t\t\tif !child.id.IsEmpty() {\n\t\t\t\t\tpath := make([]Nibble, len(context.path)+1)\n\t\t\t\t\tcopy(path, context.path)\n\t\t\t\t\tpath[len(context.path)] = Nibble(i)\n\t\t\t\t\tif err = scheduleNode(&child, context.root, context.hasSeenAccount, path); err != nil {\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\tcase *ExtensionNode:\n\t\t\tnext := cur.next\n\t\t\tif !next.id.IsEmpty() {\n\t\t\t\tpath := make([]Nibble, len(context.path), len(context.path)+cur.path.Length())\n\t\t\t\tcopy(path, context.path)\n\t\t\t\tfor i := 0; i < cur.path.Length(); i++ {\n\t\t\t\t\tpath = append(path, cur.path.Get(i))\n\t\t\t\t}\n\t\t\t\terr = scheduleNode(&next, context.root, context.hasSeenAccount, path)\n\t\t\t}\n\t\tcase *ValueNode:\n\t\t\t// terminal node without children\n\t\t\tif !context.hasSeenAccount {\n\t\t\t\terr = fmt.Errorf(\"value node %v is reachable without passing an account\", curNodeRef.Id())\n\t\t\t}\n\t\t}\n\n\t\thandle.Release()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn nil\n}\n\ntype nodeCheckContext struct {\n\troot           NodeId\n\tpath           []Nibble\n\thasSeenAccount bool\n}\n\nfunc (c *nodeCheckContext) isCompatible(other *nodeCheckContext) bool {\n\treturn c.hasSeenAccount == other.hasSeenAccount && slices.Equal(c.path, other.path)\n}\n\n// nodeBase is an optional common base type for nodes.\ntype nodeBase struct {\n\thash       common.Hash // the hash of this node (may be dirty)\n\thashStatus hashStatus  // indicating whether this node's hash is valid\n\tclean      bool        // by default nodes are dirty (clean == false)\n\tfrozen     bool        // a flag marking the node as immutable (default: mutable)\n}\n\ntype hashStatus byte\n\nconst (\n\thashStatusClean   hashStatus = 0 // the hash is up-to-date, matching the nodes content\n\thashStatusDirty   hashStatus = 1 // the hash is out-of-date and needs to be refreshed\n\thashStatusUnknown hashStatus = 2 // the hash is missing / invalid\n)\n\nfunc (s hashStatus) String() string {\n\tswitch s {\n\tcase hashStatusClean:\n\t\treturn \"clean\"\n\tcase hashStatusDirty:\n\t\treturn \"dirty\"\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc (n *nodeBase) GetHash() (common.Hash, bool) {\n\treturn n.hash, n.hashStatus != hashStatusClean\n}\n\nfunc (n *nodeBase) SetHash(hash common.Hash) {\n\tn.hash = hash\n\tn.hashStatus = hashStatusClean\n}\n\nfunc (n *nodeBase) hasCleanHash() bool {\n\treturn n.hashStatus == hashStatusClean\n}\n\nfunc (n *nodeBase) getHashStatus() hashStatus {\n\treturn n.hashStatus\n}\n\nfunc (n *nodeBase) IsFrozen() bool {\n\treturn n.frozen\n}\n\nfunc (n *nodeBase) MarkFrozen() {\n\tn.frozen = true\n}\n\nfunc (n *nodeBase) markMutable() {\n\tn.frozen = false\n}\n\nfunc (n *nodeBase) IsDirty() bool {\n\treturn !n.clean\n}\n\nfunc (n *nodeBase) MarkClean() {\n\tn.clean = true\n}\n\nfunc (n *nodeBase) markDirty() {\n\tn.clean = false\n\tn.hashStatus = hashStatusDirty\n}\n\nfunc (n *nodeBase) Release() {\n\t// The node is disconnected from the disk version and thus clean.\n\tn.clean = true\n\tn.hashStatus = hashStatusClean\n}\n\nfunc (n *nodeBase) check(thisRef *NodeReference) error {\n\tvar errs []error\n\tif !n.IsDirty() && n.hashStatus == hashStatusDirty {\n\t\terrs = append(errs, fmt.Errorf(\"node %v is marked clean but hash is dirty\", thisRef.Id()))\n\t}\n\tif n.IsDirty() && n.hashStatus == hashStatusUnknown {\n\t\terrs = append(errs, fmt.Errorf(\"node %v is marked dirty but hash is marked unknown (should be dirty or clean)\", thisRef.Id()))\n\t}\n\treturn errors.Join(errs...)\n}\n\n// ----------------------------------------------------------------------------\n//                               Empty Node\n// ----------------------------------------------------------------------------\n\n// EmptyNode is the node type used to represent an empty sub-trie. Empty nodes\n// have no state and can thus not be modified. Any modification results in the\n// creation of new nodes representing the new state.\ntype EmptyNode struct{}\n\nfunc (EmptyNode) GetAccount(source NodeSource, address common.Address, path []Nibble) (AccountInfo, bool, error) {\n\treturn AccountInfo{}, false, nil\n}\n\nfunc (EmptyNode) GetValue(NodeSource, common.Key, []Nibble) (common.Value, bool, error) {\n\treturn common.Value{}, false, nil\n}\n\nfunc (EmptyNode) GetSlot(NodeSource, common.Address, []Nibble, common.Key) (common.Value, bool, error) {\n\treturn common.Value{}, false, nil\n}\n\nfunc (e EmptyNode) SetAccount(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, info AccountInfo) (NodeReference, bool, error) {\n\tif info.IsEmpty() {\n\t\treturn *thisRef, false, nil\n\t}\n\tref, handle, err := manager.createAccount()\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tdefer handle.Release()\n\tres := handle.Get().(*AccountNode)\n\tres.markDirty()\n\tres.address = address\n\tres.info = info\n\tres.pathLength = byte(len(path))\n\treturn ref, false, nil\n}\n\nfunc (e EmptyNode) SetValue(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], key common.Key, path []Nibble, value common.Value) (NodeReference, bool, error) {\n\tif value == (common.Value{}) {\n\t\treturn *thisRef, false, nil\n\t}\n\tref, handle, err := manager.createValue()\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tdefer handle.Release()\n\tres := handle.Get().(*ValueNode)\n\tres.key = key\n\tres.value = value\n\tres.markDirty()\n\tres.pathLength = byte(len(path))\n\treturn ref, true, nil\n}\n\nfunc (e EmptyNode) SetSlot(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, key common.Key, value common.Value) (NodeReference, bool, error) {\n\t// We can stop here, since the account does not exist and it should not\n\t// be implicitly created by setting a value.\n\t// Note: this function can only be reached while looking for the account.\n\t// Once the account is reached, the SetValue(..) function is used.\n\treturn *thisRef, false, nil\n}\n\nfunc (e EmptyNode) ClearStorage(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble) (newRoot NodeReference, changed bool, err error) {\n\treturn *thisRef, false, nil\n}\n\nfunc (e EmptyNode) Release(NodeManager, *NodeReference, shared.WriteHandle[Node]) error {\n\treturn nil\n}\n\nfunc (e EmptyNode) IsDirty() bool {\n\treturn false\n}\n\nfunc (e EmptyNode) MarkClean() {}\n\nfunc (e EmptyNode) GetHash() (common.Hash, bool) {\n\t// The hash of an empty node should be defined by the hash algorithm as\n\t// a constant, and not stored in an empty node instance. Thus, the empty\n\t// node is not required to store a hash and if asked for it, it is always\n\t// appearing as to have a dirty hash.\n\treturn common.Hash{}, true\n}\n\nfunc (e EmptyNode) SetHash(common.Hash) { /* ignored */ }\n\nfunc (e EmptyNode) IsFrozen() bool {\n\treturn true\n}\n\nfunc (e EmptyNode) MarkFrozen() {}\n\nfunc (e EmptyNode) Freeze(NodeManager, shared.WriteHandle[Node]) error {\n\t// empty nodes are always frozen\n\treturn nil\n}\n\nfunc (EmptyNode) Check(NodeSource, *NodeReference, []Nibble) error {\n\t// No invariants to be checked.\n\treturn nil\n}\n\nfunc (EmptyNode) Dump(out io.Writer, _ NodeSource, thisRef *NodeReference, indent string) error {\n\tfmt.Fprintf(out, \"%s-empty- (ID: %v)\\n\", indent, thisRef.Id())\n\treturn nil\n}\n\nfunc (EmptyNode) Visit(_ NodeSource, ref *NodeReference, depth int, visitor NodeVisitor) (bool, error) {\n\treturn visitor.Visit(EmptyNode{}, NodeInfo{Id: ref.Id(), Depth: &depth}) == VisitResponseAbort, nil\n}\n\n// ----------------------------------------------------------------------------\n//                               Branch Node\n// ----------------------------------------------------------------------------\n\n// BranchNode implements a node consuming one Nibble along the path from the\n// root to a leaf node in a trie. The Nibble is used to select one out of 16\n// potential child nodes. Each BranchNode has at least 2 non-empty children.\ntype BranchNode struct {\n\tnodeBase\n\tchildren         [16]NodeReference // the ID of child nodes\n\thashes           [16]common.Hash   // the hashes of child nodes\n\tdirtyHashes      uint16            // a bit mask marking hashes as dirty; 0 .. clean, 1 .. dirty\n\tembeddedChildren uint16            // a bit mask marking children as embedded; 0 .. not, 1 .. embedded\n\tfrozenChildren   uint16            // a bit mask marking frozen children; not persisted\n}\n\nfunc (n *BranchNode) getNextNodeInBranch(\n\tsource NodeSource,\n\tpath []Nibble,\n) (shared.ReadHandle[Node], []Nibble, error) {\n\tnext := &n.children[path[0]]\n\tnode, err := source.getReadAccess(next)\n\tif err != nil {\n\t\treturn shared.ReadHandle[Node]{}, nil, err\n\t}\n\treturn node, path[1:], err\n}\n\nfunc (n *BranchNode) GetAccount(source NodeSource, address common.Address, path []Nibble) (AccountInfo, bool, error) {\n\tnext, subPath, err := n.getNextNodeInBranch(source, path)\n\tif err != nil {\n\t\treturn AccountInfo{}, false, err\n\t}\n\tdefer next.Release()\n\treturn next.Get().GetAccount(source, address, subPath)\n}\n\nfunc (n *BranchNode) GetValue(source NodeSource, key common.Key, path []Nibble) (common.Value, bool, error) {\n\tnext, subPath, err := n.getNextNodeInBranch(source, path)\n\tif err != nil {\n\t\treturn common.Value{}, false, err\n\t}\n\tdefer next.Release()\n\treturn next.Get().GetValue(source, key, subPath)\n}\n\nfunc (n *BranchNode) GetSlot(source NodeSource, address common.Address, path []Nibble, key common.Key) (common.Value, bool, error) {\n\tnext, subPath, err := n.getNextNodeInBranch(source, path)\n\tif err != nil {\n\t\treturn common.Value{}, false, err\n\t}\n\tdefer next.Release()\n\treturn next.Get().GetSlot(source, address, subPath, key)\n}\n\nfunc (n *BranchNode) setNextNode(\n\tmanager NodeManager,\n\tthisRef *NodeReference,\n\tthis shared.WriteHandle[Node],\n\tpath []Nibble,\n\tcreateSubTree func(*NodeReference, shared.WriteHandle[Node], []Nibble) (NodeReference, bool, error),\n) (NodeReference, bool, error) {\n\t// Forward call to child node.\n\tchild := &n.children[path[0]]\n\tnode, err := manager.getWriteAccess(child)\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tnewRoot, hasChanged, err := createSubTree(child, node, path[1:])\n\tnode.Release()\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\n\tif newRoot.Id() == child.Id() {\n\t\tif hasChanged {\n\t\t\tn.markDirty()\n\t\t\tn.markChildHashDirty(byte(path[0]))\n\t\t}\n\t\treturn *thisRef, hasChanged, nil\n\t}\n\n\t// If frozen, clone the current node and modify copy.\n\tisClone := false\n\tif n.IsFrozen() {\n\t\tnewRef, handle, err := manager.createBranch()\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, false, err\n\t\t}\n\t\tdefer handle.Release()\n\t\tnewNode := handle.Get().(*BranchNode)\n\t\t*newNode = *n\n\t\tnewNode.markDirty()\n\t\tnewNode.markMutable()\n\t\tn = newNode\n\t\tthisRef = &newRef\n\t\tisClone = true\n\t}\n\n\twasEmpty := child.Id().IsEmpty()\n\tn.children[path[0]] = newRoot\n\tn.markChildHashDirty(byte(path[0]))\n\tn.setChildFrozen(byte(path[0]), false)\n\n\t// If a branch got removed, check that there are enough children left.\n\tif !wasEmpty && newRoot.Id().IsEmpty() {\n\t\tn.markChildHashClean(byte(path[0]))\n\t\tcount := 0\n\t\tvar remainingPos Nibble\n\t\tvar remaining NodeReference\n\t\tfor i, cur := range n.children {\n\t\t\tif !cur.Id().IsEmpty() {\n\t\t\t\tcount++\n\t\t\t\tif count > 1 {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tremainingPos = Nibble(i)\n\t\t\t\tremaining = cur\n\t\t\t}\n\t\t}\n\t\tif count < 2 {\n\t\t\tnewRoot := remaining\n\t\t\t// This branch became obsolete and needs to be removed.\n\t\t\tif remaining.Id().IsExtension() {\n\t\t\t\t// The present extension can be extended.\n\t\t\t\textension, err := manager.getWriteAccess(&remaining)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\t\t\t\tdefer extension.Release()\n\t\t\t\textensionNode := extension.Get().(*ExtensionNode)\n\n\t\t\t\t// If the extension is frozen, we need to modify a copy.\n\t\t\t\tif extensionNode.IsFrozen() {\n\t\t\t\t\tcopyId, handle, err := manager.createExtension()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t\tdefer handle.Release()\n\t\t\t\t\tcopy := handle.Get().(*ExtensionNode)\n\t\t\t\t\t*copy = *extensionNode\n\t\t\t\t\tcopy.markMutable()\n\t\t\t\t\textensionNode = copy\n\t\t\t\t\tremaining = copyId\n\t\t\t\t\tnewRoot = copyId\n\t\t\t\t}\n\n\t\t\t\textensionNode.path.Prepend(remainingPos)\n\t\t\t\textensionNode.markDirty()\n\t\t\t} else if remaining.Id().IsBranch() {\n\t\t\t\t// An extension needs to replace this branch.\n\t\t\t\textensionRef, handle, err := manager.createExtension()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\t\t\t\tdefer handle.Release()\n\t\t\t\textension := handle.Get().(*ExtensionNode)\n\t\t\t\textension.path = SingleStepPath(remainingPos)\n\t\t\t\textension.next = remaining\n\t\t\t\textension.nextHashDirty = n.isChildHashDirty(byte(remainingPos))\n\t\t\t\tif !extension.nextHashDirty {\n\t\t\t\t\textension.nextIsEmbedded = n.isEmbedded(byte(remainingPos))\n\t\t\t\t\textension.nextHash = n.hashes[byte(remainingPos)]\n\t\t\t\t}\n\t\t\t\textension.markDirty()\n\t\t\t\tnewRoot = extensionRef\n\t\t\t} else if manager.getConfig().TrackSuffixLengthsInLeafNodes {\n\t\t\t\t// If suffix lengths need to be tracked, leaf nodes require an update.\n\t\t\t\tif remaining.Id().IsAccount() {\n\t\t\t\t\thandle, err := manager.getWriteAccess(&remaining)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t\tnewRoot, _, err = handle.Get().(*AccountNode).setPathLength(manager, &remaining, handle, byte(len(path)))\n\t\t\t\t\thandle.Release()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t} else if remaining.Id().IsValue() {\n\t\t\t\t\thandle, err := manager.getWriteAccess(&remaining)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t\tnewRoot, _, err = handle.Get().(*ValueNode).setPathLength(manager, &remaining, handle, byte(len(path)))\n\t\t\t\t\thandle.Release()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tn.nodeBase.Release()\n\t\t\treturn newRoot, !isClone, manager.release(thisRef)\n\t\t}\n\t}\n\n\tn.markDirty()\n\treturn *thisRef, !isClone, err\n}\n\nfunc (n *BranchNode) SetAccount(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, info AccountInfo) (NodeReference, bool, error) {\n\treturn n.setNextNode(manager, thisRef, this, path,\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().SetAccount(manager, next, node, address, path, info)\n\t\t},\n\t)\n}\n\nfunc (n *BranchNode) SetValue(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], key common.Key, path []Nibble, value common.Value) (NodeReference, bool, error) {\n\treturn n.setNextNode(manager, thisRef, this, path,\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().SetValue(manager, next, node, key, path, value)\n\t\t},\n\t)\n}\n\nfunc (n *BranchNode) SetSlot(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, key common.Key, value common.Value) (NodeReference, bool, error) {\n\treturn n.setNextNode(manager, thisRef, this, path,\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().SetSlot(manager, next, node, address, path, key, value)\n\t\t},\n\t)\n}\n\nfunc (n *BranchNode) ClearStorage(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble) (newRoot NodeReference, changed bool, err error) {\n\treturn n.setNextNode(manager, thisRef, this, path,\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().ClearStorage(manager, next, node, address, path)\n\t\t},\n\t)\n}\n\nfunc (n *BranchNode) Release(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.nodeBase.Release()\n\tfor _, cur := range n.children {\n\t\tif !cur.Id().IsEmpty() {\n\t\t\thandle, err := manager.getWriteAccess(&cur)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\terr = handle.Get().Release(manager, &cur, handle)\n\t\t\thandle.Release()\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn manager.release(thisRef)\n}\n\nfunc (n *BranchNode) MarkFrozen() {\n\tn.nodeBase.MarkFrozen()\n\tn.frozenChildren = ^uint16(0)\n}\n\nfunc (n *BranchNode) Freeze(manager NodeManager, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.nodeBase.MarkFrozen()\n\tfor i := 0; i < len(n.children); i++ {\n\t\tif n.children[i].Id().IsEmpty() || n.isChildFrozen(byte(i)) {\n\t\t\tcontinue\n\t\t}\n\t\thandle, err := manager.getWriteAccess(&n.children[i])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = handle.Get().Freeze(manager, handle)\n\t\thandle.Release()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tn.setChildFrozen(byte(i), true)\n\t}\n\treturn nil\n}\n\nfunc (n *BranchNode) Check(source NodeSource, thisRef *NodeReference, _ []Nibble) error {\n\t// Checked invariants:\n\t//  - must have 2+ children\n\t//  - non-dirty hashes for child nodes are valid\n\t//  - mask of frozen children is consistent\n\tnumChildren := 0\n\tvar errs []error\n\n\tif err := n.nodeBase.check(thisRef); err != nil {\n\t\terrs = append(errs, err)\n\t}\n\n\thashWithParent := source.getConfig().HashStorageLocation == HashStoredWithParent\n\tif hashWithParent && n.hasCleanHash() && n.dirtyHashes != 0 {\n\t\terrs = append(errs, fmt.Errorf(\"node %v is has clean hash but child hashes are dirty: %016b\", thisRef.Id(), n.dirtyHashes))\n\t}\n\n\tfor i, child := range n.children {\n\t\tif child.Id().IsEmpty() {\n\t\t\tcontinue\n\t\t}\n\t\tnumChildren++\n\t\tif !n.isChildHashDirty(byte(i)) && !n.isEmbedded(byte(i)) {\n\t\t\twant, err := source.getHashFor(&child)\n\t\t\tif err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t} else if got := n.hashes[i]; want != got {\n\t\t\t\terrs = append(errs, fmt.Errorf(\"in node %v the hash for child %d is invalid\\nwant: %v\\ngot: %v\", thisRef.Id(), i, want, got))\n\t\t\t}\n\t\t}\n\t\thandle, err := source.getViewAccess(&child)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tchildIsFrozen := handle.Get().IsFrozen()\n\t\thandle.Release()\n\n\t\t// rule: child is marked as frozen -> childIsFrozen (implication)\n\t\tif flag := n.isChildFrozen(byte(i)); flag && !childIsFrozen {\n\t\t\terrs = append(errs, fmt.Errorf(\"in node %v the frozen flag for child 0x%X is invalid, flag: %t, actual: %t\", thisRef.Id(), i, flag, childIsFrozen))\n\t\t}\n\n\t\t// rule: if this node is frozen, all children must be frozen\n\t\tif n.IsFrozen() && !childIsFrozen {\n\t\t\terrs = append(errs, fmt.Errorf(\"the frozen node %v must not have a non-frozen child at position 0x%X\", thisRef.Id(), i))\n\t\t}\n\t}\n\tif numChildren < 2 {\n\t\terrs = append(errs, fmt.Errorf(\"node %v has an insufficient number of child nodes: %d\", thisRef.Id(), numChildren))\n\t}\n\treturn errors.Join(errs...)\n}\n\nfunc (n *BranchNode) Dump(out io.Writer, source NodeSource, thisRef *NodeReference, indent string) error {\n\terrs := []error{}\n\tfmt.Fprintf(out, \"%sBranch (ID: %v, dirty: %t, frozen: %t, Dirty: %016b, Embedded: %016b, Frozen: %016b, Hash: %v, hashState: %v):\\n\", indent, thisRef.Id(), n.IsDirty(), n.IsFrozen(), n.dirtyHashes, n.embeddedChildren, n.frozenChildren, formatHashForDump(n.hash), n.getHashStatus())\n\tfor i, child := range n.children {\n\t\tif child.Id().IsEmpty() {\n\t\t\tcontinue\n\t\t}\n\t\tif handle, err := source.getViewAccess(&child); err == nil {\n\t\t\tdefer handle.Release()\n\t\t\tif err := handle.Get().Dump(out, source, &child, fmt.Sprintf(\"%s  %v \", indent, Nibble(i))); err != nil {\n\t\t\t\terrs = append(errs, err)\n\t\t\t}\n\t\t} else {\n\t\t\tfmt.Fprintf(out, \"%s  ERROR: unable to load node %v: %v\", indent, child, err)\n\t\t\terrs = append(errs, err)\n\t\t}\n\t}\n\treturn errors.Join(errs...)\n}\n\nfunc (b *BranchNode) Visit(source NodeSource, thisRef *NodeReference, depth int, visitor NodeVisitor) (bool, error) {\n\tswitch visitor.Visit(b, NodeInfo{Id: thisRef.Id(), Depth: &depth}) {\n\tcase VisitResponseAbort:\n\t\treturn true, nil\n\tcase VisitResponsePrune:\n\t\treturn false, nil\n\tcase VisitResponseContinue: /* keep going */\n\t}\n\tfor _, child := range b.children {\n\t\tif child.Id().IsEmpty() {\n\t\t\tcontinue\n\t\t}\n\n\t\tif handle, err := source.getViewAccess(&child); err == nil {\n\t\t\tdefer handle.Release()\n\t\t\tif abort, err := handle.Get().Visit(source, &child, depth+1, visitor); abort || err != nil {\n\t\t\t\treturn abort, err\n\t\t\t}\n\t\t} else {\n\t\t\treturn false, err\n\t\t}\n\t}\n\treturn false, nil\n}\n\nfunc (n *BranchNode) markChildHashDirty(index byte) {\n\tn.dirtyHashes = n.dirtyHashes | (1 << index)\n}\n\nfunc (n *BranchNode) markChildHashClean(index byte) {\n\tn.dirtyHashes = n.dirtyHashes & ^(1 << index)\n}\n\nfunc (n *BranchNode) isChildHashDirty(index byte) bool {\n\treturn (n.dirtyHashes & (1 << index)) != 0\n}\n\nfunc (n *BranchNode) clearChildHashDirtyFlags() {\n\tn.dirtyHashes = 0\n}\n\nfunc (n *BranchNode) isEmbedded(index byte) bool {\n\treturn (n.embeddedChildren & (1 << index)) != 0\n}\n\nfunc (n *BranchNode) setEmbedded(index byte, embedded bool) {\n\tif embedded {\n\t\tn.embeddedChildren = n.embeddedChildren | (1 << index)\n\t} else {\n\t\tn.embeddedChildren = n.embeddedChildren & ^(1 << index)\n\t}\n}\n\nfunc (n *BranchNode) isChildFrozen(index byte) bool {\n\treturn (n.frozenChildren & (1 << index)) != 0\n}\n\nfunc (n *BranchNode) setChildFrozen(index byte, frozen bool) {\n\tif frozen {\n\t\tn.frozenChildren = n.frozenChildren | (1 << index)\n\t} else {\n\t\tn.frozenChildren = n.frozenChildren & ^(1 << index)\n\t}\n}\n\n// ----------------------------------------------------------------------------\n//                              Extension Node\n// ----------------------------------------------------------------------------\n\n// ExtensionNode are covering one or more Nibbles along the path from a root\n// node to a leaf node in a trie. Neither the path nor the referenced sub-trie\n// must be empty.\ntype ExtensionNode struct {\n\tnodeBase\n\tpath           Path\n\tnext           NodeReference\n\tnextHash       common.Hash\n\tnextHashDirty  bool\n\tnextIsEmbedded bool\n}\n\nfunc (n *ExtensionNode) getNextNodeInExtension(\n\tsource NodeSource,\n\tpath []Nibble,\n) (shared.ReadHandle[Node], []Nibble, error) {\n\tif !n.path.IsPrefixOf(path) {\n\t\tshared := shared.MakeShared[Node](EmptyNode{})\n\t\treturn shared.GetReadHandle(), nil, nil\n\t}\n\thandle, err := source.getReadAccess(&n.next)\n\tif err != nil {\n\t\treturn shared.ReadHandle[Node]{}, nil, err\n\t}\n\treturn handle, path[n.path.Length():], nil\n}\n\nfunc (n *ExtensionNode) GetAccount(source NodeSource, address common.Address, path []Nibble) (AccountInfo, bool, error) {\n\thandle, rest, err := n.getNextNodeInExtension(source, path)\n\tif err != nil {\n\t\treturn AccountInfo{}, false, err\n\t}\n\tdefer handle.Release()\n\treturn handle.Get().GetAccount(source, address, rest)\n}\n\nfunc (n *ExtensionNode) GetValue(source NodeSource, key common.Key, path []Nibble) (common.Value, bool, error) {\n\thandle, rest, err := n.getNextNodeInExtension(source, path)\n\tif err != nil {\n\t\treturn common.Value{}, false, err\n\t}\n\tdefer handle.Release()\n\treturn handle.Get().GetValue(source, key, rest)\n}\n\nfunc (n *ExtensionNode) GetSlot(source NodeSource, address common.Address, path []Nibble, key common.Key) (common.Value, bool, error) {\n\thandle, rest, err := n.getNextNodeInExtension(source, path)\n\tif err != nil {\n\t\treturn common.Value{}, false, err\n\t}\n\tdefer handle.Release()\n\treturn handle.Get().GetSlot(source, address, rest, key)\n}\n\nfunc (n *ExtensionNode) setNextNode(\n\tmanager NodeManager,\n\tthisRef *NodeReference,\n\tpath []Nibble,\n\tvalueIsEmpty bool,\n\tcreateSubTree func(*NodeReference, shared.WriteHandle[Node], []Nibble) (NodeReference, bool, error),\n) (NodeReference, bool, error) {\n\t// Check whether the updates targets the node referenced by this extension.\n\tif n.path.IsPrefixOf(path) {\n\t\thandle, err := manager.getWriteAccess(&n.next)\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, false, err\n\t\t}\n\t\tdefer handle.Release()\n\t\tnewRoot, hasChanged, err := createSubTree(&n.next, handle, path[n.path.Length():])\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, false, err\n\t\t}\n\n\t\t// The modified sub-trie is either a branch, extension, account, or\n\t\t// value node. It can not be empty, since a single modification cannot\n\t\t// convert a branch node into an empty node.\n\n\t\tif newRoot != n.next {\n\n\t\t\t// If frozen, modify a clone.\n\t\t\tisClone := false\n\t\t\tif n.IsFrozen() {\n\t\t\t\tnewRef, handle, err := manager.createExtension()\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\t\t\t\tdefer handle.Release()\n\t\t\t\tnewNode := handle.Get().(*ExtensionNode)\n\t\t\t\t*newNode = *n\n\t\t\t\tnewNode.markDirty()\n\t\t\t\tnewNode.markMutable()\n\t\t\t\tthisRef, n = &newRef, newNode\n\t\t\t\tisClone = true\n\t\t\t}\n\n\t\t\t// The referenced sub-tree has changed, so the hash needs to be updated.\n\t\t\tn.nextHashDirty = true\n\n\t\t\tif newRoot.Id().IsExtension() {\n\t\t\t\t// If the new next is an extension, merge it into this extension.\n\t\t\t\thandle, err := manager.getWriteAccess(&newRoot)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\t\t\t\tdefer handle.Release()\n\t\t\t\textension := handle.Get().(*ExtensionNode)\n\t\t\t\tn.path.AppendAll(&extension.path)\n\t\t\t\tn.next = extension.next\n\t\t\t\tn.nextHashDirty = extension.nextHashDirty\n\t\t\t\tif !extension.nextHashDirty {\n\t\t\t\t\tn.nextHash = extension.nextHash\n\t\t\t\t\tn.nextIsEmbedded = extension.nextIsEmbedded\n\t\t\t\t}\n\t\t\t\tn.markDirty()\n\t\t\t\textension.nodeBase.Release()\n\t\t\t\tif err := manager.release(&newRoot); err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\t\t\t} else if newRoot.Id().IsBranch() {\n\t\t\t\tn.next = newRoot\n\t\t\t\tn.nextHashDirty = true\n\t\t\t\tn.markDirty()\n\t\t\t} else {\n\t\t\t\t// If the next node is anything but a branch or extension, remove this extension.\n\t\t\t\tn.nodeBase.Release()\n\t\t\t\tif err := manager.release(thisRef); err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\n\t\t\t\t// Grow path length of next nodes if tracking of length is enabled.\n\t\t\t\tif manager.getConfig().TrackSuffixLengthsInLeafNodes {\n\t\t\t\t\troot, err := manager.getWriteAccess(&newRoot)\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t\tif newRoot.Id().IsAccount() {\n\t\t\t\t\t\tnewRoot, _, err = root.Get().(*AccountNode).setPathLength(manager, &newRoot, root, byte(len(path)))\n\t\t\t\t\t} else if newRoot.Id().IsValue() {\n\t\t\t\t\t\tnewRoot, _, err = root.Get().(*ValueNode).setPathLength(manager, &newRoot, root, byte(len(path)))\n\t\t\t\t\t} else {\n\t\t\t\t\t\tpanic(fmt.Sprintf(\"unsupported new next node type: %v\", newRoot))\n\t\t\t\t\t}\n\t\t\t\t\troot.Release()\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t\t}\n\t\t\t\t}\n\n\t\t\t\treturn newRoot, !isClone, nil\n\t\t\t}\n\t\t} else if hasChanged {\n\t\t\tn.markDirty()\n\t\t\tn.nextHashDirty = true\n\t\t}\n\t\treturn *thisRef, hasChanged, err\n\t}\n\n\t// Skip creation of a new sub-tree if the info is empty.\n\tif valueIsEmpty {\n\t\treturn *thisRef, false, nil\n\t}\n\n\t// If frozen, modify a clone.\n\tisClone := false\n\tif n.IsFrozen() {\n\t\tnewRef, handle, err := manager.createExtension()\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, false, err\n\t\t}\n\t\tdefer handle.Release()\n\t\tnewNode := handle.Get().(*ExtensionNode)\n\t\t*newNode = *n\n\t\tnewNode.markDirty()\n\t\tnewNode.markMutable()\n\t\tthisRef, n = &newRef, newNode\n\t\tisClone = true\n\t}\n\n\t// Extension needs to be replaced by a combination of\n\t//  - an optional common prefix extension\n\t//  - a branch node\n\t//  - an optional extension connecting to the previous next node\n\n\t// Create the branch node that will be needed in any case.\n\tbranchRef, branchHandle, err := manager.createBranch()\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tdefer branchHandle.Release()\n\tnewRoot := branchRef\n\tbranch := branchHandle.Get().(*BranchNode)\n\n\t// Determine the point at which the prefix need to be split.\n\tcommonPrefixLength := n.path.GetCommonPrefixLength(path)\n\n\t// Build the extension connecting the branch to the next node.\n\tthisNodeWasReused := false\n\tif commonPrefixLength < n.path.Length()-1 {\n\t\t// We re-use the current node for this - all we need is to update the path.\n\t\tbranch.children[n.path.Get(commonPrefixLength)] = *thisRef\n\t\tbranch.markChildHashDirty(byte(n.path.Get(commonPrefixLength)))\n\t\tn.path.ShiftLeft(commonPrefixLength + 1)\n\t\tn.markDirty()\n\t\tthisNodeWasReused = true\n\t} else {\n\t\tpos := byte(n.path.Get(commonPrefixLength))\n\t\tbranch.children[pos] = n.next\n\t\tif n.nextHashDirty {\n\t\t\tbranch.markChildHashDirty(pos)\n\t\t} else {\n\t\t\tbranch.hashes[pos] = n.nextHash\n\t\t\tbranch.setEmbedded(pos, n.nextIsEmbedded)\n\t\t}\n\t\tbranch.setChildFrozen(pos, isClone)\n\t}\n\n\t// Build the extension covering the common prefix.\n\tif commonPrefixLength > 0 {\n\t\t// Reuse current node unless already taken.\n\t\textension := n\n\t\textensionRef := *thisRef\n\t\tif thisNodeWasReused {\n\t\t\tvar extensionHandle shared.WriteHandle[Node]\n\t\t\textensionRef, extensionHandle, err = manager.createExtension()\n\t\t\tif err != nil {\n\t\t\t\treturn NodeReference{}, false, err\n\t\t\t}\n\t\t\tdefer extensionHandle.Release()\n\t\t\textension = extensionHandle.Get().(*ExtensionNode)\n\t\t} else {\n\t\t\tthisNodeWasReused = true\n\t\t}\n\n\t\textension.path = CreatePathFromNibbles(path[0:commonPrefixLength])\n\t\textension.next = branchRef\n\t\textension.nextHashDirty = true\n\t\textension.markDirty()\n\t\tnewRoot = extensionRef\n\t}\n\n\t// Continue insertion of new account at new branch level.\n\t_, _, err = createSubTree(&branchRef, branchHandle, path[commonPrefixLength:])\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\n\t// If this node was not needed any more, we can discard it.\n\tif !thisNodeWasReused {\n\t\tn.nodeBase.Release()\n\t\treturn newRoot, false, manager.release(thisRef)\n\t}\n\n\treturn newRoot, !isClone, nil\n}\n\nfunc (n *ExtensionNode) SetAccount(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, info AccountInfo) (NodeReference, bool, error) {\n\treturn n.setNextNode(manager, thisRef, path, info.IsEmpty(),\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().SetAccount(manager, next, node, address, path, info)\n\t\t},\n\t)\n}\n\nfunc (n *ExtensionNode) SetValue(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], key common.Key, path []Nibble, value common.Value) (NodeReference, bool, error) {\n\treturn n.setNextNode(manager, thisRef, path, value == (common.Value{}),\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().SetValue(manager, next, node, key, path, value)\n\t\t},\n\t)\n}\n\nfunc (n *ExtensionNode) SetSlot(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, key common.Key, value common.Value) (NodeReference, bool, error) {\n\treturn n.setNextNode(manager, thisRef, path, true,\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().SetSlot(manager, next, node, address, path, key, value)\n\t\t},\n\t)\n}\n\nfunc (n *ExtensionNode) ClearStorage(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble) (newRoot NodeReference, hasChanged bool, err error) {\n\treturn n.setNextNode(manager, thisRef, path, true,\n\t\tfunc(next *NodeReference, node shared.WriteHandle[Node], path []Nibble) (NodeReference, bool, error) {\n\t\t\treturn node.Get().ClearStorage(manager, next, node, address, path)\n\t\t},\n\t)\n}\n\nfunc (n *ExtensionNode) Release(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.nodeBase.Release()\n\thandle, err := manager.getWriteAccess(&n.next)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer handle.Release()\n\terr = handle.Get().Release(manager, &n.next, handle)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn manager.release(thisRef)\n}\n\nfunc (n *ExtensionNode) Freeze(manager NodeManager, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.MarkFrozen()\n\thandle, err := manager.getWriteAccess(&n.next)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer handle.Release()\n\treturn handle.Get().Freeze(manager, handle)\n}\n\nfunc (n *ExtensionNode) Check(source NodeSource, thisRef *NodeReference, _ []Nibble) error {\n\t// Checked invariants:\n\t//  - extension path have a length > 0\n\t//  - extension can only be followed by a branch\n\t//  - hash of sub-tree is either dirty or correct\n\t//  - frozen flags are consistent\n\tvar errs []error\n\n\tif err := n.nodeBase.check(thisRef); err != nil {\n\t\terrs = append(errs, err)\n\t}\n\n\thashWithParent := source.getConfig().HashStorageLocation == HashStoredWithParent\n\tif hashWithParent && n.hasCleanHash() && n.nextHashDirty {\n\t\terrs = append(errs, fmt.Errorf(\"node %v is marked to have a clean hash but next hash is dirty\", thisRef.Id()))\n\t}\n\n\tif n.path.Length() <= 0 {\n\t\terrs = append(errs, fmt.Errorf(\"node %v - extension path must not be empty\", thisRef.Id()))\n\t}\n\tif !n.next.Id().IsBranch() {\n\t\terrs = append(errs, fmt.Errorf(\"node %v - extension path must be followed by a branch\", thisRef.Id()))\n\t}\n\tif !n.nextHashDirty && !n.nextIsEmbedded {\n\t\twant, err := source.getHashFor(&n.next)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t} else if want != n.nextHash {\n\t\t\terrs = append(errs, fmt.Errorf(\"node %v - next node hash invalid\\nwant: %v\\ngot: %v\", thisRef.Id(), want, n.nextHash))\n\t\t}\n\t}\n\n\tif !n.next.Id().IsEmpty() {\n\t\thandle, err := source.getViewAccess(&n.next)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t} else {\n\t\t\tnextIsFrozen := handle.Get().IsFrozen()\n\t\t\thandle.Release()\n\t\t\tif n.IsFrozen() && !nextIsFrozen {\n\t\t\t\terrs = append(errs, fmt.Errorf(\"the frozen node %v must have a frozen next\", thisRef.Id()))\n\t\t\t}\n\t\t}\n\t}\n\n\treturn errors.Join(errs...)\n}\n\nfunc (n *ExtensionNode) Dump(out io.Writer, source NodeSource, thisRef *NodeReference, indent string) error {\n\terrs := []error{}\n\tfmt.Fprintf(out, \"%sExtension (ID: %v/%t, nextHashDirty: %t, Embedded: %t, Hash: %v, hashState: %v): %v\\n\", indent, thisRef.Id(), n.IsFrozen(), n.nextHashDirty, n.nextIsEmbedded, formatHashForDump(n.hash), n.getHashStatus(), &n.path)\n\tif handle, err := source.getViewAccess(&n.next); err == nil {\n\t\tdefer handle.Release()\n\t\tif err := handle.Get().Dump(out, source, &n.next, indent+\"  \"); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t} else {\n\t\tfmt.Fprintf(out, \"%s  ERROR: unable to load node %v: %v\", indent, n.next, err)\n\t\terrs = append(errs, err)\n\t}\n\treturn errors.Join(errs...)\n}\n\nfunc (n *ExtensionNode) Visit(source NodeSource, thisRef *NodeReference, depth int, visitor NodeVisitor) (bool, error) {\n\tresponse := visitor.Visit(n, NodeInfo{Id: thisRef.Id(), Depth: &depth})\n\tswitch response {\n\tcase VisitResponseAbort:\n\t\treturn true, nil\n\tcase VisitResponsePrune:\n\t\treturn false, nil\n\t}\n\tif handle, err := source.getViewAccess(&n.next); err == nil {\n\t\tdefer handle.Release()\n\t\treturn handle.Get().Visit(source, &n.next, depth+1, visitor)\n\t} else {\n\t\treturn false, err\n\t}\n}\n\n// ----------------------------------------------------------------------------\n//                               Account Node\n// ----------------------------------------------------------------------------\n\n// AccountNode is the node type found in the middle of an MPT structure\n// representing an account. It stores the account's information and references\n// the root node of the account's storage trie. It forms the boundary between\n// the usage of addresses for navigating the trie and the usage of keys.\n// No AccountNode may be present in the trie rooted by an accounts storage\n// root. Also, the retained account information must not be empty.\ntype AccountNode struct {\n\tnodeBase\n\taddress          common.Address\n\tinfo             AccountInfo\n\tstorage          NodeReference\n\tstorageHash      common.Hash\n\tstorageHashDirty bool\n\t// pathLength is the number of nibbles of the key (or its hash) not covered\n\t// by the navigation path to this node. It is only maintained if the\n\t// `TrackSuffixLengthsInLeafNodes` of the `MptConfig` is enabled.\n\tpathLength byte\n}\n\nfunc (n *AccountNode) Address() common.Address {\n\treturn n.address\n}\n\nfunc (n *AccountNode) Info() AccountInfo {\n\treturn n.info\n}\n\nfunc (n *AccountNode) GetAccount(source NodeSource, address common.Address, path []Nibble) (AccountInfo, bool, error) {\n\tif n.address == address {\n\t\treturn n.info, true, nil\n\t}\n\treturn AccountInfo{}, false, nil\n}\n\nfunc (n *AccountNode) GetValue(NodeSource, common.Key, []Nibble) (common.Value, bool, error) {\n\treturn common.Value{}, false, fmt.Errorf(\"invalid request: value query should not reach accounts\")\n}\n\nfunc (n *AccountNode) GetSlot(source NodeSource, address common.Address, path []Nibble, key common.Key) (common.Value, bool, error) {\n\tif n.address != address {\n\t\treturn common.Value{}, false, nil\n\t}\n\tsubPath := KeyToNibblePath(key, source)\n\troot, err := source.getReadAccess(&n.storage)\n\tif err != nil {\n\t\treturn common.Value{}, false, err\n\t}\n\tdefer root.Release()\n\treturn root.Get().GetValue(source, key, subPath[:])\n}\n\nfunc (n *AccountNode) SetAccount(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, info AccountInfo) (NodeReference, bool, error) {\n\t// Check whether this is the correct account.\n\tif n.address == address {\n\t\tif info == n.info {\n\t\t\treturn *thisRef, false, nil\n\t\t}\n\t\tif info.IsEmpty() {\n\t\t\tif n.IsFrozen() {\n\t\t\t\treturn NewNodeReference(EmptyId()), false, nil\n\t\t\t}\n\t\t\t// Recursively release the entire state DB.\n\t\t\tif !n.storage.Id().IsEmpty() {\n\t\t\t\tmanager.releaseTrieAsynchronous(n.storage)\n\t\t\t}\n\t\t\t// Release this account node and remove it from the trie.\n\t\t\tn.nodeBase.Release()\n\t\t\treturn NewNodeReference(EmptyId()), false, manager.release(thisRef)\n\t\t}\n\n\t\t// If this node is frozen, we need to write the result in\n\t\t// a new account node.\n\t\tif n.IsFrozen() {\n\t\t\tnewRef, handle, err := manager.createAccount()\n\t\t\tif err != nil {\n\t\t\t\treturn NodeReference{}, false, err\n\t\t\t}\n\t\t\tdefer handle.Release()\n\t\t\tnewNode := handle.Get().(*AccountNode)\n\t\t\t*newNode = *n\n\t\t\tnewNode.markDirty()\n\t\t\tnewNode.markMutable()\n\t\t\tnewNode.info = info\n\t\t\treturn newRef, false, nil\n\t\t}\n\n\t\tn.info = info\n\t\tn.markDirty()\n\t\treturn *thisRef, true, nil\n\t}\n\n\t// Skip restructuring the tree if the new info is empty.\n\tif info.IsEmpty() {\n\t\treturn *thisRef, false, nil\n\t}\n\n\t// Create a new node for the sibling to be added.\n\tsiblingRef, handle, err := manager.createAccount()\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tdefer handle.Release()\n\tsibling := handle.Get().(*AccountNode)\n\tsibling.address = address\n\tsibling.info = info\n\tsibling.markDirty()\n\n\tthisPath := AddressToNibblePath(n.address, manager)\n\tnewRoot, err := splitLeafNode(manager, thisRef, thisPath[:], n, this, path, &siblingRef, sibling, handle)\n\treturn newRoot, !n.IsFrozen() && manager.getConfig().TrackSuffixLengthsInLeafNodes, err\n}\n\ntype leafNode interface {\n\tNode\n\tsetPathLength(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], length byte) (newRoot NodeReference, changed bool, err error)\n}\n\nfunc splitLeafNode(\n\tmanager NodeManager,\n\tthisRef *NodeReference,\n\tthisPath []Nibble,\n\tthis leafNode,\n\tthisHandle shared.WriteHandle[Node],\n\tsiblingPath []Nibble,\n\tsiblingRef *NodeReference,\n\tsibling leafNode,\n\tsiblingHandle shared.WriteHandle[Node],\n) (NodeReference, error) {\n\t// This single node needs to be split into\n\t//  - an optional common prefix extension\n\t//  - a branch node linking this node and\n\t//  - a new sibling account node to be returned\n\n\tbranchRef, branchHandle, err := manager.createBranch()\n\tif err != nil {\n\t\treturn NodeReference{}, err\n\t}\n\tdefer branchHandle.Release()\n\tbranch := branchHandle.Get().(*BranchNode)\n\tnewRoot := branchRef\n\n\t// Check whether there is a common prefix.\n\tpartialPath := thisPath[len(thisPath)-len(siblingPath):]\n\tcommonPrefixLength := GetCommonPrefixLength(partialPath, siblingPath)\n\tif commonPrefixLength > 0 {\n\t\textensionRef, handle, err := manager.createExtension()\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, err\n\t\t}\n\t\tdefer handle.Release()\n\t\textension := handle.Get().(*ExtensionNode)\n\t\tnewRoot = extensionRef\n\n\t\textension.path = CreatePathFromNibbles(siblingPath[0:commonPrefixLength])\n\t\textension.next = branchRef\n\t\textension.nextHashDirty = true\n\t\textension.markDirty()\n\t}\n\n\t// If enabled, keep track of the suffix length of leaf values.\n\tthisModified := false\n\tthisIsFrozen := this.IsFrozen()\n\tremainingPathLength := byte(len(partialPath)-commonPrefixLength) - 1\n\tif manager.getConfig().TrackSuffixLengthsInLeafNodes {\n\t\tsibling.setPathLength(manager, siblingRef, siblingHandle, remainingPathLength)\n\t\tref, _, err := this.setPathLength(manager, thisRef, thisHandle, remainingPathLength)\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, err\n\t\t}\n\t\tthisModified = true\n\t\tthisRef = &ref\n\t\tthisIsFrozen = false\n\t}\n\n\t// Add this node and the new sibling node to the branch node.\n\tbranch.children[partialPath[commonPrefixLength]] = *thisRef\n\tbranch.children[siblingPath[commonPrefixLength]] = *siblingRef\n\tbranch.markChildHashDirty(byte(siblingPath[commonPrefixLength]))\n\tbranch.markDirty()\n\n\t// Update hash if present.\n\tif hash, dirty := this.GetHash(); thisModified || dirty {\n\t\tbranch.markChildHashDirty(byte(partialPath[commonPrefixLength]))\n\t} else {\n\t\tbranch.hashes[partialPath[commonPrefixLength]] = hash\n\t\t// The embedded flag can be ignored in this case as long as direct\n\t\t// hashing is used.\n\t\tif manager.getConfig().Hashing.Name != DirectHashing.Name {\n\t\t\tpanic(\"unsupported mode: disabled TrackSuffixLengthsInLeafNodes is not (yet) supported with hash algorithms depending on embedded nodes.\")\n\t\t}\n\t}\n\n\t// Track frozen state of split node.\n\tif thisIsFrozen {\n\t\tbranch.setChildFrozen(byte(partialPath[commonPrefixLength]), true)\n\t}\n\n\treturn newRoot, nil\n}\n\nfunc (n *AccountNode) SetValue(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], key common.Key, path []Nibble, value common.Value) (NodeReference, bool, error) {\n\treturn NodeReference{}, false, fmt.Errorf(\"setValue call should not reach account nodes\")\n}\n\nfunc (n *AccountNode) SetSlot(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble, key common.Key, value common.Value) (NodeReference, bool, error) {\n\t// If this is not the correct account, the real account does not exist\n\t// and the insert can be skipped. The insertion of a slot value shall\n\t// not create an account.\n\tif n.address != address {\n\t\treturn *thisRef, false, nil\n\t}\n\n\t// Continue from here with a value insertion.\n\thandle, err := manager.getWriteAccess(&n.storage)\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tdefer handle.Release()\n\tsubPath := KeyToNibblePath(key, manager)\n\troot, hasChanged, err := handle.Get().SetValue(manager, &n.storage, handle, key, subPath[:], value)\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tif root != n.storage {\n\t\t// If this node is frozen, we need to write the result in\n\t\t// a new account node.\n\t\tif n.IsFrozen() {\n\t\t\tnewRef, newHandle, err := manager.createAccount()\n\t\t\tif err != nil {\n\t\t\t\treturn NodeReference{}, false, err\n\t\t\t}\n\t\t\tdefer newHandle.Release()\n\t\t\tnewNode := newHandle.Get().(*AccountNode)\n\t\t\t*newNode = *n\n\t\t\tnewNode.markDirty()\n\t\t\tnewNode.markMutable()\n\t\t\tnewNode.storage = root\n\t\t\tnewNode.storageHashDirty = true\n\t\t\treturn newRef, false, nil\n\t\t}\n\t\tn.storage = root\n\t\tn.storageHashDirty = true\n\t\tn.markDirty()\n\t\thasChanged = true\n\t} else if hasChanged {\n\t\tn.storageHashDirty = true\n\t\tn.markDirty()\n\t}\n\treturn *thisRef, hasChanged, nil\n}\n\nfunc (n *AccountNode) ClearStorage(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], address common.Address, path []Nibble) (newRoot NodeReference, changed bool, err error) {\n\tif n.address != address || n.storage.Id().IsEmpty() {\n\t\treturn *thisRef, false, nil\n\t}\n\n\t// If this node is frozen, we need to write the result in\n\t// a new account node.\n\tif n.IsFrozen() {\n\t\tnewRef, newHandle, err := manager.createAccount()\n\t\tif err != nil {\n\t\t\treturn *thisRef, false, err\n\t\t}\n\t\tdefer newHandle.Release()\n\t\tnewNode := newHandle.Get().(*AccountNode)\n\t\t*newNode = *n\n\t\tnewNode.markDirty()\n\t\tnewNode.markMutable()\n\t\tnewNode.storage = NewNodeReference(EmptyId())\n\t\tnewNode.storageHashDirty = true\n\t\treturn newRef, false, nil\n\t}\n\n\tif !n.storage.Id().IsEmpty() {\n\t\tmanager.releaseTrieAsynchronous(n.storage)\n\t}\n\n\tn.storage = NewNodeReference(EmptyId())\n\tn.markDirty()\n\tn.storageHashDirty = true\n\treturn *thisRef, true, err\n}\n\nfunc (n *AccountNode) Release(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.nodeBase.Release()\n\tif !n.storage.Id().IsEmpty() {\n\t\trootHandle, err := manager.getWriteAccess(&n.storage)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\terr = rootHandle.Get().Release(manager, &n.storage, rootHandle)\n\t\trootHandle.Release()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\treturn manager.release(thisRef)\n}\n\nfunc (n *AccountNode) setPathLength(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], length byte) (NodeReference, bool, error) {\n\tif n.pathLength == length {\n\t\treturn *thisRef, false, nil\n\t}\n\tif n.IsFrozen() {\n\t\tnewRef, newHandle, err := manager.createAccount()\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, false, err\n\t\t}\n\t\tdefer newHandle.Release()\n\t\tnewNode := newHandle.Get().(*AccountNode)\n\t\t*newNode = *n\n\t\tnewNode.markDirty()\n\t\tnewNode.markMutable()\n\t\tnewNode.pathLength = length\n\t\treturn newRef, false, nil\n\t}\n\n\tn.pathLength = length\n\tn.markDirty()\n\treturn *thisRef, true, nil\n}\n\nfunc (n *AccountNode) Freeze(manager NodeManager, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.MarkFrozen()\n\thandle, err := manager.getWriteAccess(&n.storage)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer handle.Release()\n\treturn handle.Get().Freeze(manager, handle)\n}\n\nfunc (n *AccountNode) Check(source NodeSource, thisRef *NodeReference, path []Nibble) error {\n\t// Checked invariants:\n\t//  - account information must not be empty\n\t//  - the account is at a correct position in the trie\n\t//  - frozen flags are consistent\n\t//  - path length\n\tvar errs []error\n\n\tif err := n.nodeBase.check(thisRef); err != nil {\n\t\terrs = append(errs, err)\n\t}\n\n\thashWithParent := source.getConfig().HashStorageLocation == HashStoredWithParent\n\tif hashWithParent && n.hasCleanHash() && n.storageHashDirty {\n\t\terrs = append(errs, fmt.Errorf(\"node %v is marked to have a clean hash but storage hash is dirty\", thisRef.Id()))\n\t}\n\n\tfullPath := AddressToNibblePath(n.address, source)\n\tif !IsPrefixOf(path, fullPath[:]) {\n\t\terrs = append(errs, fmt.Errorf(\"node %v - account node %v located in wrong branch: %v\", thisRef.Id(), n.address, path))\n\t}\n\n\tif n.info.IsEmpty() {\n\t\terrs = append(errs, fmt.Errorf(\"node %v - account information must not be empty\", thisRef.Id()))\n\t}\n\n\tif source.getConfig().TrackSuffixLengthsInLeafNodes {\n\t\tmaxPathLength := 40\n\t\tif source.getConfig().UseHashedPaths {\n\t\t\tmaxPathLength = 64\n\t\t}\n\t\tif got, want := n.pathLength, byte(maxPathLength-len(path)); got != want {\n\t\t\terrs = append(errs, fmt.Errorf(\"node %v - invalid path length, wanted %d, got %d\", thisRef.Id(), want, got))\n\t\t}\n\t}\n\n\tif !n.storage.Id().IsEmpty() {\n\t\thandle, err := source.getViewAccess(&n.storage)\n\t\tif err != nil {\n\t\t\terrs = append(errs, err)\n\t\t} else {\n\t\t\tstorageIsFrozen := handle.Get().IsFrozen()\n\t\t\thandle.Release()\n\t\t\tif n.IsFrozen() && !storageIsFrozen {\n\t\t\t\terrs = append(errs, fmt.Errorf(\"the frozen node %v must not have a non-frozen storage\", thisRef.Id()))\n\t\t\t}\n\t\t}\n\t}\n\n\treturn errors.Join(errs...)\n}\n\nfunc (n *AccountNode) Dump(out io.Writer, source NodeSource, thisRef *NodeReference, indent string) error {\n\terrs := []error{}\n\tfmt.Fprintf(out, \"%sAccount (ID: %v, dirty: %t, frozen: %t, path length: %v, Hash: %v, hashState: %v): %v - %v\\n\", indent, thisRef.Id(), n.IsDirty(), n.IsFrozen(), n.pathLength, formatHashForDump(n.hash), n.getHashStatus(), n.address, n.info)\n\tif n.storage.Id().IsEmpty() {\n\t\treturn nil\n\t}\n\tif node, err := source.getViewAccess(&n.storage); err == nil {\n\t\tdefer node.Release()\n\t\tif err := node.Get().Dump(out, source, &n.storage, indent+\"  \"); err != nil {\n\t\t\terrs = append(errs, err)\n\t\t}\n\t} else {\n\t\tfmt.Fprintf(out, \"%s  ERROR: unable to load node %v: %v\", indent, n.storage, err)\n\t\terrs = append(errs, err)\n\t}\n\treturn errors.Join(errs...)\n}\n\nfunc (n *AccountNode) Visit(source NodeSource, thisRef *NodeReference, depth int, visitor NodeVisitor) (bool, error) {\n\tresponse := visitor.Visit(n, NodeInfo{Id: thisRef.Id(), Depth: &depth})\n\tswitch response {\n\tcase VisitResponseAbort:\n\t\treturn true, nil\n\tcase VisitResponsePrune:\n\t\treturn false, nil\n\t}\n\tif n.storage.Id().IsEmpty() {\n\t\treturn false, nil\n\t}\n\tif node, err := source.getViewAccess(&n.storage); err == nil {\n\t\tdefer node.Release()\n\t\treturn node.Get().Visit(source, &n.storage, depth+1, visitor)\n\t} else {\n\t\treturn false, err\n\t}\n}\n\n// ----------------------------------------------------------------------------\n//                               Value Node\n// ----------------------------------------------------------------------------\n\n// ValueNode store the value of a storage slot of an account. Values must not\n// be zero. Also, value nodes must not be reachable in a trie before crossing\n// exactly one AccountNode.\ntype ValueNode struct {\n\tnodeBase\n\tkey   common.Key\n\tvalue common.Value\n\t// pathLength is the number of nibbles of the key (or its hash) not covered\n\t// by the navigation path to this node. It is only maintained if the\n\t// `TrackSuffixLengthsInLeafNodes` of the `MptConfig` is enabled.\n\tpathLength byte\n}\n\nfunc (n *ValueNode) Key() common.Key {\n\treturn n.key\n}\n\nfunc (n *ValueNode) Value() common.Value {\n\treturn n.value\n}\n\nfunc (n *ValueNode) GetAccount(NodeSource, common.Address, []Nibble) (AccountInfo, bool, error) {\n\treturn AccountInfo{}, false, fmt.Errorf(\"invalid request: account query should not reach values\")\n}\n\nfunc (n *ValueNode) GetValue(source NodeSource, key common.Key, path []Nibble) (common.Value, bool, error) {\n\tif n.key == key {\n\t\treturn n.value, true, nil\n\t}\n\treturn common.Value{}, false, nil\n}\n\nfunc (n *ValueNode) GetSlot(NodeSource, common.Address, []Nibble, common.Key) (common.Value, bool, error) {\n\treturn common.Value{}, false, fmt.Errorf(\"invalid request: slot query should not reach values\")\n}\n\nfunc (n *ValueNode) SetAccount(NodeManager, *NodeReference, shared.WriteHandle[Node], common.Address, []Nibble, AccountInfo) (NodeReference, bool, error) {\n\treturn NodeReference{}, false, fmt.Errorf(\"invalid request: account update should not reach values\")\n}\n\nfunc (n *ValueNode) SetValue(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], key common.Key, path []Nibble, value common.Value) (NodeReference, bool, error) {\n\t// Check whether this is the correct value node.\n\tif n.key == key {\n\t\tif value == n.value {\n\t\t\treturn *thisRef, false, nil\n\t\t}\n\t\tif value == (common.Value{}) {\n\t\t\tif !n.IsFrozen() {\n\t\t\t\tn.nodeBase.Release()\n\t\t\t\tif err := manager.release(thisRef); err != nil {\n\t\t\t\t\treturn NodeReference{}, false, err\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn NewNodeReference(EmptyId()), !n.IsFrozen(), nil\n\t\t}\n\t\tif n.IsFrozen() {\n\t\t\tnewRef, newHandle, err := manager.createValue()\n\t\t\tif err != nil {\n\t\t\t\treturn NodeReference{}, false, nil\n\t\t\t}\n\t\t\tdefer newHandle.Release()\n\t\t\tnewNode := newHandle.Get().(*ValueNode)\n\t\t\tnewNode.key = n.key\n\t\t\tnewNode.value = value\n\t\t\tnewNode.markDirty()\n\t\t\tnewNode.pathLength = n.pathLength\n\t\t\treturn newRef, false, nil\n\t\t}\n\t\tn.value = value\n\t\tn.markDirty()\n\t\treturn *thisRef, true, nil\n\t}\n\n\t// Skip restructuring the tree if the new info is empty.\n\tif value == (common.Value{}) {\n\t\treturn *thisRef, false, nil\n\t}\n\n\t// Create a new node for the sibling to be added.\n\tsiblingRef, siblingHandle, err := manager.createValue()\n\tif err != nil {\n\t\treturn NodeReference{}, false, err\n\t}\n\tdefer siblingHandle.Release()\n\tsibling := siblingHandle.Get().(*ValueNode)\n\tsibling.key = key\n\tsibling.value = value\n\tsibling.markDirty()\n\n\tthisPath := KeyToNibblePath(n.key, manager)\n\tnewRootId, err := splitLeafNode(manager, thisRef, thisPath[:], n, this, path, &siblingRef, sibling, siblingHandle)\n\treturn newRootId, false, err\n}\n\nfunc (n *ValueNode) SetSlot(NodeManager, *NodeReference, shared.WriteHandle[Node], common.Address, []Nibble, common.Key, common.Value) (NodeReference, bool, error) {\n\treturn NodeReference{}, false, fmt.Errorf(\"invalid request: slot update should not reach values\")\n}\n\nfunc (n *ValueNode) ClearStorage(NodeManager, *NodeReference, shared.WriteHandle[Node], common.Address, []Nibble) (NodeReference, bool, error) {\n\treturn NodeReference{}, false, fmt.Errorf(\"invalid request: clear storage should not reach values\")\n}\n\nfunc (n *ValueNode) Release(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node]) error {\n\tif n.IsFrozen() {\n\t\treturn nil\n\t}\n\tn.nodeBase.Release()\n\treturn manager.release(thisRef)\n}\n\nfunc (n *ValueNode) setPathLength(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], length byte) (NodeReference, bool, error) {\n\tif n.pathLength == length {\n\t\treturn *thisRef, false, nil\n\t}\n\tif n.IsFrozen() {\n\t\tnewRef, newHandle, err := manager.createValue()\n\t\tif err != nil {\n\t\t\treturn NodeReference{}, false, err\n\t\t}\n\t\tdefer newHandle.Release()\n\t\tnewNode := newHandle.Get().(*ValueNode)\n\t\tnewNode.key = n.key\n\t\tnewNode.value = n.value\n\t\tnewNode.markDirty()\n\t\tnewNode.pathLength = length\n\t\treturn newRef, false, nil\n\t}\n\n\tn.pathLength = length\n\tn.markDirty()\n\treturn *thisRef, true, nil\n}\n\nfunc (n *ValueNode) Freeze(NodeManager, shared.WriteHandle[Node]) error {\n\tn.MarkFrozen()\n\treturn nil\n}\n\nfunc (n *ValueNode) Check(source NodeSource, thisRef *NodeReference, path []Nibble) error {\n\t// Checked invariants:\n\t//  - value must not be empty\n\t//  - values are in the right position of the trie\n\t//  - the path length is correct (if enabled to be tracked)\n\tvar errs []error\n\n\tif err := n.nodeBase.check(thisRef); err != nil {\n\t\terrs = append(errs, err)\n\t}\n\n\tfullPath := KeyToNibblePath(n.key, source)\n\tif !IsPrefixOf(path, fullPath[:]) {\n\t\terrs = append(errs, fmt.Errorf(\"node %v - value node %v [%v] located in wrong branch: %v\", thisRef.Id(), n.key, fullPath, path))\n\t}\n\n\tif n.value == (common.Value{}) {\n\t\terrs = append(errs, fmt.Errorf(\"node %v - value slot must not be empty\", thisRef.Id()))\n\t}\n\n\tif source.getConfig().TrackSuffixLengthsInLeafNodes {\n\t\tif got, want := n.pathLength, byte(64-len(path)); got != want {\n\t\t\terrs = append(errs, fmt.Errorf(\"node %v - invalid path length, wanted %d, got %d\", thisRef.Id(), want, got))\n\t\t}\n\t}\n\n\treturn errors.Join(errs...)\n}\n\nfunc (n *ValueNode) Dump(out io.Writer, source NodeSource, thisRef *NodeReference, indent string) error {\n\tfmt.Fprintf(out, \"%sValue (ID: %v/%t/%d, Hash: %v, hashState: %v): %v - %x\\n\", indent, thisRef.Id(), n.IsFrozen(), n.pathLength, formatHashForDump(n.hash), n.getHashStatus(), n.key, n.value)\n\treturn nil\n}\n\nfunc formatHashForDump(hash common.Hash) string {\n\treturn fmt.Sprintf(\"0x%x\", hash)\n}\n\nfunc (n *ValueNode) Visit(source NodeSource, thisRef *NodeReference, depth int, visitor NodeVisitor) (bool, error) {\n\treturn visitor.Visit(n, NodeInfo{Id: thisRef.Id(), Depth: &depth}) == VisitResponseAbort, nil\n}\n\n// ----------------------------------------------------------------------------\n//                               Node Encoders\n// ----------------------------------------------------------------------------\n\n// TODO [cleanup]: move encoder to extra file and clean-up definitions\n\ntype BranchNodeEncoderWithNodeHash struct{}\n\nfunc (BranchNodeEncoderWithNodeHash) GetEncodedSize() int {\n\tencoder := NodeIdEncoder{}\n\treturn encoder.GetEncodedSize()*16 + common.HashSize\n}\n\nfunc (BranchNodeEncoderWithNodeHash) Store(dst []byte, node *BranchNode) error {\n\tif !node.hasCleanHash() {\n\t\tpanic(\"unable to store branch node with dirty hash\")\n\t}\n\tencoder := NodeIdEncoder{}\n\tstep := encoder.GetEncodedSize()\n\tfor i := 0; i < 16; i++ {\n\t\tencoder.Store(dst[i*step:], &node.children[i].id)\n\t}\n\tdst = dst[step*16:]\n\tcopy(dst, node.hash[:])\n\treturn nil\n}\n\nfunc (BranchNodeEncoderWithNodeHash) Load(src []byte, node *BranchNode) error {\n\tencoder := NodeIdEncoder{}\n\tstep := encoder.GetEncodedSize()\n\tfor i := 0; i < 16; i++ {\n\t\tvar id NodeId\n\t\tencoder.Load(src[i*step:], &id)\n\t\tnode.children[i] = NewNodeReference(id)\n\t}\n\tsrc = src[step*16:]\n\tcopy(node.hash[:], src)\n\tnode.hashStatus = hashStatusClean\n\n\t// The hashes of the child nodes are not stored with the node, so they are\n\t// marked as dirty to trigger a re-computation the next time they are used.\n\tfor i := 0; i < 16; i++ {\n\t\tif !node.children[i].Id().IsEmpty() {\n\t\t\tnode.markChildHashDirty(byte(i))\n\t\t}\n\t}\n\n\treturn nil\n}\n\ntype BranchNodeEncoderWithChildHashes struct{}\n\nfunc (BranchNodeEncoderWithChildHashes) GetEncodedSize() int {\n\tencoder := NodeIdEncoder{}\n\treturn encoder.GetEncodedSize()*16 + common.HashSize*16 + 2\n}\n\nfunc (BranchNodeEncoderWithChildHashes) Store(dst []byte, node *BranchNode) error {\n\tif node.dirtyHashes != 0 {\n\t\tpanic(\"unable to store branch node with dirty hash\")\n\t}\n\tencoder := NodeIdEncoder{}\n\tstep := encoder.GetEncodedSize()\n\tfor i := 0; i < 16; i++ {\n\t\tencoder.Store(dst[i*step:], &node.children[i].id)\n\t}\n\tdst = dst[step*16:]\n\tfor i := 0; i < 16; i++ {\n\t\tcopy(dst, node.hashes[i][:])\n\t\tdst = dst[common.HashSize:]\n\t}\n\tbinary.BigEndian.PutUint16(dst, node.embeddedChildren)\n\treturn nil\n}\n\nfunc (BranchNodeEncoderWithChildHashes) Load(src []byte, node *BranchNode) error {\n\tencoder := NodeIdEncoder{}\n\tstep := encoder.GetEncodedSize()\n\tfor i := 0; i < 16; i++ {\n\t\tvar id NodeId\n\t\tencoder.Load(src[i*step:], &id)\n\t\tnode.children[i] = NewNodeReference(id)\n\t}\n\tsrc = src[step*16:]\n\tfor i := 0; i < 16; i++ {\n\t\tcopy(node.hashes[i][:], src)\n\t\tsrc = src[common.HashSize:]\n\t}\n\tnode.embeddedChildren = binary.BigEndian.Uint16(src)\n\n\t// The node's hash is not stored with the node, so it is marked unknown.\n\tnode.hashStatus = hashStatusUnknown\n\n\treturn nil\n}\n\ntype ExtensionNodeEncoderWithNodeHash struct{}\n\nfunc (ExtensionNodeEncoderWithNodeHash) GetEncodedSize() int {\n\tpathEncoder := PathEncoder{}\n\tidEncoder := NodeIdEncoder{}\n\treturn pathEncoder.GetEncodedSize() + idEncoder.GetEncodedSize() + common.HashSize\n}\n\nfunc (ExtensionNodeEncoderWithNodeHash) Store(dst []byte, value *ExtensionNode) error {\n\tif !value.hasCleanHash() {\n\t\tpanic(\"unable to store extension node with dirty hash\")\n\t}\n\tpathEncoder := PathEncoder{}\n\tidEncoder := NodeIdEncoder{}\n\tpathEncoder.Store(dst, &value.path)\n\tdst = dst[pathEncoder.GetEncodedSize():]\n\tidEncoder.Store(dst, &value.next.id)\n\tdst = dst[idEncoder.GetEncodedSize():]\n\tcopy(dst, value.hash[:])\n\treturn nil\n}\n\nfunc (ExtensionNodeEncoderWithNodeHash) Load(src []byte, node *ExtensionNode) error {\n\tpathEncoder := PathEncoder{}\n\tidEncoder := NodeIdEncoder{}\n\tpathEncoder.Load(src, &node.path)\n\tsrc = src[pathEncoder.GetEncodedSize():]\n\tvar id NodeId\n\tidEncoder.Load(src, &id)\n\tnode.next = NewNodeReference(id)\n\tsrc = src[idEncoder.GetEncodedSize():]\n\tcopy(node.hash[:], src)\n\tnode.hashStatus = hashStatusClean\n\n\t// The hash of the next node is not stored with the node, so it is marked\n\t// as dirty to trigger a re-computation the next time it is accessed.\n\tnode.nextHashDirty = true\n\n\treturn nil\n}\n\ntype ExtensionNodeEncoderWithChildHash struct{}\n\nfunc (ExtensionNodeEncoderWithChildHash) GetEncodedSize() int {\n\tpathEncoder := PathEncoder{}\n\tidEncoder := NodeIdEncoder{}\n\treturn pathEncoder.GetEncodedSize() + idEncoder.GetEncodedSize() + common.HashSize + 1\n}\n\nfunc (ExtensionNodeEncoderWithChildHash) Store(dst []byte, value *ExtensionNode) error {\n\tif value.nextHashDirty {\n\t\tpanic(\"unable to store extension node with dirty hash\")\n\t}\n\tpathEncoder := PathEncoder{}\n\tidEncoder := NodeIdEncoder{}\n\tpathEncoder.Store(dst, &value.path)\n\tdst = dst[pathEncoder.GetEncodedSize():]\n\tidEncoder.Store(dst, &value.next.id)\n\tdst = dst[idEncoder.GetEncodedSize():]\n\tcopy(dst, value.nextHash[:])\n\tdst = dst[common.HashSize:]\n\tif value.nextIsEmbedded {\n\t\tdst[0] = 1\n\t} else {\n\t\tdst[0] = 0\n\t}\n\treturn nil\n}\n\nfunc (ExtensionNodeEncoderWithChildHash) Load(src []byte, node *ExtensionNode) error {\n\tpathEncoder := PathEncoder{}\n\tidEncoder := NodeIdEncoder{}\n\tpathEncoder.Load(src, &node.path)\n\tsrc = src[pathEncoder.GetEncodedSize():]\n\tvar id NodeId\n\tidEncoder.Load(src, &id)\n\tnode.next = NewNodeReference(id)\n\tsrc = src[idEncoder.GetEncodedSize():]\n\tcopy(node.nextHash[:], src)\n\tsrc = src[common.HashSize:]\n\tnode.nextIsEmbedded = src[0] != 0\n\n\t// The node's hash is not stored with the node, so it is marked unknown.\n\tnode.hashStatus = hashStatusUnknown\n\n\treturn nil\n}\n\ntype AccountNodeEncoderWithNodeHash struct{}\n\nfunc (AccountNodeEncoderWithNodeHash) GetEncodedSize() int {\n\treturn common.AddressSize +\n\t\tAccountInfoEncoder{}.GetEncodedSize() +\n\t\tNodeIdEncoder{}.GetEncodedSize() +\n\t\tcommon.HashSize\n}\n\nfunc (AccountNodeEncoderWithNodeHash) Store(dst []byte, node *AccountNode) error {\n\tif !node.hasCleanHash() {\n\t\tpanic(\"unable to store account node with dirty hash\")\n\t}\n\tcopy(dst, node.address[:])\n\tdst = dst[len(node.address):]\n\n\tinfoEncoder := AccountInfoEncoder{}\n\tinfoEncoder.Store(dst, &node.info)\n\tdst = dst[infoEncoder.GetEncodedSize():]\n\n\tidEncoder := NodeIdEncoder{}\n\tidEncoder.Store(dst, &node.storage.id)\n\tdst = dst[idEncoder.GetEncodedSize():]\n\tcopy(dst[:], node.hash[:])\n\treturn nil\n}\n\nfunc (AccountNodeEncoderWithNodeHash) Load(src []byte, node *AccountNode) error {\n\tcopy(node.address[:], src)\n\tsrc = src[len(node.address):]\n\n\tinfoEncoder := AccountInfoEncoder{}\n\tinfoEncoder.Load(src, &node.info)\n\tsrc = src[infoEncoder.GetEncodedSize():]\n\n\tidEncoder := NodeIdEncoder{}\n\tvar id NodeId\n\tidEncoder.Load(src, &id)\n\tnode.storage = NewNodeReference(id)\n\tsrc = src[idEncoder.GetEncodedSize():]\n\tcopy(node.hash[:], src)\n\tnode.hashStatus = hashStatusClean\n\n\t// The storage hash is not stored with the node, so it is marked as dirty to\n\t// trigger a re-computation the next time it is accessed.\n\tnode.storageHashDirty = true\n\n\treturn nil\n}\n\ntype AccountNodeEncoderWithChildHash struct{}\n\nfunc (AccountNodeEncoderWithChildHash) GetEncodedSize() int {\n\treturn common.AddressSize +\n\t\tAccountInfoEncoder{}.GetEncodedSize() +\n\t\tNodeIdEncoder{}.GetEncodedSize() +\n\t\tcommon.HashSize\n}\n\nfunc (AccountNodeEncoderWithChildHash) Store(dst []byte, node *AccountNode) error {\n\tif node.storageHashDirty {\n\t\tpanic(\"unable to store account node with dirty hash\")\n\t}\n\tcopy(dst, node.address[:])\n\tdst = dst[len(node.address):]\n\n\tinfoEncoder := AccountInfoEncoder{}\n\tinfoEncoder.Store(dst, &node.info)\n\tdst = dst[infoEncoder.GetEncodedSize():]\n\n\tidEncoder := NodeIdEncoder{}\n\tidEncoder.Store(dst, &node.storage.id)\n\tdst = dst[idEncoder.GetEncodedSize():]\n\tcopy(dst[:], node.storageHash[:])\n\treturn nil\n}\n\nfunc (AccountNodeEncoderWithChildHash) Load(src []byte, node *AccountNode) error {\n\tcopy(node.address[:], src)\n\tsrc = src[len(node.address):]\n\n\tinfoEncoder := AccountInfoEncoder{}\n\tinfoEncoder.Load(src, &node.info)\n\tsrc = src[infoEncoder.GetEncodedSize():]\n\n\tidEncoder := NodeIdEncoder{}\n\tvar id NodeId\n\tidEncoder.Load(src, &id)\n\tnode.storage = NewNodeReference(id)\n\tsrc = src[idEncoder.GetEncodedSize():]\n\tcopy(node.storageHash[:], src)\n\n\t// The node's hash is not stored with the node, so it is marked unknown.\n\tnode.hashStatus = hashStatusUnknown\n\n\treturn nil\n}\n\ntype AccountNodeWithPathLengthEncoderWithNodeHash struct{}\n\nfunc (AccountNodeWithPathLengthEncoderWithNodeHash) GetEncodedSize() int {\n\treturn AccountNodeEncoderWithNodeHash{}.GetEncodedSize() + 1\n}\n\nfunc (AccountNodeWithPathLengthEncoderWithNodeHash) Store(dst []byte, node *AccountNode) error {\n\tAccountNodeEncoderWithNodeHash{}.Store(dst, node)\n\tdst[len(dst)-1] = node.pathLength\n\treturn nil\n}\n\nfunc (AccountNodeWithPathLengthEncoderWithNodeHash) Load(src []byte, node *AccountNode) error {\n\tAccountNodeEncoderWithNodeHash{}.Load(src, node)\n\tnode.pathLength = src[len(src)-1]\n\treturn nil\n}\n\ntype AccountNodeWithPathLengthEncoderWithChildHash struct{}\n\nfunc (AccountNodeWithPathLengthEncoderWithChildHash) GetEncodedSize() int {\n\treturn AccountNodeEncoderWithChildHash{}.GetEncodedSize() + 1\n}\n\nfunc (AccountNodeWithPathLengthEncoderWithChildHash) Store(dst []byte, node *AccountNode) error {\n\tAccountNodeEncoderWithChildHash{}.Store(dst, node)\n\tdst[len(dst)-1] = node.pathLength\n\treturn nil\n}\n\nfunc (AccountNodeWithPathLengthEncoderWithChildHash) Load(src []byte, node *AccountNode) error {\n\tAccountNodeEncoderWithChildHash{}.Load(src, node)\n\tnode.pathLength = src[len(src)-1]\n\treturn nil\n}\n\ntype ValueNodeEncoderWithoutNodeHash struct{}\n\nfunc (ValueNodeEncoderWithoutNodeHash) GetEncodedSize() int {\n\treturn common.KeySize + common.ValueSize\n}\n\nfunc (ValueNodeEncoderWithoutNodeHash) Store(dst []byte, node *ValueNode) error {\n\tcopy(dst, node.key[:])\n\tdst = dst[len(node.key):]\n\tcopy(dst, node.value[:])\n\treturn nil\n}\n\nfunc (ValueNodeEncoderWithoutNodeHash) Load(src []byte, node *ValueNode) error {\n\tcopy(node.key[:], src)\n\tsrc = src[len(node.key):]\n\tcopy(node.value[:], src)\n\n\t// The node's hash is not stored with the node, so it is marked unknown.\n\tnode.hashStatus = hashStatusUnknown\n\n\treturn nil\n}\n\ntype ValueNodeEncoderWithNodeHash struct{}\n\nfunc (ValueNodeEncoderWithNodeHash) GetEncodedSize() int {\n\treturn ValueNodeEncoderWithoutNodeHash{}.GetEncodedSize() + common.HashSize\n}\n\nfunc (ValueNodeEncoderWithNodeHash) Store(dst []byte, node *ValueNode) error {\n\tif !node.hasCleanHash() {\n\t\tpanic(\"unable to store value node with dirty hash\")\n\t}\n\tValueNodeEncoderWithoutNodeHash{}.Store(dst, node)\n\tdst = dst[ValueNodeEncoderWithoutNodeHash{}.GetEncodedSize():]\n\tcopy(dst, node.hash[:])\n\treturn nil\n}\n\nfunc (ValueNodeEncoderWithNodeHash) Load(src []byte, node *ValueNode) error {\n\tValueNodeEncoderWithoutNodeHash{}.Load(src, node)\n\tsrc = src[ValueNodeEncoderWithoutNodeHash{}.GetEncodedSize():]\n\tcopy(node.hash[:], src)\n\tnode.hashStatus = hashStatusClean\n\treturn nil\n}\n\ntype ValueNodeWithPathLengthEncoderWithoutNodeHash struct{}\n\nfunc (ValueNodeWithPathLengthEncoderWithoutNodeHash) GetEncodedSize() int {\n\treturn ValueNodeEncoderWithoutNodeHash{}.GetEncodedSize() + 1\n}\n\nfunc (ValueNodeWithPathLengthEncoderWithoutNodeHash) Store(dst []byte, node *ValueNode) error {\n\tValueNodeEncoderWithoutNodeHash{}.Store(dst, node)\n\tdst[len(dst)-1] = node.pathLength\n\treturn nil\n}\n\nfunc (ValueNodeWithPathLengthEncoderWithoutNodeHash) Load(src []byte, node *ValueNode) error {\n\tValueNodeEncoderWithoutNodeHash{}.Load(src, node)\n\tnode.pathLength = src[len(src)-1]\n\treturn nil\n}\n\ntype ValueNodeWithPathLengthEncoderWithNodeHash struct{}\n\nfunc (ValueNodeWithPathLengthEncoderWithNodeHash) GetEncodedSize() int {\n\treturn ValueNodeEncoderWithNodeHash{}.GetEncodedSize() + 1\n}\n\nfunc (ValueNodeWithPathLengthEncoderWithNodeHash) Store(dst []byte, node *ValueNode) error {\n\tValueNodeEncoderWithNodeHash{}.Store(dst, node)\n\tdst[len(dst)-1] = node.pathLength\n\treturn nil\n}\n\nfunc (ValueNodeWithPathLengthEncoderWithNodeHash) Load(src []byte, node *ValueNode) error {\n\tValueNodeEncoderWithNodeHash{}.Load(src, node)\n\tnode.pathLength = src[len(src)-1]\n\treturn nil\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/go/database/mpt/nodes.go b/go/database/mpt/nodes.go
--- a/go/database/mpt/nodes.go	(revision cb0032b7724512a252766f973bddcacdc9752c54)
+++ b/go/database/mpt/nodes.go	(date 1717749409152)
@@ -16,10 +16,11 @@
 	"encoding/binary"
 	"errors"
 	"fmt"
-	"github.com/Fantom-foundation/Carmen/go/common"
-	"github.com/Fantom-foundation/Carmen/go/database/mpt/shared"
 	"io"
 	"slices"
+
+	"github.com/Fantom-foundation/Carmen/go/common"
+	"github.com/Fantom-foundation/Carmen/go/database/mpt/shared"
 )
 
 // This file defines the interface and implementation of all node types in a
@@ -1598,6 +1599,10 @@
 	return newRoot, !n.IsFrozen() && manager.getConfig().TrackSuffixLengthsInLeafNodes, err
 }
 
+func (n *AccountNode) HasEmptyStorage() bool {
+	return n.storage.Id().IsEmpty()
+}
+
 type leafNode interface {
 	Node
 	setPathLength(manager NodeManager, thisRef *NodeReference, this shared.WriteHandle[Node], length byte) (newRoot NodeReference, changed bool, err error)
